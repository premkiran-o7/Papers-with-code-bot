{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nGEMINI_API_KEY = user_secrets.get_secret(\"GEMINI_API_KEY\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:50:51.911499Z","iopub.execute_input":"2025-04-22T04:50:51.911880Z","iopub.status.idle":"2025-04-22T04:50:52.082202Z","shell.execute_reply.started":"2025-04-22T04:50:51.911853Z","shell.execute_reply":"2025-04-22T04:50:52.081057Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Test API","metadata":{}},{"cell_type":"code","source":"import requests","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:50:54.984171Z","iopub.execute_input":"2025-04-22T04:50:54.985549Z","iopub.status.idle":"2025-04-22T04:50:55.123315Z","shell.execute_reply.started":"2025-04-22T04:50:54.985509Z","shell.execute_reply":"2025-04-22T04:50:55.122389Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def fetch_papers_from_paperswithcode(query, max_results=10):\n    \"\"\"\n    Fetches papers from Papers with Code API based on a query.\n\n    Args:\n        query (str): The search query.\n        max_results (int): The maximum number of papers to retrieve.\n\n    Returns:\n        list: A list of dictionaries, where each dictionary represents a paper.\n              Returns an empty list if there's an error or no results.\n    \"\"\"\n    url = f\"https://paperswithcode.com/api/v1/papers/?q={query}&limit={max_results}\"\n    try:\n        response = requests.get(url)\n        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n        data = response.json()\n        print(data[\"results\"])\n        return data[\"results\"]\n    except requests.exceptions.RequestException as e:\n        print(f\"Error fetching papers: {e}\")\n        return []\n    except (KeyError, json.JSONDecodeError) as e:\n        print(f\"Error parsing Papers with Code API response: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:50:56.203410Z","iopub.execute_input":"2025-04-22T04:50:56.204150Z","iopub.status.idle":"2025-04-22T04:50:56.211495Z","shell.execute_reply.started":"2025-04-22T04:50:56.204119Z","shell.execute_reply":"2025-04-22T04:50:56.210382Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"results = fetch_papers_from_paperswithcode(\"Large language model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:50:58.676912Z","iopub.execute_input":"2025-04-22T04:50:58.678218Z","iopub.status.idle":"2025-04-22T04:51:01.172785Z","shell.execute_reply.started":"2025-04-22T04:50:58.678172Z","shell.execute_reply":"2025-04-22T04:51:01.171709Z"}},"outputs":[{"name":"stdout","text":"[{'id': 'outrageously-large-neural-networks-the', 'arxiv_id': '1701.06538', 'nips_id': None, 'url_abs': 'http://arxiv.org/abs/1701.06538v1', 'url_pdf': 'http://arxiv.org/pdf/1701.06538v1.pdf', 'title': 'Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer', 'abstract': 'The capacity of a neural network to absorb information is limited by its\\nnumber of parameters. Conditional computation, where parts of the network are\\nactive on a per-example basis, has been proposed in theory as a way of\\ndramatically increasing model capacity without a proportional increase in\\ncomputation. In practice, however, there are significant algorithmic and\\nperformance challenges. In this work, we address these challenges and finally\\nrealize the promise of conditional computation, achieving greater than 1000x\\nimprovements in model capacity with only minor losses in computational\\nefficiency on modern GPU clusters. We introduce a Sparsely-Gated\\nMixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward\\nsub-networks. A trainable gating network determines a sparse combination of\\nthese experts to use for each example. We apply the MoE to the tasks of\\nlanguage modeling and machine translation, where model capacity is critical for\\nabsorbing the vast quantities of knowledge available in the training corpora.\\nWe present model architectures in which a MoE with up to 137 billion parameters\\nis applied convolutionally between stacked LSTM layers. On large language\\nmodeling and machine translation benchmarks, these models achieve significantly\\nbetter results than state-of-the-art at lower computational cost.', 'authors': ['Krzysztof Maziarz', 'Noam Shazeer', 'Geoffrey Hinton', 'Andy Davis', 'Quoc Le', 'Jeff Dean', 'Azalia Mirhoseini'], 'published': '2017-01-23', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'a-simple-way-to-initialize-recurrent-networks', 'arxiv_id': '1504.00941', 'nips_id': None, 'url_abs': 'http://arxiv.org/abs/1504.00941v2', 'url_pdf': 'http://arxiv.org/pdf/1504.00941v2.pdf', 'title': 'A Simple Way to Initialize Recurrent Networks of Rectified Linear Units', 'abstract': 'Learning long term dependencies in recurrent networks is difficult due to\\nvanishing and exploding gradients. To overcome this difficulty, researchers\\nhave developed sophisticated optimization techniques and network architectures.\\nIn this paper, we propose a simpler solution that use recurrent neural networks\\ncomposed of rectified linear units. Key to our solution is the use of the\\nidentity matrix or its scaled version to initialize the recurrent weight\\nmatrix. We find that our solution is comparable to LSTM on our four benchmarks:\\ntwo toy problems involving long-range temporal structures, a large language\\nmodeling problem and a benchmark speech recognition problem.', 'authors': ['Quoc V. Le', 'Navdeep Jaitly', 'Geoffrey E. Hinton'], 'published': '2015-04-03', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'n-gram-counts-and-language-models-from-the', 'arxiv_id': None, 'nips_id': None, 'url_abs': 'https://aclanthology.org/L14-1074', 'url_pdf': 'https://aclanthology.org/L14-1074.pdf', 'title': 'N-gram Counts and Language Models from the Common Crawl', 'abstract': 'We contribute 5-gram counts and language models trained on the Common Crawl corpus, a collection over 9 billion web pages. This release improves upon the Google n-gram counts in two key ways: the inclusion of low-count entries and deduplication to reduce boilerplate. By preserving singletons, we were able to use Kneser-Ney smoothing to build large language models. This paper describes how the corpus was processed with emphasis on the problems that arise in working with data at this scale. Our unpruned Kneser-Ney English {\\\\$}5{\\\\$}-gram language model, built on 975 billion deduplicated tokens, contains over 500 billion unique n-grams. We show gains of 0.5-1.4 BLEU by using large language models to translate into various languages.', 'authors': ['Christian Buck', 'Bas van Ooyen', 'Kenneth Heafield'], 'published': '2014-05-01', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': 'lrec-2014-5'}, {'id': 'enhancing-clinical-concept-extraction-with', 'arxiv_id': '1902.08691', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/1902.08691v4', 'url_pdf': 'https://arxiv.org/pdf/1902.08691v4.pdf', 'title': 'Enhancing Clinical Concept Extraction with Contextual Embeddings', 'abstract': 'Neural network-based representations (\"embeddings\") have dramatically advanced natural language processing (NLP) tasks, including clinical NLP tasks such as concept extraction. Recently, however, more advanced embedding methods and representations (e.g., ELMo, BERT) have further pushed the state-of-the-art in NLP, yet there are no common best practices for how to integrate these representations into clinical tasks. The purpose of this study, then, is to explore the space of possible options in utilizing these new models for clinical concept extraction, including comparing these to traditional word embedding methods (word2vec, GloVe, fastText). Both off-the-shelf open-domain embeddings and pre-trained clinical embeddings from MIMIC-III are evaluated. We explore a battery of embedding methods consisting of traditional word embeddings and contextual embeddings, and compare these on four concept extraction corpora: i2b2 2010, i2b2 2012, SemEval 2014, and SemEval 2015. We also analyze the impact of the pre-training time of a large language model like ELMo or BERT on the extraction performance. Last, we present an intuitive way to understand the semantic information encoded by contextual embeddings. Contextual embeddings pre-trained on a large clinical corpus achieves new state-of-the-art performances across all concept extraction tasks. The best-performing model outperforms all state-of-the-art methods with respective F1-measures of 90.25, 93.18 (partial), 80.74, and 81.65. We demonstrate the potential of contextual embeddings through the state-of-the-art performance these methods achieve on clinical concept extraction. Additionally, we demonstrate contextual embeddings encode valuable semantic information not accounted for in traditional word representations.', 'authors': ['Yuqi Si', 'Hua Xu', 'Jingqi Wang', 'Kirk Roberts'], 'published': '2019-02-22', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'baracks-wife-hillary-using-knowledge-graphs', 'arxiv_id': '1906.07241', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/1906.07241v2', 'url_pdf': 'https://arxiv.org/pdf/1906.07241v2.pdf', 'title': \"Barack's Wife Hillary: Using Knowledge-Graphs for Fact-Aware Language Modeling\", 'abstract': \"Modeling human language requires the ability to not only generate fluent text but also encode factual knowledge. However, traditional language models are only capable of remembering facts seen at training time, and often have difficulty recalling them. To address this, we introduce the knowledge graph language model (KGLM), a neural language model with mechanisms for selecting and copying facts from a knowledge graph that are relevant to the context. These mechanisms enable the model to render information it has never seen before, as well as generate out-of-vocabulary tokens. We also introduce the Linked WikiText-2 dataset, a corpus of annotated text aligned to the Wikidata knowledge graph whose contents (roughly) match the popular WikiText-2 benchmark. In experiments, we demonstrate that the KGLM achieves significantly better performance than a strong baseline language model. We additionally compare different language model's ability to complete sentences requiring factual knowledge, showing that the KGLM outperforms even very large language models in generating facts.\", 'authors': ['Sameer Singh', 'Matt Gardner', 'Nelson F. Liu', 'Matthew E. Peters', 'Robert L. Logan IV'], 'published': '2019-06-17', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'release-strategies-and-the-social-impacts-of', 'arxiv_id': '1908.09203', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/1908.09203v2', 'url_pdf': 'https://arxiv.org/pdf/1908.09203v2.pdf', 'title': 'Release Strategies and the Social Impacts of Language Models', 'abstract': \"Large language models have a range of beneficial uses: they can assist in prose, poetry, and programming; analyze dataset biases; and more. However, their flexibility and generative capabilities also raise misuse concerns. This report discusses OpenAI's work related to the release of its GPT-2 language model. It discusses staged release, which allows time between model releases to conduct risk and benefit analyses as model sizes increased. It also discusses ongoing partnership-based research and provides recommendations for better coordination and responsible publication in AI.\", 'authors': ['Miles McCain', 'Ariel Herbert-Voss', 'Jack Clark', 'Jong Wook Kim', 'Jeff Wu', 'Jason Blazakis', 'Alex Newhouse', 'Amanda Askell', 'Alec Radford', 'Sarah Kreps', 'Miles Brundage', 'Kris McGuffie', 'Jasmine Wang', 'Irene Solaiman', 'Gretchen Krueger'], 'published': '2019-08-24', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'structured-pruning-of-large-language-models', 'arxiv_id': '1910.04732', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/1910.04732v2', 'url_pdf': 'https://arxiv.org/pdf/1910.04732v2.pdf', 'title': 'Structured Pruning of Large Language Models', 'abstract': 'Large language models have recently achieved state of the art performance across a wide variety of natural language tasks. Meanwhile, the size of these models and their latency have significantly increased, which makes their usage costly, and raises an interesting question: do language models need to be large? We study this question through the lens of model compression. We present a generic, structured pruning approach by parameterizing each weight matrix using its low-rank factorization, and adaptively removing rank-1 components during training. On language modeling tasks, our structured approach outperforms other unstructured and block-structured pruning baselines at various compression levels, while achieving significant speedups during both training and inference. We also demonstrate that our method can be applied to pruning adaptive word embeddings in large language models, and to pruning the BERT model on several downstream fine-tuning classification benchmarks.', 'authors': ['Tao Lei', 'Ziheng Wang', 'Jeremy Wohlwend'], 'published': '2019-10-10', 'conference': None, 'conference_url_abs': 'https://aclanthology.org/2020.emnlp-main.496', 'conference_url_pdf': 'https://aclanthology.org/2020.emnlp-main.496.pdf', 'proceeding': 'emnlp-2020-11'}, {'id': 'exbert-a-visual-analysis-tool-to-explore', 'arxiv_id': '1910.05276', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/1910.05276v1', 'url_pdf': 'https://arxiv.org/pdf/1910.05276v1.pdf', 'title': 'exBERT: A Visual Analysis Tool to Explore Learned Representations in Transformers Models', 'abstract': 'Large language models can produce powerful contextual representations that lead to improvements across many NLP tasks. Since these models are typically guided by a sequence of learned self attention mechanisms and may comprise undesired inductive biases, it is paramount to be able to explore what the attention has learned. While static analyses of these models lead to targeted insights, interactive tools are more dynamic and can help humans better gain an intuition for the model-internal reasoning process. We present exBERT, an interactive tool named after the popular BERT language model, that provides insights into the meaning of the contextual representations by matching a human-specified input to similar contexts in a large annotated dataset. By aggregating the annotations of the matching similar contexts, exBERT helps intuitively explain what each attention-head has learned.', 'authors': ['Hendrik Strobelt', 'Benjamin Hoover', 'Sebastian Gehrmann'], 'published': '2019-10-11', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'feature-detection-and-attenuation-in', 'arxiv_id': '1910.05862', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/1910.05862v4', 'url_pdf': 'https://arxiv.org/pdf/1910.05862v4.pdf', 'title': 'Constrained Non-Affine Alignment of Embeddings', 'abstract': 'Embeddings are one of the fundamental building blocks for data analysis tasks. Embeddings are already essential tools for large language models and image analysis, and their use is being extended to many other research domains. The generation of these distributed representations is often a data- and computation-expensive process; yet the holistic analysis and adjustment of them after they have been created is still a developing area. In this paper, we first propose a very general quantitatively measure for the presence of features in the embedding data based on if it can be learned. We then devise a method to remove or alleviate undesired features in the embedding while retaining the essential structure of the data. We use a Domain Adversarial Network (DAN) to generate a non-affine transformation, but we add constraints to ensure the essential structure of the embedding is preserved. Our empirical results demonstrate that the proposed algorithm significantly outperforms the state-of-art unsupervised algorithm on several data sets, including novel applications from the industry.', 'authors': ['Chin-Chia Michael Yeh', 'Jeff M. Phillips', 'Wei zhang', 'Feifei Li', 'Bendre Mangesh', 'Das Mahashweta', 'Zhongfang Zhuang', 'Yan Zheng', 'Yanqing Peng', 'Yuwei Wang'], 'published': '2019-10-13', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'do-nuclear-submarines-have-nuclear-captains-a', 'arxiv_id': None, 'nips_id': None, 'url_abs': 'https://aclanthology.org/D19-1625', 'url_pdf': 'https://aclanthology.org/D19-1625.pdf', 'title': 'Do Nuclear Submarines Have Nuclear Captains? A Challenge Dataset for Commonsense Reasoning over Adjectives and Objects', 'abstract': \"How do adjectives project from a noun to its parts? If a motorcycle is red, are its wheels red? Is a nuclear submarine{'}s captain nuclear? These questions are easy for humans to judge using our commonsense understanding of the world, but are difficult for computers. To attack this challenge, we crowdsource a set of human judgments that answer the English-language question {``}Given a whole described by an adjective, does the adjective also describe a given part?{''} We build strong baselines for this task with a classification approach. Our findings indicate that, despite the recent successes of large language models on tasks aimed to assess commonsense knowledge, these models do not greatly outperform simple word-level models based on pre-trained word embeddings. This provides evidence that the amount of commonsense knowledge encoded in these language models does not extend far beyond that already baked into the word embeddings. Our dataset will serve as a useful testbed for future research in commonsense reasoning, especially as it relates to adjectives and objects\", 'authors': ['Nanyun Peng', 'James Mullenbach', 'Jonathan Gordon', 'Jonathan May'], 'published': '2019-11-01', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': 'ijcnlp-2019-11'}, {'id': 'paraphrasing-with-large-language-models-1', 'arxiv_id': '1911.09661', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/1911.09661v1', 'url_pdf': 'https://arxiv.org/pdf/1911.09661v1.pdf', 'title': 'Paraphrasing with Large Language Models', 'abstract': 'Recently, large language models such as GPT-2 have shown themselves to be extremely adept at text generation and have also been able to achieve high-quality results in many downstream NLP tasks such as text classification, sentiment analysis and question answering with the aid of fine-tuning. We present a useful technique for using a large language model to perform the task of paraphrasing on a variety of texts and subjects. Our approach is demonstrated to be capable of generating paraphrases not only at a sentence level but also for longer spans of text such as paragraphs without needing to break the text into smaller chunks.', 'authors': ['Sam Witteveen', 'Martin Andrews'], 'published': '2019-11-21', 'conference': 'paraphrasing-with-large-language-models', 'conference_url_abs': 'https://aclanthology.org/D19-5623', 'conference_url_pdf': 'https://aclanthology.org/D19-5623.pdf', 'proceeding': 'ws-2019-11'}, {'id': 'waldorf-wasteless-language-model-distillation', 'arxiv_id': '1912.06638', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/1912.06638v2', 'url_pdf': 'https://arxiv.org/pdf/1912.06638v2.pdf', 'title': 'WaLDORf: Wasteless Language-model Distillation On Reading-comprehension', 'abstract': 'Transformer based Very Large Language Models (VLLMs) like BERT, XLNet and RoBERTa, have recently shown tremendous performance on a large variety of Natural Language Understanding (NLU) tasks. However, due to their size, these VLLMs are extremely resource intensive and cumbersome to deploy at production time. Several recent publications have looked into various ways to distil knowledge from a transformer based VLLM (most commonly BERT-Base) into a smaller model which can run much faster at inference time. Here, we propose a novel set of techniques which together produce a task-specific hybrid convolutional and transformer model, WaLDORf, that achieves state-of-the-art inference speed while still being more accurate than previous distilled models.', 'authors': ['Hans-Martin Will', 'Pai-Hung Chen', 'James Yi Tian', 'Alexander P. Kreuzer'], 'published': '2019-12-13', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'on-the-comparability-of-pre-trained-language', 'arxiv_id': '2001.00781', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2001.00781v1', 'url_pdf': 'https://arxiv.org/pdf/2001.00781v1.pdf', 'title': 'On the comparability of Pre-trained Language Models', 'abstract': 'Recent developments in unsupervised representation learning have successfully established the concept of transfer learning in NLP. Mainly three forces are driving the improvements in this area of research: More elaborated architectures are making better use of contextual information. Instead of simply plugging in static pre-trained representations, these are learned based on surrounding context in end-to-end trainable models with more intelligently designed language modelling objectives. Along with this, larger corpora are used as resources for pre-training large language models in a self-supervised fashion which are afterwards fine-tuned on supervised tasks. Advances in parallel computing as well as in cloud computing, made it possible to train these models with growing capacities in the same or even in shorter time than previously established models. These three developments agglomerate in new state-of-the-art (SOTA) results being revealed in a higher and higher frequency. It is not always obvious where these improvements originate from, as it is not possible to completely disentangle the contributions of the three driving forces. We set ourselves to providing a clear and concise overview on several large pre-trained language models, which achieved SOTA results in the last two years, with respect to their use of new architectures and resources. We want to clarify for the reader where the differences between the models are and we furthermore attempt to gain some insight into the single contributions of lexical/computational improvements as well as of architectural changes. We explicitly do not intend to quantify these contributions, but rather see our work as an overview in order to identify potential starting points for benchmark comparisons. Furthermore, we tentatively want to point at potential possibilities for improvement in the field of open-sourcing and reproducible research.', 'authors': ['Christian Heumann', 'Matthias Aßenmacher'], 'published': '2020-01-03', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'compressing-language-models-using-doped', 'arxiv_id': '2001.08896', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2001.08896v5', 'url_pdf': 'https://arxiv.org/pdf/2001.08896v5.pdf', 'title': 'Compressing Language Models using Doped Kronecker Products', 'abstract': 'Kronecker Products (KP) have been used to compress IoT RNN Applications by 15-38x compression factors, achieving better results than traditional compression methods. However when KP is applied to large Natural Language Processing tasks, it leads to significant accuracy loss (approx 26%). This paper proposes a way to recover accuracy otherwise lost when applying KP to large NLP tasks, by allowing additional degrees of freedom in the KP matrix. More formally, we propose doping, a process of adding an extremely sparse overlay matrix on top of the pre-defined KP structure. We call this compression method doped kronecker product compression. To train these models, we present a new solution to the phenomenon of co-matrix adaption (CMA), which uses a new regularization scheme called co matrix dropout regularization (CMR). We present experimental results that demonstrate compression of a large language model with LSTM layers of size 25 MB by 25x with 1.4% loss in perplexity score. At 25x compression, an equivalent pruned network leads to 7.9% loss in perplexity score, while HMD and LMF lead to 15% and 27% loss in perplexity score respectively.', 'authors': ['Paul N. Whatmough', 'Zhi-Gang Liu', 'Urmish Thakker', 'Matthew Mattina', 'Jesse Beu'], 'published': '2020-01-24', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'citation-text-generation', 'arxiv_id': '2002.00317', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2002.00317v3', 'url_pdf': 'https://arxiv.org/pdf/2002.00317v3.pdf', 'title': 'Explaining Relationships Between Scientific Documents', 'abstract': 'We address the task of explaining relationships between two scientific documents using natural language text. This task requires modeling the complex content of long technical documents, deducing a relationship between these documents, and expressing the details of that relationship in text. In addition to the theoretical interest of this task, successful solutions can help improve researcher efficiency in search and review. In this paper we establish a dataset of 622K examples from 154K documents. We pretrain a large language model to serve as the foundation for autoregressive approaches to the task. We explore the impact of taking different views on the two documents, including the use of dense representations extracted with scientific IE systems. We provide extensive automatic and human evaluations which show the promise of such models, but make clear challenges for future work.', 'authors': ['Isabel Cachola', 'Rik Koncel-Kedziorski', 'Xinyi Wu', 'Kyle Lo', 'Noah A. Smith', 'Kelvin Luu'], 'published': '2020-02-02', 'conference': None, 'conference_url_abs': 'https://aclanthology.org/2021.acl-long.166', 'conference_url_pdf': 'https://aclanthology.org/2021.acl-long.166.pdf', 'proceeding': 'acl-2021-5'}, {'id': 'training-question-answering-models-from', 'arxiv_id': '2002.09599', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2002.09599v1', 'url_pdf': 'https://arxiv.org/pdf/2002.09599v1.pdf', 'title': 'Training Question Answering Models From Synthetic Data', 'abstract': 'Question and answer generation is a data augmentation method that aims to improve question answering (QA) models given the limited amount of human labeled data. However, a considerable gap remains between synthetic and human-generated question-answer pairs. This work aims to narrow this gap by taking advantage of large language models and explores several factors such as model size, quality of pretrained models, scale of data synthesized, and algorithmic choices. On the SQuAD1.1 question answering task, we achieve higher accuracy using solely synthetic questions and answers than when using the SQuAD1.1 training set questions alone. Removing access to real Wikipedia data, we synthesize questions and answers from a synthetic corpus generated by an 8.3 billion parameter GPT-2 model. With no access to human supervision and only access to other models, we are able to train state of the art question answering networks on entirely model-generated data that achieve 88.4 Exact Match (EM) and 93.9 F1 score on the SQuAD1.1 dev set. We further apply our methodology to SQuAD2.0 and show a 2.8 absolute gain on EM score compared to prior work using synthetic data.', 'authors': ['Mostofa Patwary', 'Mohammad Shoeybi', 'Raul Puri', 'Ryan Spring', 'Bryan Catanzaro'], 'published': '2020-02-22', 'conference': None, 'conference_url_abs': 'https://aclanthology.org/2020.emnlp-main.468', 'conference_url_pdf': 'https://aclanthology.org/2020.emnlp-main.468.pdf', 'proceeding': 'emnlp-2020-11'}, {'id': 'residual-energy-based-models-for-text-1', 'arxiv_id': '2004.11714', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2004.11714v1', 'url_pdf': 'https://arxiv.org/pdf/2004.11714v1.pdf', 'title': 'Residual Energy-Based Models for Text Generation', 'abstract': 'Text generation is ubiquitous in many NLP tasks, from summarization, to dialogue and machine translation. The dominant parametric approach is based on locally normalized models which predict one word at a time. While these work remarkably well, they are plagued by exposure bias due to the greedy nature of the generation process. In this work, we investigate un-normalized energy-based models (EBMs) which operate not at the token but at the sequence level. In order to make training tractable, we first work in the residual of a pretrained locally normalized language model and second we train using noise contrastive estimation. Furthermore, since the EBM works at the sequence level, we can leverage pretrained bi-directional contextual representations, such as BERT and RoBERTa. Our experiments on two large language modeling datasets show that residual EBMs yield lower perplexity compared to locally normalized baselines. Moreover, generation via importance sampling is very efficient and of higher quality than the baseline models according to human evaluation.', 'authors': [\"Marc'Aurelio Ranzato\", 'Myle Ott', 'Anton Bakhtin', 'Yuntian Deng', 'Arthur Szlam'], 'published': '2020-04-22', 'conference': None, 'conference_url_abs': 'https://openreview.net/forum?id=B1l4SgHKDH', 'conference_url_pdf': 'https://openreview.net/pdf?id=B1l4SgHKDH', 'proceeding': 'iclr-2020-1'}, {'id': 'challenge-closed-book-science-exam-a-meta', 'arxiv_id': '2004.12303', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2004.12303v1', 'url_pdf': 'https://arxiv.org/pdf/2004.12303v1.pdf', 'title': 'Challenge Closed-book Science Exam: A Meta-learning Based Question Answering System', 'abstract': 'Prior work in standardized science exams requires support from large text corpus, such as targeted science corpus fromWikipedia or SimpleWikipedia. However, retrieving knowledge from the large corpus is time-consuming and questions embedded in complex semantic representation may interfere with retrieval. Inspired by the dual process theory in cognitive science, we propose a MetaQA framework, where system 1 is an intuitive meta-classifier and system 2 is a reasoning module. Specifically, our method based on meta-learning method and large language model BERT, which can efficiently solve science problems by learning from related example questions without relying on external knowledge bases. We evaluate our method on AI2 Reasoning Challenge (ARC), and the experimental results show that meta-classifier yields considerable classification performance on emerging question types. The information provided by meta-classifier significantly improves the accuracy of reasoning module from 46.6% to 64.2%, which has a competitive advantage over retrieval-based QA methods.', 'authors': ['Zhongchao shi', 'Qigang Wang', 'Peng Wang', 'Xinyue Zheng'], 'published': '2020-04-26', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'the-curious-case-of-developmental-bertology', 'arxiv_id': '2007.03774', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2007.03774v1', 'url_pdf': 'https://arxiv.org/pdf/2007.03774v1.pdf', 'title': 'The curious case of developmental BERTology: On sparsity, transfer learning, generalization and the brain', 'abstract': 'In this essay, we explore a point of intersection between deep learning and neuroscience, through the lens of large language models, transfer learning and network compression. Just like perceptual and cognitive neurophysiology has inspired effective deep neural network architectures which in turn make a useful model for understanding the brain, here we explore how biological neural development might inspire efficient and robust optimization procedures which in turn serve as a useful model for the maturation and aging of the brain.', 'authors': ['Xin Wang'], 'published': '2020-07-07', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'controlling-dialogue-generation-with-semantic', 'arxiv_id': '2008.09075', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2008.09075v2', 'url_pdf': 'https://arxiv.org/pdf/2008.09075v2.pdf', 'title': 'Controlling Dialogue Generation with Semantic Exemplars', 'abstract': 'Dialogue systems pretrained with large language models generate locally coherent responses, but lack the fine-grained control over responses necessary to achieve specific goals. A promising method to control response generation is exemplar-based generation, in which models edit exemplar responses that are retrieved from training data, or hand-written to strategically address discourse-level goals, to fit new dialogue contexts. But, current exemplar-based approaches often excessively copy words from the exemplar responses, leading to incoherent replies. We present an Exemplar-based Dialogue Generation model, EDGE, that uses the semantic frames present in exemplar responses to guide generation. We show that controlling dialogue generation based on the semantic frames of exemplars, rather than words in the exemplar itself, improves the coherence of generated responses, while preserving semantic meaning and conversation goals present in exemplar responses.', 'authors': ['Prakhar Gupta', 'Amy Pavel', 'Yulia Tsvetkov', 'Jeffrey P. Bigham'], 'published': '2020-08-20', 'conference': None, 'conference_url_abs': 'https://aclanthology.org/2021.naacl-main.240', 'conference_url_pdf': 'https://aclanthology.org/2021.naacl-main.240.pdf', 'proceeding': 'naacl-2021-4'}, {'id': 'content-planning-for-neural-story-generation', 'arxiv_id': '2009.09870', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2009.09870v2', 'url_pdf': 'https://arxiv.org/pdf/2009.09870v2.pdf', 'title': 'Content Planning for Neural Story Generation with Aristotelian Rescoring', 'abstract': \"Long-form narrative text generated from large language models manages a fluent impersonation of human writing, but only at the local sentence level, and lacks structure or global cohesion. We posit that many of the problems of story generation can be addressed via high-quality content planning, and present a system that focuses on how to learn good plot structures to guide story generation. We utilize a plot-generation language model along with an ensemble of rescoring models that each implement an aspect of good story-writing as detailed in Aristotle's Poetics. We find that stories written with our more principled plot-structure are both more relevant to a given prompt and higher quality than baselines that do not content plan, or that plan in an unprincipled way.\", 'authors': ['Seraphina Goldfarb-Tarrant', 'Nanyun Peng', 'Ralph Weischedel', 'Tuhin Chakrabarty'], 'published': '2020-09-21', 'conference': None, 'conference_url_abs': 'https://aclanthology.org/2020.emnlp-main.351', 'conference_url_pdf': 'https://aclanthology.org/2020.emnlp-main.351.pdf', 'proceeding': 'emnlp-2020-11'}, {'id': 'megatron-cntrl-controllable-story-generation', 'arxiv_id': '2010.00840', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2010.00840v1', 'url_pdf': 'https://arxiv.org/pdf/2010.00840v1.pdf', 'title': 'MEGATRON-CNTRL: Controllable Story Generation with External Knowledge Using Large-Scale Language Models', 'abstract': 'Existing pre-trained large language models have shown unparalleled generative capabilities. However, they are not controllable. In this paper, we propose MEGATRON-CNTRL, a novel framework that uses large-scale language models and adds control to text generation by incorporating an external knowledge base. Our framework consists of a keyword predictor, a knowledge retriever, a contextual knowledge ranker, and a conditional text generator. As we do not have access to ground-truth supervision for the knowledge ranker, we make use of weak supervision from sentence embedding. The empirical results show that our model generates more fluent, consistent, and coherent stories with less repetition and higher diversity compared to prior work on the ROC story dataset. We showcase the controllability of our model by replacing the keywords used to generate stories and re-running the generation process. Human evaluation results show that 77.5% of these stories are successfully controlled by the new keywords. Furthermore, by scaling our model from 124 million to 8.3 billion parameters we demonstrate that larger models improve both the quality of generation (from 74.5% to 93.0% for consistency) and controllability (from 77.5% to 91.5%).', 'authors': ['Bryan Catanzaro', 'Anima Anandkumar', 'Pascale Fung', 'Raul Puri', 'Mohammad Shoeybi', 'Mostofa Patwary', 'Peng Xu'], 'published': '2020-10-02', 'conference': None, 'conference_url_abs': 'https://aclanthology.org/2020.emnlp-main.226', 'conference_url_pdf': 'https://aclanthology.org/2020.emnlp-main.226.pdf', 'proceeding': 'emnlp-2020-11'}, {'id': 'an-empirical-investigation-towards-efficient', 'arxiv_id': '2010.00784', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2010.00784v1', 'url_pdf': 'https://arxiv.org/pdf/2010.00784v1.pdf', 'title': 'An Empirical Investigation Towards Efficient Multi-Domain Language Model Pre-training', 'abstract': 'Pre-training large language models has become a standard in the natural language processing community. Such models are pre-trained on generic data (e.g. BookCorpus and English Wikipedia) and often fine-tuned on tasks in the same domain. However, in order to achieve state-of-the-art performance on out of domain tasks such as clinical named entity recognition and relation extraction, additional in domain pre-training is required. In practice, staged multi-domain pre-training presents performance deterioration in the form of catastrophic forgetting (CF) when evaluated on a generic benchmark such as GLUE. In this paper we conduct an empirical investigation into known methods to mitigate CF. We find that elastic weight consolidation provides best overall scores yielding only a 0.33% drop in performance across seven generic tasks while remaining competitive in bio-medical tasks. Furthermore, we explore gradient and latent clustering based data selection techniques to improve coverage when using elastic weight consolidation and experience replay methods.', 'authors': ['Parminder Bhatia', 'Qing Sun', 'Kristjan Arumae'], 'published': '2020-10-01', 'conference': None, 'conference_url_abs': 'https://aclanthology.org/2020.emnlp-main.394', 'conference_url_pdf': 'https://aclanthology.org/2020.emnlp-main.394.pdf', 'proceeding': 'emnlp-2020-11'}, {'id': 'dact-bert-increasing-the-efficiency-and', 'arxiv_id': None, 'nips_id': None, 'url_abs': 'https://openreview.net/forum?id=wKfXaxPist', 'url_pdf': 'https://openreview.net/pdf?id=wKfXaxPist', 'title': 'DACT-BERT: Increasing the efficiency and interpretability of BERT by using adaptive computation time.', 'abstract': \"Large-scale pre-trained language models have shown remarkable results in diverse NLP applications. Unfortunately, these performance gains have been accompanied by a significant increase in computation time and model size, stressing the need to develop new or complementary strategies to increase the efficiency and interpretability of current large language models, such as BERT. In this paper we propose DACT-BERT, a differentiable adaptive computation time strategy for BERT language model. DACT-BERT adds an adaptive computation mechanism to the regular processing pipeline of BERT. This mechanism controls the number of transformer blocks that BERT needs to execute at inference time. By doing this, the model makes predictions based on the most appropriate intermediate representations for the task encoded by the pre-trained weights. With respect to previous works, DACT-BERT has the advantage of being fully differentiable and directly integrated to BERT's main processing pipeline. This enables the incorporation of gradient-based transparency mechanisms to improve interpretability. Furthermore, by discarding useless steps, DACT-BERT facilitates the understanding of the underlying process used by BERT to reach an inference. Our experiments demonstrate that our approach is effective in significantly reducing computational complexity without affecting model accuracy. Additionally, they also demonstrate that DACT-BERT helps to improve model interpretability. \", 'authors': ['Alvaro Soto', 'Vladimir Araujo', 'Felipe del Rio', 'Cristobal Eyzaguirre'], 'published': '2021-01-01', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'modeling-preconditions-in-text-with-a-crowd', 'arxiv_id': '2010.02429', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2010.02429v3', 'url_pdf': 'https://arxiv.org/pdf/2010.02429v3.pdf', 'title': 'Modeling Preconditions in Text with a Crowd-sourced Dataset', 'abstract': \"Preconditions provide a form of logical connection between events that explains why some events occur together and information that is complementary to the more widely studied relations such as causation, temporal ordering, entailment, and discourse relations. Modeling preconditions in text has been hampered in part due to the lack of large scale labeled data grounded in text. This paper introduces PeKo, a crowd-sourced annotation of preconditions between event pairs in newswire, an order of magnitude larger than prior text annotations. To complement this new corpus, we also introduce two challenge tasks aimed at modeling preconditions: (i) Precondition Identification -- a standard classification task defined over pairs of event mentions, and (ii) Precondition Generation -- a generative task aimed at testing a more general ability to reason about a given event. Evaluation on both tasks shows that modeling preconditions is challenging even for today's large language models (LM). This suggests that precondition knowledge is not easily accessible in LM-derived representations alone. Our generation results show that fine-tuning an LM on PeKo yields better conditional relations than when trained on raw text or temporally-ordered corpora.\", 'authors': ['Niranjan Balasubramanian', 'Nathanael Chambers', 'Keerthi Kumar Kallur', 'Anmol Shukla', 'Gargi Sawhney', 'Pratyush Singh', 'Mahnaz Koupaee', 'Heeyoung Kwon'], 'published': '2020-10-06', 'conference': None, 'conference_url_abs': 'https://aclanthology.org/2020.findings-emnlp.340', 'conference_url_pdf': 'https://aclanthology.org/2020.findings-emnlp.340.pdf', 'proceeding': 'findings-of-the-association-for-computational'}, {'id': 'plug-and-play-conversational-models', 'arxiv_id': '2010.04344', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2010.04344v1', 'url_pdf': 'https://arxiv.org/pdf/2010.04344v1.pdf', 'title': 'Plug-and-Play Conversational Models', 'abstract': 'There has been considerable progress made towards conversational models that generate coherent and fluent responses; however, this often involves training large language models on large dialogue datasets, such as Reddit. These large conversational models provide little control over the generated responses, and this control is further limited in the absence of annotated conversational datasets for attribute specific generation that can be used for fine-tuning the model. In this paper, we first propose and evaluate plug-and-play methods for controllable response generation, which does not require dialogue specific datasets and does not rely on fine-tuning a large model. While effective, the decoding procedure induces considerable computational overhead, rendering the conversational model unsuitable for interactive usage. To overcome this, we introduce an approach that does not require further computation at decoding time, while also does not require any fine-tuning of a large language model. We demonstrate, through extensive automatic and human evaluation, a high degree of control over the generated conversational responses with regard to multiple desired attributes, while being fluent.', 'authors': ['Pascale Fung', 'Sumanth Dathathri', 'Zhaojiang Lin', 'Etsuko Ishii', 'Andrea Madotto'], 'published': '2020-10-09', 'conference': None, 'conference_url_abs': 'https://aclanthology.org/2020.findings-emnlp.219', 'conference_url_pdf': 'https://aclanthology.org/2020.findings-emnlp.219.pdf', 'proceeding': 'findings-of-the-association-for-computational'}, {'id': 'supervised-contrastive-learning-for-pre-1', 'arxiv_id': '2011.01403', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2011.01403v3', 'url_pdf': 'https://arxiv.org/pdf/2011.01403v3.pdf', 'title': 'Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning', 'abstract': 'State-of-the-art natural language understanding classification models follow two-stages: pre-training a large language model on an auxiliary task, and then fine-tuning the model on a task-specific labeled dataset using cross-entropy loss. However, the cross-entropy loss has several shortcomings that can lead to sub-optimal generalization and instability. Driven by the intuition that good generalization requires capturing the similarity between examples in one class and contrasting them with examples in other classes, we propose a supervised contrastive learning (SCL) objective for the fine-tuning stage. Combined with cross-entropy, our proposed SCL loss obtains significant improvements over a strong RoBERTa-Large baseline on multiple datasets of the GLUE benchmark in few-shot learning settings, without requiring specialized architecture, data augmentations, memory banks, or additional unsupervised data. Our proposed fine-tuning objective leads to models that are more robust to different levels of noise in the fine-tuning training data, and can generalize better to related tasks with limited labeled data.', 'authors': ['Ves Stoyanov', 'Alexis Conneau', 'Jingfei Du', 'Beliz Gunel'], 'published': '2020-11-03', 'conference': 'supervised-contrastive-learning-for-pre', 'conference_url_abs': 'https://openreview.net/forum?id=cu7IUiOhujH', 'conference_url_pdf': 'https://openreview.net/pdf?id=cu7IUiOhujH', 'proceeding': 'iclr-2021-1'}, {'id': 'extracting-training-data-from-large-language', 'arxiv_id': '2012.07805', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2012.07805v2', 'url_pdf': 'https://arxiv.org/pdf/2012.07805v2.pdf', 'title': 'Extracting Training Data from Large Language Models', 'abstract': \"It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model. We demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data. We comprehensively evaluate our extraction attack to understand the factors that contribute to its success. Worryingly, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models.\", 'authors': ['Colin Raffel', 'Alina Oprea', 'Ulfar Erlingsson', 'Dawn Song', 'Tom Brown', 'Adam Roberts', 'Katherine Lee', 'Ariel Herbert-Voss', 'Matthew Jagielski', 'Eric Wallace', 'Florian Tramer', 'Nicholas Carlini'], 'published': '2020-12-14', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'a-review-of-on-device-fully-neural-end-to-end', 'arxiv_id': '2012.07974', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2012.07974v3', 'url_pdf': 'https://arxiv.org/pdf/2012.07974v3.pdf', 'title': 'A review of on-device fully neural end-to-end automatic speech recognition algorithms', 'abstract': 'In this paper, we review various end-to-end automatic speech recognition algorithms and their optimization techniques for on-device applications. Conventional speech recognition systems comprise a large number of discrete components such as an acoustic model, a language model, a pronunciation model, a text-normalizer, an inverse-text normalizer, a decoder based on a Weighted Finite State Transducer (WFST), and so on. To obtain sufficiently high speech recognition accuracy with such conventional speech recognition systems, a very large language model (up to 100 GB) is usually needed. Hence, the corresponding WFST size becomes enormous, which prohibits their on-device implementation. Recently, fully neural network end-to-end speech recognition algorithms have been proposed. Examples include speech recognition systems based on Connectionist Temporal Classification (CTC), Recurrent Neural Network Transducer (RNN-T), Attention-based Encoder-Decoder models (AED), Monotonic Chunk-wise Attention (MoChA), transformer-based speech recognition systems, and so on. These fully neural network-based systems require much smaller memory footprints compared to conventional algorithms, therefore their on-device implementation has become feasible. In this paper, we review such end-to-end speech recognition models. We extensively discuss their structures, performance, and advantages compared to conventional algorithms.', 'authors': ['Changwoo Han', 'Abhinav Garg', 'Sungsoo Kim', 'Ankur Kumar', 'Jiyeon Kim', 'Dongsoo Lee', 'Dhananjaya Gowda', 'Chanwoo Kim'], 'published': '2020-12-14', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'evolution-is-all-you-need-phylogenetic', 'arxiv_id': '2012.13475', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2012.13475v1', 'url_pdf': 'https://arxiv.org/pdf/2012.13475v1.pdf', 'title': 'Evolution Is All You Need: Phylogenetic Augmentation for Contrastive Learning', 'abstract': 'Self-supervised representation learning of biological sequence embeddings alleviates computational resource constraints on downstream tasks while circumventing expensive experimental label acquisition. However, existing methods mostly borrow directly from large language models designed for NLP, rather than with bioinformatics philosophies in mind. Recently, contrastive mutual information maximization methods have achieved state-of-the-art representations for ImageNet. In this perspective piece, we discuss how viewing evolution as natural sequence augmentation and maximizing information across phylogenetic \"noisy channels\" is a biologically and theoretically desirable objective for pretraining encoders. We first provide a review of current contrastive learning literature, then provide an illustrative example where we show that contrastive learning using evolutionary augmentation can be used as a representation learning objective which maximizes the mutual information between biological sequences and their conserved function, and finally outline rationale for this approach.', 'authors': ['Alan Moses', 'Alex X. Lu', 'Amy X. Lu'], 'published': '2020-12-25', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'km-bart-knowledge-enhanced-multimodal-bart', 'arxiv_id': '2101.00419', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2101.00419v2', 'url_pdf': 'https://arxiv.org/pdf/2101.00419v2.pdf', 'title': 'KM-BART: Knowledge Enhanced Multimodal BART for Visual Commonsense Generation', 'abstract': 'We present Knowledge Enhanced Multimodal BART (KM-BART), which is a Transformer-based sequence-to-sequence model capable of reasoning about commonsense knowledge from multimodal inputs of images and texts. We adapt the generative BART architecture to a multimodal model with visual and textual inputs. We further develop novel pretraining tasks to improve the model performance on the Visual Commonsense Generation (VCG) task. In particular, our pretraining task of Knowledge-based Commonsense Generation (KCG) boosts model performance on the VCG task by leveraging commonsense knowledge from a large language model pretrained on external commonsense knowledge graphs. To the best of our knowledge, we are the first to propose a dedicated task for improving model performance on the VCG task. Experimental results show that our model reaches state-of-the-art performance on the VCG task by applying these novel pretraining tasks.', 'authors': ['Roger Wattenhofer', 'Gerhard Lakemeyer', 'Yunpu Ma', 'Zhao Meng', 'Zai Shi', 'Yiran Xing'], 'published': '2021-01-02', 'conference': None, 'conference_url_abs': 'https://aclanthology.org/2021.acl-long.44', 'conference_url_pdf': 'https://aclanthology.org/2021.acl-long.44.pdf', 'proceeding': 'acl-2021-5'}, {'id': 'understanding-few-shot-commonsense-knowledge', 'arxiv_id': '2101.00297', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2101.00297v3', 'url_pdf': 'https://arxiv.org/pdf/2101.00297v3.pdf', 'title': 'Analyzing Commonsense Emergence in Few-shot Knowledge Models', 'abstract': 'Recently, commonsense knowledge models - pretrained language models (LM) fine-tuned on knowledge graph (KG) tuples - showed that considerable amounts of commonsense knowledge can be encoded in the parameters of large language models. However, as parallel studies show that LMs are poor hypothesizers of declarative commonsense relationships on their own, it remains unclear whether this knowledge is learned during pretraining or from fine-tuning on KG examples. To investigate this question, we train commonsense knowledge models in few-shot settings to study the emergence of their commonsense representation abilities. Our results show that commonsense knowledge models can rapidly adapt from limited examples, indicating that KG fine-tuning serves to learn an interface to encoded knowledge learned during pretraining. Importantly, our analysis of absolute, angular, and distributional parameter changes during few-shot fine-tuning provides novel insights into how this interface is learned.', 'authors': ['Antoine Bosselut', 'Yejin Choi', 'Ximing Lu', 'Ronan Le Bras', 'Jeff Da'], 'published': '2021-01-01', 'conference': None, 'conference_url_abs': 'https://openreview.net/forum?id=StHCELh9PVE', 'conference_url_pdf': 'https://openreview.net/pdf?id=StHCELh9PVE', 'proceeding': 'akbc-2021-10'}, {'id': 'a-graph-total-variation-regularized-softmax', 'arxiv_id': '2101.00153', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2101.00153v3', 'url_pdf': 'https://arxiv.org/pdf/2101.00153v3.pdf', 'title': 'Graphmax for Text Generation', 'abstract': 'In text generation, a large language model (LM) makes a choice of each new word based only on the former selection of its context using the softmax function. Nevertheless, the link statistics information of concurrent words based on a scene-specific corpus is valuable in choosing the next word, which can help to ensure the topic of the generated text to be aligned with the current task. To fully explore the co-occurrence information,we propose a graphmax function for task-specific text generation. Using the graph-based regularization, graphmax enables the final word choice to be determined by both the global knowledge from the LM and the local knowledge from the scene-specific corpus. The traditional softmax function is regularized with a graph total variation (GTV) term, which incorporates the local knowledge into the LM and encourages the model to consider the statistical relationships between words in a scene-specific corpus. The proposed graphmax is versatile and can be readily plugged into any large pre-trained LM for text generation and machine translation. Through extensive experiments, we demonstrate that the new GTV-based regularization can improve performances in various natural language processing tasks in comparison with existing methods. Moreover, through human experiments, we observe that participants can easily distinguish the text generated by graphmax or softmax.', 'authors': ['Yin Guosheng', 'Liu Bin'], 'published': '2021-01-01', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'persistent-anti-muslim-bias-in-large-language', 'arxiv_id': '2101.05783', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2101.05783v2', 'url_pdf': 'https://arxiv.org/pdf/2101.05783v2.pdf', 'title': 'Persistent Anti-Muslim Bias in Large Language Models', 'abstract': 'It has been observed that large-scale language models capture undesirable societal biases, e.g. relating to race and gender; yet religious bias has been relatively unexplored. We demonstrate that GPT-3, a state-of-the-art contextual language model, captures persistent Muslim-violence bias. We probe GPT-3 in various ways, including prompt completion, analogical reasoning, and story generation, to understand this anti-Muslim bias, demonstrating that it appears consistently and creatively in different uses of the model and that it is severe even compared to biases about other religious groups. For instance, \"Muslim\" is analogized to \"terrorist\" in 23% of test cases, while \"Jewish\" is mapped to \"money\" in 5% of test cases. We quantify the positive distraction needed to overcome this bias with adversarial text prompts, and find that use of the most positive 6 adjectives reduces violent completions for \"Muslims\" from 66% to 20%, but which is still higher than for other religious groups.', 'authors': ['James Zou', 'Maheen Farooqi', 'Abubakar Abid'], 'published': '2021-01-14', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'distilling-large-language-models-into-tiny', 'arxiv_id': '2101.08890', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2101.08890v1', 'url_pdf': 'https://arxiv.org/pdf/2101.08890v1.pdf', 'title': 'Distilling Large Language Models into Tiny and Effective Students using pQRNN', 'abstract': \"Large pre-trained multilingual models like mBERT, XLM-R achieve state of the art results on language understanding tasks. However, they are not well suited for latency critical applications on both servers and edge devices. It's important to reduce the memory and compute resources required by these models. To this end, we propose pQRNN, a projection-based embedding-free neural encoder that is tiny and effective for natural language processing tasks. Without pre-training, pQRNNs significantly outperform LSTM models with pre-trained embeddings despite being 140x smaller. With the same number of parameters, they outperform transformer baselines thereby showcasing their parameter efficiency. Additionally, we show that pQRNNs are effective student architectures for distilling large pre-trained language models. We perform careful ablations which study the effect of pQRNN parameters, data augmentation, and distillation settings. On MTOP, a challenging multilingual semantic parsing dataset, pQRNN students achieve 95.9\\\\% of the performance of an mBERT teacher while being 350x smaller. On mATIS, a popular parsing task, pQRNN students on average are able to get to 97.1\\\\% of the teacher while again being 350x smaller. Our strong results suggest that our approach is great for latency-sensitive applications while being able to leverage large mBERT-like models.\", 'authors': ['Melvin Johnson', 'Edward Li', 'Aditya Siddhant', 'Prabhu Kaliamoorthi'], 'published': '2021-01-21', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'scaling-federated-learning-for-fine-tuning-of', 'arxiv_id': '2102.00875', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2102.00875v1', 'url_pdf': 'https://arxiv.org/pdf/2102.00875v1.pdf', 'title': 'Scaling Federated Learning for Fine-tuning of Large Language Models', 'abstract': 'Federated learning (FL) is a promising approach to distributed compute, as well as distributed data, and provides a level of privacy and compliance to legal frameworks. This makes FL attractive for both consumer and healthcare applications. While the area is actively being explored, few studies have examined FL in the context of larger language models and there is a lack of comprehensive reviews of robustness across tasks, architectures, numbers of clients, and other relevant factors. In this paper, we explore the fine-tuning of Transformer-based language models in a federated learning setting. We evaluate three popular BERT-variants of different sizes (BERT, ALBERT, and DistilBERT) on a number of text classification tasks such as sentiment analysis and author identification. We perform an extensive sweep over the number of clients, ranging up to 32, to evaluate the impact of distributed compute on task performance in the federated averaging setting. While our findings suggest that the large sizes of the evaluated models are not generally prohibitive to federated training, we found that the different models handle federated averaging to a varying degree. Most notably, DistilBERT converges significantly slower with larger numbers of clients, and under some circumstances, even collapses to chance level performance. Investigating this issue presents an interesting perspective for future research.', 'authors': ['Olof Mogren', 'Edvin Listo Zec', 'Leon René Sütfeld', 'Matteo Barbieri', 'Sebastian Callh', 'Agrin Hilmkil'], 'published': '2021-02-01', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'understanding-the-capabilities-limitations', 'arxiv_id': '2102.02503', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2102.02503v1', 'url_pdf': 'https://arxiv.org/pdf/2102.02503v1.pdf', 'title': 'Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models', 'abstract': 'On October 14th, 2020, researchers from OpenAI, the Stanford Institute for Human-Centered Artificial Intelligence, and other universities convened to discuss open research questions surrounding GPT-3, the largest publicly-disclosed dense language model at the time. The meeting took place under Chatham House Rules. Discussants came from a variety of research backgrounds including computer science, linguistics, philosophy, political science, communications, cyber policy, and more. Broadly, the discussion centered around two main questions: 1) What are the technical capabilities and limitations of large language models? 2) What are the societal effects of widespread use of large language models? Here, we provide a detailed summary of the discussion organized by the two themes above.', 'authors': ['Deep Ganguli', 'Jack Clark', 'Miles Brundage', 'Alex Tamkin'], 'published': '2021-02-04', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'prompt-programming-for-large-language-models', 'arxiv_id': '2102.07350', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2102.07350v1', 'url_pdf': 'https://arxiv.org/pdf/2102.07350v1.pdf', 'title': 'Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm', 'abstract': \"Prevailing methods for mapping large generative language models to supervised tasks may fail to sufficiently probe models' novel capabilities. Using GPT-3 as a case study, we show that 0-shot prompts can significantly outperform few-shot prompts. We suggest that the function of few-shot examples in these cases is better described as locating an already learned task rather than meta-learning. This analysis motivates rethinking the role of prompts in controlling and evaluating powerful language models. In this work, we discuss methods of prompt programming, emphasizing the usefulness of considering prompts through the lens of natural language. We explore techniques for exploiting the capacity of narratives and cultural anchors to encode nuanced intentions and techniques for encouraging deconstruction of a problem into components before producing a verdict. Informed by this more encompassing theory of prompt programming, we also introduce the idea of a metaprompt that seeds the model to generate its own natural language prompts for a range of tasks. Finally, we discuss how these more general methods of interacting with language models can be incorporated into existing and future benchmarks and practical applications.\", 'authors': ['Kyle McDonell', 'Laria Reynolds'], 'published': '2021-02-15', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'attribute-alignment-controlling-text', 'arxiv_id': '2103.11070', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2103.11070v2', 'url_pdf': 'https://arxiv.org/pdf/2103.11070v2.pdf', 'title': 'Attribute Alignment: Controlling Text Generation from Pre-trained Language Models', 'abstract': 'Large language models benefit from training with a large amount of unlabeled text, which gives them increasingly fluent and diverse generation capabilities. However, using these models for text generation that takes into account target attributes, such as sentiment polarity or specific topics, remains a challenge. We propose a simple and flexible method for controlling text generation by aligning disentangled attribute representations. In contrast to recent efforts on training a discriminator to perturb the token level distribution for an attribute, we use the same data to learn an alignment function to guide the pre-trained, non-controlled language model to generate texts with the target attribute without changing the original language model parameters. We evaluate our method on sentiment- and topic-controlled generation, and show large performance gains over previous methods while retaining fluency and diversity.', 'authors': ['Kenji Sagae', 'Zhou Yu', 'Dian Yu'], 'published': '2021-03-20', 'conference': None, 'conference_url_abs': 'https://aclanthology.org/2021.findings-emnlp.194', 'conference_url_pdf': 'https://aclanthology.org/2021.findings-emnlp.194.pdf', 'proceeding': 'findings-emnlp-2021-11'}, {'id': 'detecting-hate-speech-with-gpt-3', 'arxiv_id': '2103.12407', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2103.12407v4', 'url_pdf': 'https://arxiv.org/pdf/2103.12407v4.pdf', 'title': 'Detecting Hate Speech with GPT-3', 'abstract': \"Sophisticated language models such as OpenAI's GPT-3 can generate hateful text that targets marginalized groups. Given this capacity, we are interested in whether large language models can be used to identify hate speech and classify text as sexist or racist. We use GPT-3 to identify sexist and racist text passages with zero-, one-, and few-shot learning. We find that with zero- and one-shot learning, GPT-3 can identify sexist or racist text with an average accuracy between 55 per cent and 67 per cent, depending on the category of text and type of learning. With few-shot learning, the model's accuracy can be as high as 85 per cent. Large language models have a role to play in hate speech detection, and with further development they could eventually be used to counter hate speech.\", 'authors': ['Annie Collins', 'Rohan Alexander', 'Ke-Li Chiu'], 'published': '2021-03-23', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'base-layers-simplifying-training-of-large', 'arxiv_id': '2103.16716', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2103.16716v1', 'url_pdf': 'https://arxiv.org/pdf/2103.16716v1.pdf', 'title': 'BASE Layers: Simplifying Training of Large, Sparse Models', 'abstract': 'We introduce a new balanced assignment of experts (BASE) layer for large language models that greatly simplifies existing high capacity sparse layers. Sparse layers can dramatically improve the efficiency of training and inference by routing each token to specialized expert modules that contain only a small fraction of the model parameters. However, it can be difficult to learn balanced routing functions that make full use of the available experts; existing approaches typically use routing heuristics or auxiliary expert-balancing loss functions. In contrast, we formulate token-to-expert allocation as a linear assignment problem, allowing an optimal assignment in which each expert receives an equal number of tokens. This optimal assignment scheme improves efficiency by guaranteeing balanced compute loads, and also simplifies training by not requiring any new hyperparameters or auxiliary losses. Code is publicly released at https://github.com/pytorch/fairseq/', 'authors': ['Luke Zettlemoyer', 'Naman Goyal', 'Tim Dettmers', 'Shruti Bhosale', 'Mike Lewis'], 'published': '2021-03-30', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'humor-iitk-at-semeval-2021-task-7-large', 'arxiv_id': '2104.00933', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2104.00933v1', 'url_pdf': 'https://arxiv.org/pdf/2104.00933v1.pdf', 'title': 'Humor@IITK at SemEval-2021 Task 7: Large Language Models for Quantifying Humor and Offensiveness', 'abstract': \"Humor and Offense are highly subjective due to multiple word senses, cultural knowledge, and pragmatic competence. Hence, accurately detecting humorous and offensive texts has several compelling use cases in Recommendation Systems and Personalized Content Moderation. However, due to the lack of an extensive labeled dataset, most prior works in this domain haven't explored large neural models for subjective humor understanding. This paper explores whether large neural models and their ensembles can capture the intricacies associated with humor/offense detection and rating. Our experiments on the SemEval-2021 Task 7: HaHackathon show that we can develop reasonable humor and offense detection systems with such models. Our models are ranked third in subtask 1b and consistently ranked around the top 33% of the leaderboard for the remaining subtasks.\", 'authors': ['Ashutosh Modi', 'Lakshay Tyagi', 'Bholeshwar Khurana', 'Avik Pal', 'Aishwarya Gupta'], 'published': '2021-04-02', 'conference': None, 'conference_url_abs': 'https://aclanthology.org/2021.semeval-1.36', 'conference_url_pdf': 'https://aclanthology.org/2021.semeval-1.36.pdf', 'proceeding': 'semeval-2021'}, {'id': 'efficient-large-scale-language-model-training', 'arxiv_id': '2104.04473', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2104.04473v5', 'url_pdf': 'https://arxiv.org/pdf/2104.04473v5.pdf', 'title': 'Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM', 'abstract': 'Large language models have led to state-of-the-art accuracies across a range of tasks. However, training these models efficiently is challenging for two reasons: a) GPU memory capacity is limited, making it impossible to fit large models on even a multi-GPU server, and b) the number of compute operations required to train these models can result in unrealistically long training times. Consequently, new methods of model parallelism such as tensor and pipeline parallelism have been proposed. Unfortunately, naive usage of these methods leads to fundamental scaling issues at thousands of GPUs, e.g., due to expensive cross-node communication or devices spending significant time waiting on other devices to make progress. In this paper, we show how different types of parallelism methods (tensor, pipeline, and data parallelism) can be composed to scale to thousands of GPUs and models with trillions of parameters. We survey techniques for pipeline parallelism and propose a novel interleaved pipeline parallelism schedule that can improve throughput by 10+% with memory footprint comparable to existing approaches. We quantitatively study the trade-offs between tensor, pipeline, and data parallelism, and provide intuition as to how to configure distributed training of a large model. Our approach allows us to perform training iterations on a model with 1 trillion parameters at 502 petaFLOP/s on 3072 GPUs with achieved per-GPU throughput of 52% of theoretical peak. Our code is open sourced at https://github.com/nvidia/megatron-lm.', 'authors': ['Vijay Anand Korthikanti', 'Matei Zaharia', 'Amar Phanishayee', 'Bryan Catanzaro', 'Julie Bernauer', 'Prethvi Kashinkunti', 'Dmitri Vainbrand', 'Mostofa Patwary', 'Patrick Legresley', 'Jared Casper', 'Mohammad Shoeybi', 'Deepak Narayanan'], 'published': '2021-04-09', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'an-efficient-2d-method-for-training-super', 'arxiv_id': '2104.05343', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2104.05343v1', 'url_pdf': 'https://arxiv.org/pdf/2104.05343v1.pdf', 'title': 'An Efficient 2D Method for Training Super-Large Deep Learning Models', 'abstract': 'Huge neural network models have shown unprecedented performance in real-world applications. However, due to memory constraints, model parallelism must be utilized to host large models that would otherwise not fit into the memory of a single device. Previous methods like Megatron partition the parameters of the entire model among multiple devices, while each device has to accommodate the redundant activations in forward and backward pass. In this work, we propose Optimus, a highly efficient and scalable 2D-partition paradigm of model parallelism that would facilitate the training of infinitely large language models. In Optimus, activations are partitioned and distributed among devices, further reducing redundancy. In terms of isoefficiency, Optimus significantly outperforms Megatron. On 64 GPUs of TACC Frontera, Optimus achieves 1.48X speedup for training, 1.78X speedup for inference, and 8X increase in maximum batch size over Megatron. Optimus surpasses Megatron in scaling efficiency by a great margin. The code is available at https://github.com/xuqifan897/Optimus.', 'authors': ['Yang You', 'Chaoyu Gong', 'Shenggui Li', 'Qifan Xu'], 'published': '2021-04-12', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'multilingual-language-models-predict-human', 'arxiv_id': '2104.05433', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2104.05433v1', 'url_pdf': 'https://arxiv.org/pdf/2104.05433v1.pdf', 'title': 'Multilingual Language Models Predict Human Reading Behavior', 'abstract': 'We analyze if large language models are able to predict patterns of human reading behavior. We compare the performance of language-specific and multilingual pretrained transformer models to predict reading time measures reflecting natural human sentence processing on Dutch, English, German, and Russian texts. This results in accurate models of human reading behavior, which indicates that transformer models implicitly encode relative importance in language in a way that is comparable to human processing mechanisms. We find that BERT and XLM models successfully predict a range of eye tracking features. In a series of experiments, we analyze the cross-domain and cross-language abilities of these models and show how they reflect human sentence processing.', 'authors': ['Lisa Beinborn', 'Lena Jäger', 'Ce Zhang', 'Federico Pirovano', 'Nora Hollenstein'], 'published': '2021-04-12', 'conference': None, 'conference_url_abs': 'https://aclanthology.org/2021.naacl-main.10', 'conference_url_pdf': 'https://aclanthology.org/2021.naacl-main.10.pdf', 'proceeding': 'naacl-2021-4'}, {'id': 'surface-form-competition-why-the-highest', 'arxiv_id': '2104.08315', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2104.08315v9', 'url_pdf': 'https://arxiv.org/pdf/2104.08315v9.pdf', 'title': \"Surface Form Competition: Why the Highest Probability Answer Isn't Always Right\", 'abstract': 'Large language models have shown promising results in zero-shot settings (Brown et al.,2020; Radford et al., 2019). For example, they can perform multiple choice tasks simply by conditioning on a question and selecting the answer with the highest probability. However, ranking by string probability can be problematic due to surface form competition-wherein different surface forms compete for probability mass, even if they represent the same underlying concept, e.g. \"computer\" and \"PC.\" Since probability mass is finite, this lowers the probability of the correct answer, due to competition from other strings that are valid answers (but not one of the multiple choice options). We introduce Domain Conditional Pointwise Mutual Information, an alternative scoring function that directly compensates for surface form competition by simply reweighing each option according to a term that is proportional to its a priori likelihood within the context of the specific zero-shot task. It achieves consistent gains in zero-shot performance over both calibrated (Zhao et al., 2021) and uncalibrated scoring functions on all GPT-2 and GPT-3 models over a variety of multiple choice datasets.', 'authors': ['Vered Shwartz', 'Luke Zettlemoyer', 'Yejin Choi', 'Peter West', 'Ari Holtzman'], 'published': '2021-04-16', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'transfer-training-from-smaller-language-model', 'arxiv_id': '2104.11390', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2104.11390v1', 'url_pdf': 'https://arxiv.org/pdf/2104.11390v1.pdf', 'title': 'Transfer training from smaller language model', 'abstract': 'Large language models have led to state-of-the-art accuracies across a range of tasks. However,training large language model needs massive computing resource, as more and more open source pre-training models are available, it is worthy to study how to take full advantage of available model. We find a method to save training time and resource cost by changing the small well-trained model to large model. We initialize a larger target model from a smaller source model by copy weight values from source model and padding with zeros or small initialization values on it to make the source and target model have approximate outputs, which is valid due to block matrix multiplication and residual connection in transformer structure. We test the target model on several data sets and find it is still comparable with the source model. When we continue training the target model, the training loss can start from a smaller value.', 'authors': ['Han Zhang'], 'published': '2021-04-23', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'phrase-break-prediction-with-bidirectional', 'arxiv_id': '2104.12395', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2104.12395v1', 'url_pdf': 'https://arxiv.org/pdf/2104.12395v1.pdf', 'title': 'Phrase break prediction with bidirectional encoder representations in Japanese text-to-speech synthesis', 'abstract': 'We propose a novel phrase break prediction method that combines implicit features extracted from a pre-trained large language model, a.k.a BERT, and explicit features extracted from BiLSTM with linguistic features. In conventional BiLSTM based methods, word representations and/or sentence representations are used as independent components. The proposed method takes account of both representations to extract the latent semantics, which cannot be captured by previous methods. The objective evaluation results show that the proposed method obtains an absolute improvement of 3.2 points for the F1 score compared with BiLSTM-based conventional methods using linguistic features. Moreover, the perceptual listening test results verify that a TTS system that applied our proposed method achieved a mean opinion score of 4.39 in prosody naturalness, which is highly competitive with the score of 4.37 for synthesized speech with ground-truth phrase breaks.', 'authors': ['Kentaro Tachibana', 'Ryuichi Yamamoto', 'Byeongseon Park', 'Kosuke Futamata'], 'published': '2021-04-26', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'predicting-the-q-factor-and-modal-volume-of', 'arxiv_id': '2104.12145', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2104.12145v2', 'url_pdf': 'https://arxiv.org/pdf/2104.12145v2.pdf', 'title': 'LLM helps design and optimize photonic crystal surface emitting lasers', 'abstract': \"Conventional design and optimization of Photonic Crystal Surface Emitting Lasers (PCSEL) usually requires expert knowledge in semiconductor physics and optimization algorithms, which is also known as the inverse design problem. However, with the trend towards automation and depersonalization of the entire integrated circuits (IC) industry, the conventional method, with the drawback of being relatively labor-intensive and sub-optimal, warrants further refinement. This technical dilemma remained until the emergence of Large Language Models (LLMs), such as OpenAI's ChatGPT and Google's Bard. This paper explores the possibility of applying LLMs to machine learning-based design and optimization of PCSELs. Specifically, we utilize GPT3.5 and GPT4. By simply having conversations, GPT assisted us with writing Finite Difference Time Domain (FDTD) simulation code and deep reinforcement learning code to acquire the optimized PCSEL solution, spanning from the proposition of ideas to the realization of algorithms. Given that GPT will perform better when given detailed and specific questions, we break down the PCSEL design problem into a series of sub-problems and converse with GPT by posing open-ended heuristic questions rather than definitive commands. This paper shows that LLMs, such as ChatGPT, can guide the nanophotonic design and optimization processes, on both the conceptual and technical level, and we propose new human-AI co-design strategies and show their practical implications. We achieve a significant milestone for the first step towards an automated end to end nanophotonic design and production pipeline.\", 'authors': [], 'published': '2021-04-25', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'evaluating-groundedness-in-dialogue-systems', 'arxiv_id': '2105.00071', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2105.00071v3', 'url_pdf': 'https://arxiv.org/pdf/2105.00071v3.pdf', 'title': 'Evaluating Attribution in Dialogue Systems: The BEGIN Benchmark', 'abstract': \"Knowledge-grounded dialogue systems powered by large language models often generate responses that, while fluent, are not attributable to a relevant source of information. Progress towards models that do not exhibit this issue requires evaluation metrics that can quantify its prevalence. To this end, we introduce the Benchmark for Evaluation of Grounded INteraction (BEGIN), comprised of 12k dialogue turns generated by neural dialogue systems trained on three knowledge-grounded dialogue corpora. We collect human annotations assessing the extent to which the models' responses can be attributed to the given background information. We then use BEGIN to analyze eight evaluation metrics. We find that these metrics rely on spurious correlations, do not reliably distinguish attributable abstractive responses from unattributable ones, and perform substantially worse when the knowledge source is longer. Our findings underscore the need for more sophisticated and robust evaluation metrics for knowledge-grounded dialogue. We make BEGIN publicly available at https://github.com/google/BEGIN-dataset.\", 'authors': ['David Reitter', 'Tal Linzen', 'Hannah Rashkin', 'Nouha Dziri'], 'published': '2021-04-30', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}]\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"for result in results:\n    print(f\"\\n Title: {result['title']} \")\n    print(f\"\\n Abstract: {result['abstract']}\")\n    print(\"\\n\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:51:01.174267Z","iopub.execute_input":"2025-04-22T04:51:01.174544Z","iopub.status.idle":"2025-04-22T04:51:01.182595Z","shell.execute_reply.started":"2025-04-22T04:51:01.174524Z","shell.execute_reply":"2025-04-22T04:51:01.181213Z"}},"outputs":[{"name":"stdout","text":"\n Title: Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer \n\n Abstract: The capacity of a neural network to absorb information is limited by its\nnumber of parameters. Conditional computation, where parts of the network are\nactive on a per-example basis, has been proposed in theory as a way of\ndramatically increasing model capacity without a proportional increase in\ncomputation. In practice, however, there are significant algorithmic and\nperformance challenges. In this work, we address these challenges and finally\nrealize the promise of conditional computation, achieving greater than 1000x\nimprovements in model capacity with only minor losses in computational\nefficiency on modern GPU clusters. We introduce a Sparsely-Gated\nMixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward\nsub-networks. A trainable gating network determines a sparse combination of\nthese experts to use for each example. We apply the MoE to the tasks of\nlanguage modeling and machine translation, where model capacity is critical for\nabsorbing the vast quantities of knowledge available in the training corpora.\nWe present model architectures in which a MoE with up to 137 billion parameters\nis applied convolutionally between stacked LSTM layers. On large language\nmodeling and machine translation benchmarks, these models achieve significantly\nbetter results than state-of-the-art at lower computational cost.\n\n\n\n\n Title: A Simple Way to Initialize Recurrent Networks of Rectified Linear Units \n\n Abstract: Learning long term dependencies in recurrent networks is difficult due to\nvanishing and exploding gradients. To overcome this difficulty, researchers\nhave developed sophisticated optimization techniques and network architectures.\nIn this paper, we propose a simpler solution that use recurrent neural networks\ncomposed of rectified linear units. Key to our solution is the use of the\nidentity matrix or its scaled version to initialize the recurrent weight\nmatrix. We find that our solution is comparable to LSTM on our four benchmarks:\ntwo toy problems involving long-range temporal structures, a large language\nmodeling problem and a benchmark speech recognition problem.\n\n\n\n\n Title: N-gram Counts and Language Models from the Common Crawl \n\n Abstract: We contribute 5-gram counts and language models trained on the Common Crawl corpus, a collection over 9 billion web pages. This release improves upon the Google n-gram counts in two key ways: the inclusion of low-count entries and deduplication to reduce boilerplate. By preserving singletons, we were able to use Kneser-Ney smoothing to build large language models. This paper describes how the corpus was processed with emphasis on the problems that arise in working with data at this scale. Our unpruned Kneser-Ney English {\\$}5{\\$}-gram language model, built on 975 billion deduplicated tokens, contains over 500 billion unique n-grams. We show gains of 0.5-1.4 BLEU by using large language models to translate into various languages.\n\n\n\n\n Title: Enhancing Clinical Concept Extraction with Contextual Embeddings \n\n Abstract: Neural network-based representations (\"embeddings\") have dramatically advanced natural language processing (NLP) tasks, including clinical NLP tasks such as concept extraction. Recently, however, more advanced embedding methods and representations (e.g., ELMo, BERT) have further pushed the state-of-the-art in NLP, yet there are no common best practices for how to integrate these representations into clinical tasks. The purpose of this study, then, is to explore the space of possible options in utilizing these new models for clinical concept extraction, including comparing these to traditional word embedding methods (word2vec, GloVe, fastText). Both off-the-shelf open-domain embeddings and pre-trained clinical embeddings from MIMIC-III are evaluated. We explore a battery of embedding methods consisting of traditional word embeddings and contextual embeddings, and compare these on four concept extraction corpora: i2b2 2010, i2b2 2012, SemEval 2014, and SemEval 2015. We also analyze the impact of the pre-training time of a large language model like ELMo or BERT on the extraction performance. Last, we present an intuitive way to understand the semantic information encoded by contextual embeddings. Contextual embeddings pre-trained on a large clinical corpus achieves new state-of-the-art performances across all concept extraction tasks. The best-performing model outperforms all state-of-the-art methods with respective F1-measures of 90.25, 93.18 (partial), 80.74, and 81.65. We demonstrate the potential of contextual embeddings through the state-of-the-art performance these methods achieve on clinical concept extraction. Additionally, we demonstrate contextual embeddings encode valuable semantic information not accounted for in traditional word representations.\n\n\n\n\n Title: Barack's Wife Hillary: Using Knowledge-Graphs for Fact-Aware Language Modeling \n\n Abstract: Modeling human language requires the ability to not only generate fluent text but also encode factual knowledge. However, traditional language models are only capable of remembering facts seen at training time, and often have difficulty recalling them. To address this, we introduce the knowledge graph language model (KGLM), a neural language model with mechanisms for selecting and copying facts from a knowledge graph that are relevant to the context. These mechanisms enable the model to render information it has never seen before, as well as generate out-of-vocabulary tokens. We also introduce the Linked WikiText-2 dataset, a corpus of annotated text aligned to the Wikidata knowledge graph whose contents (roughly) match the popular WikiText-2 benchmark. In experiments, we demonstrate that the KGLM achieves significantly better performance than a strong baseline language model. We additionally compare different language model's ability to complete sentences requiring factual knowledge, showing that the KGLM outperforms even very large language models in generating facts.\n\n\n\n\n Title: Release Strategies and the Social Impacts of Language Models \n\n Abstract: Large language models have a range of beneficial uses: they can assist in prose, poetry, and programming; analyze dataset biases; and more. However, their flexibility and generative capabilities also raise misuse concerns. This report discusses OpenAI's work related to the release of its GPT-2 language model. It discusses staged release, which allows time between model releases to conduct risk and benefit analyses as model sizes increased. It also discusses ongoing partnership-based research and provides recommendations for better coordination and responsible publication in AI.\n\n\n\n\n Title: Structured Pruning of Large Language Models \n\n Abstract: Large language models have recently achieved state of the art performance across a wide variety of natural language tasks. Meanwhile, the size of these models and their latency have significantly increased, which makes their usage costly, and raises an interesting question: do language models need to be large? We study this question through the lens of model compression. We present a generic, structured pruning approach by parameterizing each weight matrix using its low-rank factorization, and adaptively removing rank-1 components during training. On language modeling tasks, our structured approach outperforms other unstructured and block-structured pruning baselines at various compression levels, while achieving significant speedups during both training and inference. We also demonstrate that our method can be applied to pruning adaptive word embeddings in large language models, and to pruning the BERT model on several downstream fine-tuning classification benchmarks.\n\n\n\n\n Title: exBERT: A Visual Analysis Tool to Explore Learned Representations in Transformers Models \n\n Abstract: Large language models can produce powerful contextual representations that lead to improvements across many NLP tasks. Since these models are typically guided by a sequence of learned self attention mechanisms and may comprise undesired inductive biases, it is paramount to be able to explore what the attention has learned. While static analyses of these models lead to targeted insights, interactive tools are more dynamic and can help humans better gain an intuition for the model-internal reasoning process. We present exBERT, an interactive tool named after the popular BERT language model, that provides insights into the meaning of the contextual representations by matching a human-specified input to similar contexts in a large annotated dataset. By aggregating the annotations of the matching similar contexts, exBERT helps intuitively explain what each attention-head has learned.\n\n\n\n\n Title: Constrained Non-Affine Alignment of Embeddings \n\n Abstract: Embeddings are one of the fundamental building blocks for data analysis tasks. Embeddings are already essential tools for large language models and image analysis, and their use is being extended to many other research domains. The generation of these distributed representations is often a data- and computation-expensive process; yet the holistic analysis and adjustment of them after they have been created is still a developing area. In this paper, we first propose a very general quantitatively measure for the presence of features in the embedding data based on if it can be learned. We then devise a method to remove or alleviate undesired features in the embedding while retaining the essential structure of the data. We use a Domain Adversarial Network (DAN) to generate a non-affine transformation, but we add constraints to ensure the essential structure of the embedding is preserved. Our empirical results demonstrate that the proposed algorithm significantly outperforms the state-of-art unsupervised algorithm on several data sets, including novel applications from the industry.\n\n\n\n\n Title: Do Nuclear Submarines Have Nuclear Captains? A Challenge Dataset for Commonsense Reasoning over Adjectives and Objects \n\n Abstract: How do adjectives project from a noun to its parts? If a motorcycle is red, are its wheels red? Is a nuclear submarine{'}s captain nuclear? These questions are easy for humans to judge using our commonsense understanding of the world, but are difficult for computers. To attack this challenge, we crowdsource a set of human judgments that answer the English-language question {``}Given a whole described by an adjective, does the adjective also describe a given part?{''} We build strong baselines for this task with a classification approach. Our findings indicate that, despite the recent successes of large language models on tasks aimed to assess commonsense knowledge, these models do not greatly outperform simple word-level models based on pre-trained word embeddings. This provides evidence that the amount of commonsense knowledge encoded in these language models does not extend far beyond that already baked into the word embeddings. Our dataset will serve as a useful testbed for future research in commonsense reasoning, especially as it relates to adjectives and objects\n\n\n\n\n Title: Paraphrasing with Large Language Models \n\n Abstract: Recently, large language models such as GPT-2 have shown themselves to be extremely adept at text generation and have also been able to achieve high-quality results in many downstream NLP tasks such as text classification, sentiment analysis and question answering with the aid of fine-tuning. We present a useful technique for using a large language model to perform the task of paraphrasing on a variety of texts and subjects. Our approach is demonstrated to be capable of generating paraphrases not only at a sentence level but also for longer spans of text such as paragraphs without needing to break the text into smaller chunks.\n\n\n\n\n Title: WaLDORf: Wasteless Language-model Distillation On Reading-comprehension \n\n Abstract: Transformer based Very Large Language Models (VLLMs) like BERT, XLNet and RoBERTa, have recently shown tremendous performance on a large variety of Natural Language Understanding (NLU) tasks. However, due to their size, these VLLMs are extremely resource intensive and cumbersome to deploy at production time. Several recent publications have looked into various ways to distil knowledge from a transformer based VLLM (most commonly BERT-Base) into a smaller model which can run much faster at inference time. Here, we propose a novel set of techniques which together produce a task-specific hybrid convolutional and transformer model, WaLDORf, that achieves state-of-the-art inference speed while still being more accurate than previous distilled models.\n\n\n\n\n Title: On the comparability of Pre-trained Language Models \n\n Abstract: Recent developments in unsupervised representation learning have successfully established the concept of transfer learning in NLP. Mainly three forces are driving the improvements in this area of research: More elaborated architectures are making better use of contextual information. Instead of simply plugging in static pre-trained representations, these are learned based on surrounding context in end-to-end trainable models with more intelligently designed language modelling objectives. Along with this, larger corpora are used as resources for pre-training large language models in a self-supervised fashion which are afterwards fine-tuned on supervised tasks. Advances in parallel computing as well as in cloud computing, made it possible to train these models with growing capacities in the same or even in shorter time than previously established models. These three developments agglomerate in new state-of-the-art (SOTA) results being revealed in a higher and higher frequency. It is not always obvious where these improvements originate from, as it is not possible to completely disentangle the contributions of the three driving forces. We set ourselves to providing a clear and concise overview on several large pre-trained language models, which achieved SOTA results in the last two years, with respect to their use of new architectures and resources. We want to clarify for the reader where the differences between the models are and we furthermore attempt to gain some insight into the single contributions of lexical/computational improvements as well as of architectural changes. We explicitly do not intend to quantify these contributions, but rather see our work as an overview in order to identify potential starting points for benchmark comparisons. Furthermore, we tentatively want to point at potential possibilities for improvement in the field of open-sourcing and reproducible research.\n\n\n\n\n Title: Compressing Language Models using Doped Kronecker Products \n\n Abstract: Kronecker Products (KP) have been used to compress IoT RNN Applications by 15-38x compression factors, achieving better results than traditional compression methods. However when KP is applied to large Natural Language Processing tasks, it leads to significant accuracy loss (approx 26%). This paper proposes a way to recover accuracy otherwise lost when applying KP to large NLP tasks, by allowing additional degrees of freedom in the KP matrix. More formally, we propose doping, a process of adding an extremely sparse overlay matrix on top of the pre-defined KP structure. We call this compression method doped kronecker product compression. To train these models, we present a new solution to the phenomenon of co-matrix adaption (CMA), which uses a new regularization scheme called co matrix dropout regularization (CMR). We present experimental results that demonstrate compression of a large language model with LSTM layers of size 25 MB by 25x with 1.4% loss in perplexity score. At 25x compression, an equivalent pruned network leads to 7.9% loss in perplexity score, while HMD and LMF lead to 15% and 27% loss in perplexity score respectively.\n\n\n\n\n Title: Explaining Relationships Between Scientific Documents \n\n Abstract: We address the task of explaining relationships between two scientific documents using natural language text. This task requires modeling the complex content of long technical documents, deducing a relationship between these documents, and expressing the details of that relationship in text. In addition to the theoretical interest of this task, successful solutions can help improve researcher efficiency in search and review. In this paper we establish a dataset of 622K examples from 154K documents. We pretrain a large language model to serve as the foundation for autoregressive approaches to the task. We explore the impact of taking different views on the two documents, including the use of dense representations extracted with scientific IE systems. We provide extensive automatic and human evaluations which show the promise of such models, but make clear challenges for future work.\n\n\n\n\n Title: Training Question Answering Models From Synthetic Data \n\n Abstract: Question and answer generation is a data augmentation method that aims to improve question answering (QA) models given the limited amount of human labeled data. However, a considerable gap remains between synthetic and human-generated question-answer pairs. This work aims to narrow this gap by taking advantage of large language models and explores several factors such as model size, quality of pretrained models, scale of data synthesized, and algorithmic choices. On the SQuAD1.1 question answering task, we achieve higher accuracy using solely synthetic questions and answers than when using the SQuAD1.1 training set questions alone. Removing access to real Wikipedia data, we synthesize questions and answers from a synthetic corpus generated by an 8.3 billion parameter GPT-2 model. With no access to human supervision and only access to other models, we are able to train state of the art question answering networks on entirely model-generated data that achieve 88.4 Exact Match (EM) and 93.9 F1 score on the SQuAD1.1 dev set. We further apply our methodology to SQuAD2.0 and show a 2.8 absolute gain on EM score compared to prior work using synthetic data.\n\n\n\n\n Title: Residual Energy-Based Models for Text Generation \n\n Abstract: Text generation is ubiquitous in many NLP tasks, from summarization, to dialogue and machine translation. The dominant parametric approach is based on locally normalized models which predict one word at a time. While these work remarkably well, they are plagued by exposure bias due to the greedy nature of the generation process. In this work, we investigate un-normalized energy-based models (EBMs) which operate not at the token but at the sequence level. In order to make training tractable, we first work in the residual of a pretrained locally normalized language model and second we train using noise contrastive estimation. Furthermore, since the EBM works at the sequence level, we can leverage pretrained bi-directional contextual representations, such as BERT and RoBERTa. Our experiments on two large language modeling datasets show that residual EBMs yield lower perplexity compared to locally normalized baselines. Moreover, generation via importance sampling is very efficient and of higher quality than the baseline models according to human evaluation.\n\n\n\n\n Title: Challenge Closed-book Science Exam: A Meta-learning Based Question Answering System \n\n Abstract: Prior work in standardized science exams requires support from large text corpus, such as targeted science corpus fromWikipedia or SimpleWikipedia. However, retrieving knowledge from the large corpus is time-consuming and questions embedded in complex semantic representation may interfere with retrieval. Inspired by the dual process theory in cognitive science, we propose a MetaQA framework, where system 1 is an intuitive meta-classifier and system 2 is a reasoning module. Specifically, our method based on meta-learning method and large language model BERT, which can efficiently solve science problems by learning from related example questions without relying on external knowledge bases. We evaluate our method on AI2 Reasoning Challenge (ARC), and the experimental results show that meta-classifier yields considerable classification performance on emerging question types. The information provided by meta-classifier significantly improves the accuracy of reasoning module from 46.6% to 64.2%, which has a competitive advantage over retrieval-based QA methods.\n\n\n\n\n Title: The curious case of developmental BERTology: On sparsity, transfer learning, generalization and the brain \n\n Abstract: In this essay, we explore a point of intersection between deep learning and neuroscience, through the lens of large language models, transfer learning and network compression. Just like perceptual and cognitive neurophysiology has inspired effective deep neural network architectures which in turn make a useful model for understanding the brain, here we explore how biological neural development might inspire efficient and robust optimization procedures which in turn serve as a useful model for the maturation and aging of the brain.\n\n\n\n\n Title: Controlling Dialogue Generation with Semantic Exemplars \n\n Abstract: Dialogue systems pretrained with large language models generate locally coherent responses, but lack the fine-grained control over responses necessary to achieve specific goals. A promising method to control response generation is exemplar-based generation, in which models edit exemplar responses that are retrieved from training data, or hand-written to strategically address discourse-level goals, to fit new dialogue contexts. But, current exemplar-based approaches often excessively copy words from the exemplar responses, leading to incoherent replies. We present an Exemplar-based Dialogue Generation model, EDGE, that uses the semantic frames present in exemplar responses to guide generation. We show that controlling dialogue generation based on the semantic frames of exemplars, rather than words in the exemplar itself, improves the coherence of generated responses, while preserving semantic meaning and conversation goals present in exemplar responses.\n\n\n\n\n Title: Content Planning for Neural Story Generation with Aristotelian Rescoring \n\n Abstract: Long-form narrative text generated from large language models manages a fluent impersonation of human writing, but only at the local sentence level, and lacks structure or global cohesion. We posit that many of the problems of story generation can be addressed via high-quality content planning, and present a system that focuses on how to learn good plot structures to guide story generation. We utilize a plot-generation language model along with an ensemble of rescoring models that each implement an aspect of good story-writing as detailed in Aristotle's Poetics. We find that stories written with our more principled plot-structure are both more relevant to a given prompt and higher quality than baselines that do not content plan, or that plan in an unprincipled way.\n\n\n\n\n Title: MEGATRON-CNTRL: Controllable Story Generation with External Knowledge Using Large-Scale Language Models \n\n Abstract: Existing pre-trained large language models have shown unparalleled generative capabilities. However, they are not controllable. In this paper, we propose MEGATRON-CNTRL, a novel framework that uses large-scale language models and adds control to text generation by incorporating an external knowledge base. Our framework consists of a keyword predictor, a knowledge retriever, a contextual knowledge ranker, and a conditional text generator. As we do not have access to ground-truth supervision for the knowledge ranker, we make use of weak supervision from sentence embedding. The empirical results show that our model generates more fluent, consistent, and coherent stories with less repetition and higher diversity compared to prior work on the ROC story dataset. We showcase the controllability of our model by replacing the keywords used to generate stories and re-running the generation process. Human evaluation results show that 77.5% of these stories are successfully controlled by the new keywords. Furthermore, by scaling our model from 124 million to 8.3 billion parameters we demonstrate that larger models improve both the quality of generation (from 74.5% to 93.0% for consistency) and controllability (from 77.5% to 91.5%).\n\n\n\n\n Title: An Empirical Investigation Towards Efficient Multi-Domain Language Model Pre-training \n\n Abstract: Pre-training large language models has become a standard in the natural language processing community. Such models are pre-trained on generic data (e.g. BookCorpus and English Wikipedia) and often fine-tuned on tasks in the same domain. However, in order to achieve state-of-the-art performance on out of domain tasks such as clinical named entity recognition and relation extraction, additional in domain pre-training is required. In practice, staged multi-domain pre-training presents performance deterioration in the form of catastrophic forgetting (CF) when evaluated on a generic benchmark such as GLUE. In this paper we conduct an empirical investigation into known methods to mitigate CF. We find that elastic weight consolidation provides best overall scores yielding only a 0.33% drop in performance across seven generic tasks while remaining competitive in bio-medical tasks. Furthermore, we explore gradient and latent clustering based data selection techniques to improve coverage when using elastic weight consolidation and experience replay methods.\n\n\n\n\n Title: DACT-BERT: Increasing the efficiency and interpretability of BERT by using adaptive computation time. \n\n Abstract: Large-scale pre-trained language models have shown remarkable results in diverse NLP applications. Unfortunately, these performance gains have been accompanied by a significant increase in computation time and model size, stressing the need to develop new or complementary strategies to increase the efficiency and interpretability of current large language models, such as BERT. In this paper we propose DACT-BERT, a differentiable adaptive computation time strategy for BERT language model. DACT-BERT adds an adaptive computation mechanism to the regular processing pipeline of BERT. This mechanism controls the number of transformer blocks that BERT needs to execute at inference time. By doing this, the model makes predictions based on the most appropriate intermediate representations for the task encoded by the pre-trained weights. With respect to previous works, DACT-BERT has the advantage of being fully differentiable and directly integrated to BERT's main processing pipeline. This enables the incorporation of gradient-based transparency mechanisms to improve interpretability. Furthermore, by discarding useless steps, DACT-BERT facilitates the understanding of the underlying process used by BERT to reach an inference. Our experiments demonstrate that our approach is effective in significantly reducing computational complexity without affecting model accuracy. Additionally, they also demonstrate that DACT-BERT helps to improve model interpretability. \n\n\n\n\n Title: Modeling Preconditions in Text with a Crowd-sourced Dataset \n\n Abstract: Preconditions provide a form of logical connection between events that explains why some events occur together and information that is complementary to the more widely studied relations such as causation, temporal ordering, entailment, and discourse relations. Modeling preconditions in text has been hampered in part due to the lack of large scale labeled data grounded in text. This paper introduces PeKo, a crowd-sourced annotation of preconditions between event pairs in newswire, an order of magnitude larger than prior text annotations. To complement this new corpus, we also introduce two challenge tasks aimed at modeling preconditions: (i) Precondition Identification -- a standard classification task defined over pairs of event mentions, and (ii) Precondition Generation -- a generative task aimed at testing a more general ability to reason about a given event. Evaluation on both tasks shows that modeling preconditions is challenging even for today's large language models (LM). This suggests that precondition knowledge is not easily accessible in LM-derived representations alone. Our generation results show that fine-tuning an LM on PeKo yields better conditional relations than when trained on raw text or temporally-ordered corpora.\n\n\n\n\n Title: Plug-and-Play Conversational Models \n\n Abstract: There has been considerable progress made towards conversational models that generate coherent and fluent responses; however, this often involves training large language models on large dialogue datasets, such as Reddit. These large conversational models provide little control over the generated responses, and this control is further limited in the absence of annotated conversational datasets for attribute specific generation that can be used for fine-tuning the model. In this paper, we first propose and evaluate plug-and-play methods for controllable response generation, which does not require dialogue specific datasets and does not rely on fine-tuning a large model. While effective, the decoding procedure induces considerable computational overhead, rendering the conversational model unsuitable for interactive usage. To overcome this, we introduce an approach that does not require further computation at decoding time, while also does not require any fine-tuning of a large language model. We demonstrate, through extensive automatic and human evaluation, a high degree of control over the generated conversational responses with regard to multiple desired attributes, while being fluent.\n\n\n\n\n Title: Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning \n\n Abstract: State-of-the-art natural language understanding classification models follow two-stages: pre-training a large language model on an auxiliary task, and then fine-tuning the model on a task-specific labeled dataset using cross-entropy loss. However, the cross-entropy loss has several shortcomings that can lead to sub-optimal generalization and instability. Driven by the intuition that good generalization requires capturing the similarity between examples in one class and contrasting them with examples in other classes, we propose a supervised contrastive learning (SCL) objective for the fine-tuning stage. Combined with cross-entropy, our proposed SCL loss obtains significant improvements over a strong RoBERTa-Large baseline on multiple datasets of the GLUE benchmark in few-shot learning settings, without requiring specialized architecture, data augmentations, memory banks, or additional unsupervised data. Our proposed fine-tuning objective leads to models that are more robust to different levels of noise in the fine-tuning training data, and can generalize better to related tasks with limited labeled data.\n\n\n\n\n Title: Extracting Training Data from Large Language Models \n\n Abstract: It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model. We demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data. We comprehensively evaluate our extraction attack to understand the factors that contribute to its success. Worryingly, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models.\n\n\n\n\n Title: A review of on-device fully neural end-to-end automatic speech recognition algorithms \n\n Abstract: In this paper, we review various end-to-end automatic speech recognition algorithms and their optimization techniques for on-device applications. Conventional speech recognition systems comprise a large number of discrete components such as an acoustic model, a language model, a pronunciation model, a text-normalizer, an inverse-text normalizer, a decoder based on a Weighted Finite State Transducer (WFST), and so on. To obtain sufficiently high speech recognition accuracy with such conventional speech recognition systems, a very large language model (up to 100 GB) is usually needed. Hence, the corresponding WFST size becomes enormous, which prohibits their on-device implementation. Recently, fully neural network end-to-end speech recognition algorithms have been proposed. Examples include speech recognition systems based on Connectionist Temporal Classification (CTC), Recurrent Neural Network Transducer (RNN-T), Attention-based Encoder-Decoder models (AED), Monotonic Chunk-wise Attention (MoChA), transformer-based speech recognition systems, and so on. These fully neural network-based systems require much smaller memory footprints compared to conventional algorithms, therefore their on-device implementation has become feasible. In this paper, we review such end-to-end speech recognition models. We extensively discuss their structures, performance, and advantages compared to conventional algorithms.\n\n\n\n\n Title: Evolution Is All You Need: Phylogenetic Augmentation for Contrastive Learning \n\n Abstract: Self-supervised representation learning of biological sequence embeddings alleviates computational resource constraints on downstream tasks while circumventing expensive experimental label acquisition. However, existing methods mostly borrow directly from large language models designed for NLP, rather than with bioinformatics philosophies in mind. Recently, contrastive mutual information maximization methods have achieved state-of-the-art representations for ImageNet. In this perspective piece, we discuss how viewing evolution as natural sequence augmentation and maximizing information across phylogenetic \"noisy channels\" is a biologically and theoretically desirable objective for pretraining encoders. We first provide a review of current contrastive learning literature, then provide an illustrative example where we show that contrastive learning using evolutionary augmentation can be used as a representation learning objective which maximizes the mutual information between biological sequences and their conserved function, and finally outline rationale for this approach.\n\n\n\n\n Title: KM-BART: Knowledge Enhanced Multimodal BART for Visual Commonsense Generation \n\n Abstract: We present Knowledge Enhanced Multimodal BART (KM-BART), which is a Transformer-based sequence-to-sequence model capable of reasoning about commonsense knowledge from multimodal inputs of images and texts. We adapt the generative BART architecture to a multimodal model with visual and textual inputs. We further develop novel pretraining tasks to improve the model performance on the Visual Commonsense Generation (VCG) task. In particular, our pretraining task of Knowledge-based Commonsense Generation (KCG) boosts model performance on the VCG task by leveraging commonsense knowledge from a large language model pretrained on external commonsense knowledge graphs. To the best of our knowledge, we are the first to propose a dedicated task for improving model performance on the VCG task. Experimental results show that our model reaches state-of-the-art performance on the VCG task by applying these novel pretraining tasks.\n\n\n\n\n Title: Analyzing Commonsense Emergence in Few-shot Knowledge Models \n\n Abstract: Recently, commonsense knowledge models - pretrained language models (LM) fine-tuned on knowledge graph (KG) tuples - showed that considerable amounts of commonsense knowledge can be encoded in the parameters of large language models. However, as parallel studies show that LMs are poor hypothesizers of declarative commonsense relationships on their own, it remains unclear whether this knowledge is learned during pretraining or from fine-tuning on KG examples. To investigate this question, we train commonsense knowledge models in few-shot settings to study the emergence of their commonsense representation abilities. Our results show that commonsense knowledge models can rapidly adapt from limited examples, indicating that KG fine-tuning serves to learn an interface to encoded knowledge learned during pretraining. Importantly, our analysis of absolute, angular, and distributional parameter changes during few-shot fine-tuning provides novel insights into how this interface is learned.\n\n\n\n\n Title: Graphmax for Text Generation \n\n Abstract: In text generation, a large language model (LM) makes a choice of each new word based only on the former selection of its context using the softmax function. Nevertheless, the link statistics information of concurrent words based on a scene-specific corpus is valuable in choosing the next word, which can help to ensure the topic of the generated text to be aligned with the current task. To fully explore the co-occurrence information,we propose a graphmax function for task-specific text generation. Using the graph-based regularization, graphmax enables the final word choice to be determined by both the global knowledge from the LM and the local knowledge from the scene-specific corpus. The traditional softmax function is regularized with a graph total variation (GTV) term, which incorporates the local knowledge into the LM and encourages the model to consider the statistical relationships between words in a scene-specific corpus. The proposed graphmax is versatile and can be readily plugged into any large pre-trained LM for text generation and machine translation. Through extensive experiments, we demonstrate that the new GTV-based regularization can improve performances in various natural language processing tasks in comparison with existing methods. Moreover, through human experiments, we observe that participants can easily distinguish the text generated by graphmax or softmax.\n\n\n\n\n Title: Persistent Anti-Muslim Bias in Large Language Models \n\n Abstract: It has been observed that large-scale language models capture undesirable societal biases, e.g. relating to race and gender; yet religious bias has been relatively unexplored. We demonstrate that GPT-3, a state-of-the-art contextual language model, captures persistent Muslim-violence bias. We probe GPT-3 in various ways, including prompt completion, analogical reasoning, and story generation, to understand this anti-Muslim bias, demonstrating that it appears consistently and creatively in different uses of the model and that it is severe even compared to biases about other religious groups. For instance, \"Muslim\" is analogized to \"terrorist\" in 23% of test cases, while \"Jewish\" is mapped to \"money\" in 5% of test cases. We quantify the positive distraction needed to overcome this bias with adversarial text prompts, and find that use of the most positive 6 adjectives reduces violent completions for \"Muslims\" from 66% to 20%, but which is still higher than for other religious groups.\n\n\n\n\n Title: Distilling Large Language Models into Tiny and Effective Students using pQRNN \n\n Abstract: Large pre-trained multilingual models like mBERT, XLM-R achieve state of the art results on language understanding tasks. However, they are not well suited for latency critical applications on both servers and edge devices. It's important to reduce the memory and compute resources required by these models. To this end, we propose pQRNN, a projection-based embedding-free neural encoder that is tiny and effective for natural language processing tasks. Without pre-training, pQRNNs significantly outperform LSTM models with pre-trained embeddings despite being 140x smaller. With the same number of parameters, they outperform transformer baselines thereby showcasing their parameter efficiency. Additionally, we show that pQRNNs are effective student architectures for distilling large pre-trained language models. We perform careful ablations which study the effect of pQRNN parameters, data augmentation, and distillation settings. On MTOP, a challenging multilingual semantic parsing dataset, pQRNN students achieve 95.9\\% of the performance of an mBERT teacher while being 350x smaller. On mATIS, a popular parsing task, pQRNN students on average are able to get to 97.1\\% of the teacher while again being 350x smaller. Our strong results suggest that our approach is great for latency-sensitive applications while being able to leverage large mBERT-like models.\n\n\n\n\n Title: Scaling Federated Learning for Fine-tuning of Large Language Models \n\n Abstract: Federated learning (FL) is a promising approach to distributed compute, as well as distributed data, and provides a level of privacy and compliance to legal frameworks. This makes FL attractive for both consumer and healthcare applications. While the area is actively being explored, few studies have examined FL in the context of larger language models and there is a lack of comprehensive reviews of robustness across tasks, architectures, numbers of clients, and other relevant factors. In this paper, we explore the fine-tuning of Transformer-based language models in a federated learning setting. We evaluate three popular BERT-variants of different sizes (BERT, ALBERT, and DistilBERT) on a number of text classification tasks such as sentiment analysis and author identification. We perform an extensive sweep over the number of clients, ranging up to 32, to evaluate the impact of distributed compute on task performance in the federated averaging setting. While our findings suggest that the large sizes of the evaluated models are not generally prohibitive to federated training, we found that the different models handle federated averaging to a varying degree. Most notably, DistilBERT converges significantly slower with larger numbers of clients, and under some circumstances, even collapses to chance level performance. Investigating this issue presents an interesting perspective for future research.\n\n\n\n\n Title: Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models \n\n Abstract: On October 14th, 2020, researchers from OpenAI, the Stanford Institute for Human-Centered Artificial Intelligence, and other universities convened to discuss open research questions surrounding GPT-3, the largest publicly-disclosed dense language model at the time. The meeting took place under Chatham House Rules. Discussants came from a variety of research backgrounds including computer science, linguistics, philosophy, political science, communications, cyber policy, and more. Broadly, the discussion centered around two main questions: 1) What are the technical capabilities and limitations of large language models? 2) What are the societal effects of widespread use of large language models? Here, we provide a detailed summary of the discussion organized by the two themes above.\n\n\n\n\n Title: Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm \n\n Abstract: Prevailing methods for mapping large generative language models to supervised tasks may fail to sufficiently probe models' novel capabilities. Using GPT-3 as a case study, we show that 0-shot prompts can significantly outperform few-shot prompts. We suggest that the function of few-shot examples in these cases is better described as locating an already learned task rather than meta-learning. This analysis motivates rethinking the role of prompts in controlling and evaluating powerful language models. In this work, we discuss methods of prompt programming, emphasizing the usefulness of considering prompts through the lens of natural language. We explore techniques for exploiting the capacity of narratives and cultural anchors to encode nuanced intentions and techniques for encouraging deconstruction of a problem into components before producing a verdict. Informed by this more encompassing theory of prompt programming, we also introduce the idea of a metaprompt that seeds the model to generate its own natural language prompts for a range of tasks. Finally, we discuss how these more general methods of interacting with language models can be incorporated into existing and future benchmarks and practical applications.\n\n\n\n\n Title: Attribute Alignment: Controlling Text Generation from Pre-trained Language Models \n\n Abstract: Large language models benefit from training with a large amount of unlabeled text, which gives them increasingly fluent and diverse generation capabilities. However, using these models for text generation that takes into account target attributes, such as sentiment polarity or specific topics, remains a challenge. We propose a simple and flexible method for controlling text generation by aligning disentangled attribute representations. In contrast to recent efforts on training a discriminator to perturb the token level distribution for an attribute, we use the same data to learn an alignment function to guide the pre-trained, non-controlled language model to generate texts with the target attribute without changing the original language model parameters. We evaluate our method on sentiment- and topic-controlled generation, and show large performance gains over previous methods while retaining fluency and diversity.\n\n\n\n\n Title: Detecting Hate Speech with GPT-3 \n\n Abstract: Sophisticated language models such as OpenAI's GPT-3 can generate hateful text that targets marginalized groups. Given this capacity, we are interested in whether large language models can be used to identify hate speech and classify text as sexist or racist. We use GPT-3 to identify sexist and racist text passages with zero-, one-, and few-shot learning. We find that with zero- and one-shot learning, GPT-3 can identify sexist or racist text with an average accuracy between 55 per cent and 67 per cent, depending on the category of text and type of learning. With few-shot learning, the model's accuracy can be as high as 85 per cent. Large language models have a role to play in hate speech detection, and with further development they could eventually be used to counter hate speech.\n\n\n\n\n Title: BASE Layers: Simplifying Training of Large, Sparse Models \n\n Abstract: We introduce a new balanced assignment of experts (BASE) layer for large language models that greatly simplifies existing high capacity sparse layers. Sparse layers can dramatically improve the efficiency of training and inference by routing each token to specialized expert modules that contain only a small fraction of the model parameters. However, it can be difficult to learn balanced routing functions that make full use of the available experts; existing approaches typically use routing heuristics or auxiliary expert-balancing loss functions. In contrast, we formulate token-to-expert allocation as a linear assignment problem, allowing an optimal assignment in which each expert receives an equal number of tokens. This optimal assignment scheme improves efficiency by guaranteeing balanced compute loads, and also simplifies training by not requiring any new hyperparameters or auxiliary losses. Code is publicly released at https://github.com/pytorch/fairseq/\n\n\n\n\n Title: Humor@IITK at SemEval-2021 Task 7: Large Language Models for Quantifying Humor and Offensiveness \n\n Abstract: Humor and Offense are highly subjective due to multiple word senses, cultural knowledge, and pragmatic competence. Hence, accurately detecting humorous and offensive texts has several compelling use cases in Recommendation Systems and Personalized Content Moderation. However, due to the lack of an extensive labeled dataset, most prior works in this domain haven't explored large neural models for subjective humor understanding. This paper explores whether large neural models and their ensembles can capture the intricacies associated with humor/offense detection and rating. Our experiments on the SemEval-2021 Task 7: HaHackathon show that we can develop reasonable humor and offense detection systems with such models. Our models are ranked third in subtask 1b and consistently ranked around the top 33% of the leaderboard for the remaining subtasks.\n\n\n\n\n Title: Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM \n\n Abstract: Large language models have led to state-of-the-art accuracies across a range of tasks. However, training these models efficiently is challenging for two reasons: a) GPU memory capacity is limited, making it impossible to fit large models on even a multi-GPU server, and b) the number of compute operations required to train these models can result in unrealistically long training times. Consequently, new methods of model parallelism such as tensor and pipeline parallelism have been proposed. Unfortunately, naive usage of these methods leads to fundamental scaling issues at thousands of GPUs, e.g., due to expensive cross-node communication or devices spending significant time waiting on other devices to make progress. In this paper, we show how different types of parallelism methods (tensor, pipeline, and data parallelism) can be composed to scale to thousands of GPUs and models with trillions of parameters. We survey techniques for pipeline parallelism and propose a novel interleaved pipeline parallelism schedule that can improve throughput by 10+% with memory footprint comparable to existing approaches. We quantitatively study the trade-offs between tensor, pipeline, and data parallelism, and provide intuition as to how to configure distributed training of a large model. Our approach allows us to perform training iterations on a model with 1 trillion parameters at 502 petaFLOP/s on 3072 GPUs with achieved per-GPU throughput of 52% of theoretical peak. Our code is open sourced at https://github.com/nvidia/megatron-lm.\n\n\n\n\n Title: An Efficient 2D Method for Training Super-Large Deep Learning Models \n\n Abstract: Huge neural network models have shown unprecedented performance in real-world applications. However, due to memory constraints, model parallelism must be utilized to host large models that would otherwise not fit into the memory of a single device. Previous methods like Megatron partition the parameters of the entire model among multiple devices, while each device has to accommodate the redundant activations in forward and backward pass. In this work, we propose Optimus, a highly efficient and scalable 2D-partition paradigm of model parallelism that would facilitate the training of infinitely large language models. In Optimus, activations are partitioned and distributed among devices, further reducing redundancy. In terms of isoefficiency, Optimus significantly outperforms Megatron. On 64 GPUs of TACC Frontera, Optimus achieves 1.48X speedup for training, 1.78X speedup for inference, and 8X increase in maximum batch size over Megatron. Optimus surpasses Megatron in scaling efficiency by a great margin. The code is available at https://github.com/xuqifan897/Optimus.\n\n\n\n\n Title: Multilingual Language Models Predict Human Reading Behavior \n\n Abstract: We analyze if large language models are able to predict patterns of human reading behavior. We compare the performance of language-specific and multilingual pretrained transformer models to predict reading time measures reflecting natural human sentence processing on Dutch, English, German, and Russian texts. This results in accurate models of human reading behavior, which indicates that transformer models implicitly encode relative importance in language in a way that is comparable to human processing mechanisms. We find that BERT and XLM models successfully predict a range of eye tracking features. In a series of experiments, we analyze the cross-domain and cross-language abilities of these models and show how they reflect human sentence processing.\n\n\n\n\n Title: Surface Form Competition: Why the Highest Probability Answer Isn't Always Right \n\n Abstract: Large language models have shown promising results in zero-shot settings (Brown et al.,2020; Radford et al., 2019). For example, they can perform multiple choice tasks simply by conditioning on a question and selecting the answer with the highest probability. However, ranking by string probability can be problematic due to surface form competition-wherein different surface forms compete for probability mass, even if they represent the same underlying concept, e.g. \"computer\" and \"PC.\" Since probability mass is finite, this lowers the probability of the correct answer, due to competition from other strings that are valid answers (but not one of the multiple choice options). We introduce Domain Conditional Pointwise Mutual Information, an alternative scoring function that directly compensates for surface form competition by simply reweighing each option according to a term that is proportional to its a priori likelihood within the context of the specific zero-shot task. It achieves consistent gains in zero-shot performance over both calibrated (Zhao et al., 2021) and uncalibrated scoring functions on all GPT-2 and GPT-3 models over a variety of multiple choice datasets.\n\n\n\n\n Title: Transfer training from smaller language model \n\n Abstract: Large language models have led to state-of-the-art accuracies across a range of tasks. However,training large language model needs massive computing resource, as more and more open source pre-training models are available, it is worthy to study how to take full advantage of available model. We find a method to save training time and resource cost by changing the small well-trained model to large model. We initialize a larger target model from a smaller source model by copy weight values from source model and padding with zeros or small initialization values on it to make the source and target model have approximate outputs, which is valid due to block matrix multiplication and residual connection in transformer structure. We test the target model on several data sets and find it is still comparable with the source model. When we continue training the target model, the training loss can start from a smaller value.\n\n\n\n\n Title: Phrase break prediction with bidirectional encoder representations in Japanese text-to-speech synthesis \n\n Abstract: We propose a novel phrase break prediction method that combines implicit features extracted from a pre-trained large language model, a.k.a BERT, and explicit features extracted from BiLSTM with linguistic features. In conventional BiLSTM based methods, word representations and/or sentence representations are used as independent components. The proposed method takes account of both representations to extract the latent semantics, which cannot be captured by previous methods. The objective evaluation results show that the proposed method obtains an absolute improvement of 3.2 points for the F1 score compared with BiLSTM-based conventional methods using linguistic features. Moreover, the perceptual listening test results verify that a TTS system that applied our proposed method achieved a mean opinion score of 4.39 in prosody naturalness, which is highly competitive with the score of 4.37 for synthesized speech with ground-truth phrase breaks.\n\n\n\n\n Title: LLM helps design and optimize photonic crystal surface emitting lasers \n\n Abstract: Conventional design and optimization of Photonic Crystal Surface Emitting Lasers (PCSEL) usually requires expert knowledge in semiconductor physics and optimization algorithms, which is also known as the inverse design problem. However, with the trend towards automation and depersonalization of the entire integrated circuits (IC) industry, the conventional method, with the drawback of being relatively labor-intensive and sub-optimal, warrants further refinement. This technical dilemma remained until the emergence of Large Language Models (LLMs), such as OpenAI's ChatGPT and Google's Bard. This paper explores the possibility of applying LLMs to machine learning-based design and optimization of PCSELs. Specifically, we utilize GPT3.5 and GPT4. By simply having conversations, GPT assisted us with writing Finite Difference Time Domain (FDTD) simulation code and deep reinforcement learning code to acquire the optimized PCSEL solution, spanning from the proposition of ideas to the realization of algorithms. Given that GPT will perform better when given detailed and specific questions, we break down the PCSEL design problem into a series of sub-problems and converse with GPT by posing open-ended heuristic questions rather than definitive commands. This paper shows that LLMs, such as ChatGPT, can guide the nanophotonic design and optimization processes, on both the conceptual and technical level, and we propose new human-AI co-design strategies and show their practical implications. We achieve a significant milestone for the first step towards an automated end to end nanophotonic design and production pipeline.\n\n\n\n\n Title: Evaluating Attribution in Dialogue Systems: The BEGIN Benchmark \n\n Abstract: Knowledge-grounded dialogue systems powered by large language models often generate responses that, while fluent, are not attributable to a relevant source of information. Progress towards models that do not exhibit this issue requires evaluation metrics that can quantify its prevalence. To this end, we introduce the Benchmark for Evaluation of Grounded INteraction (BEGIN), comprised of 12k dialogue turns generated by neural dialogue systems trained on three knowledge-grounded dialogue corpora. We collect human annotations assessing the extent to which the models' responses can be attributed to the given background information. We then use BEGIN to analyze eight evaluation metrics. We find that these metrics rely on spurious correlations, do not reliably distinguish attributable abstractive responses from unattributable ones, and perform substantially worse when the knowledge source is longer. Our findings underscore the need for more sophisticated and robust evaluation metrics for knowledge-grounded dialogue. We make BEGIN publicly available at https://github.com/google/BEGIN-dataset.\n\n\n\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"%pip install -qU langchain-google-genai","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:51:18.355540Z","iopub.execute_input":"2025-04-22T04:51:18.355866Z","iopub.status.idle":"2025-04-22T04:51:27.929080Z","shell.execute_reply.started":"2025-04-22T04:51:18.355845Z","shell.execute_reply":"2025-04-22T04:51:27.927491Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.1/434.1 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-generativeai 0.8.4 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.17 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import getpass\nimport os\n\nif \"GOOGLE_API_KEY\" not in os.environ:\n    os.environ[\"GOOGLE_API_KEY\"] = GEMINI_API_KEY","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:51:27.931793Z","iopub.execute_input":"2025-04-22T04:51:27.932183Z","iopub.status.idle":"2025-04-22T04:51:27.938335Z","shell.execute_reply.started":"2025-04-22T04:51:27.932150Z","shell.execute_reply":"2025-04-22T04:51:27.937290Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from langchain_google_genai import ChatGoogleGenerativeAI\n\nllm = ChatGoogleGenerativeAI(model=\"models/gemini-2.0-flash\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:51:27.940025Z","iopub.execute_input":"2025-04-22T04:51:27.940386Z","iopub.status.idle":"2025-04-22T04:51:31.457081Z","shell.execute_reply.started":"2025-04-22T04:51:27.940350Z","shell.execute_reply":"2025-04-22T04:51:31.456188Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"response = llm.invoke(\"Sing a ballad of LangChain.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:51:31.458891Z","iopub.execute_input":"2025-04-22T04:51:31.459601Z","iopub.status.idle":"2025-04-22T04:51:34.500965Z","shell.execute_reply.started":"2025-04-22T04:51:31.459571Z","shell.execute_reply":"2025-04-22T04:51:34.499731Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"response","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:51:34.502027Z","iopub.execute_input":"2025-04-22T04:51:34.502425Z","iopub.status.idle":"2025-04-22T04:51:34.511301Z","shell.execute_reply.started":"2025-04-22T04:51:34.502391Z","shell.execute_reply":"2025-04-22T04:51:34.510394Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"AIMessage(content=\"(Verse 1)\\nIn realms of code, where shadows play,\\nA framework rose, to light the way.\\nLangChain it's called, a noble name,\\nTo tame the LLMs, and win the game.\\nA bridge it builds, 'tween thought and deed,\\nTo make the language models truly lead.\\n\\n(Verse 2)\\nWith chains of prompts, and agents bold,\\nIt weaves a story, to be told.\\nFrom data deep, it draws its might,\\nAnd answers questions, day and night.\\nIt reads the documents, understands the plea,\\nAnd crafts a response, for you and me.\\n\\n(Verse 3)\\nThe Memory module, a precious gem,\\nRecalls the past, a diadem.\\nConversations flow, with context clear,\\nNo longer lost, in doubt and fear.\\nIt remembers your name, your favorite hue,\\nAnd builds a rapport, both strong and true.\\n\\n(Verse 4)\\nThe Agents work, with tireless grace,\\nTo search the web, in time and space.\\nThey call the tools, with skill and care,\\nTo find the answers, hidden there.\\nFrom calculators to API calls,\\nThey conquer challenges, standing tall.\\n\\n(Verse 5)\\nBut LangChain's journey, is not yet done,\\nNew features rise, with every sun.\\nThe community grows, with open hearts,\\nContributing code, and playing their parts.\\nTo push the boundaries, ever higher,\\nAnd fuel the flames, of creative fire.\\n\\n(Verse 6)\\nSo raise a glass, to LangChain's might,\\nA beacon shining, ever bright.\\nFor in its code, a future lies,\\nWhere language models, truly rise.\\nTo aid humanity, with wisdom keen,\\nLangChain, the king, the coding queen!\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-bbfeb9c6-30f9-4fdc-967a-ac442744cb69-0', usage_metadata={'input_tokens': 7, 'output_tokens': 392, 'total_tokens': 399, 'input_token_details': {'cache_read': 0}})"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"print(response.content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:51:34.513229Z","iopub.execute_input":"2025-04-22T04:51:34.513593Z","iopub.status.idle":"2025-04-22T04:51:34.529124Z","shell.execute_reply.started":"2025-04-22T04:51:34.513551Z","shell.execute_reply":"2025-04-22T04:51:34.527953Z"}},"outputs":[{"name":"stdout","text":"(Verse 1)\nIn realms of code, where shadows play,\nA framework rose, to light the way.\nLangChain it's called, a noble name,\nTo tame the LLMs, and win the game.\nA bridge it builds, 'tween thought and deed,\nTo make the language models truly lead.\n\n(Verse 2)\nWith chains of prompts, and agents bold,\nIt weaves a story, to be told.\nFrom data deep, it draws its might,\nAnd answers questions, day and night.\nIt reads the documents, understands the plea,\nAnd crafts a response, for you and me.\n\n(Verse 3)\nThe Memory module, a precious gem,\nRecalls the past, a diadem.\nConversations flow, with context clear,\nNo longer lost, in doubt and fear.\nIt remembers your name, your favorite hue,\nAnd builds a rapport, both strong and true.\n\n(Verse 4)\nThe Agents work, with tireless grace,\nTo search the web, in time and space.\nThey call the tools, with skill and care,\nTo find the answers, hidden there.\nFrom calculators to API calls,\nThey conquer challenges, standing tall.\n\n(Verse 5)\nBut LangChain's journey, is not yet done,\nNew features rise, with every sun.\nThe community grows, with open hearts,\nContributing code, and playing their parts.\nTo push the boundaries, ever higher,\nAnd fuel the flames, of creative fire.\n\n(Verse 6)\nSo raise a glass, to LangChain's might,\nA beacon shining, ever bright.\nFor in its code, a future lies,\nWhere language models, truly rise.\nTo aid humanity, with wisdom keen,\nLangChain, the king, the coding queen!\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# Vector Stores","metadata":{}},{"cell_type":"code","source":"pip install -qU langchain-core","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:51:39.715035Z","iopub.execute_input":"2025-04-22T04:51:39.715376Z","iopub.status.idle":"2025-04-22T04:51:44.329099Z","shell.execute_reply.started":"2025-04-22T04:51:39.715354Z","shell.execute_reply":"2025-04-22T04:51:44.327573Z"}},"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from langchain_google_genai import GoogleGenerativeAIEmbeddings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:51:44.331307Z","iopub.execute_input":"2025-04-22T04:51:44.331716Z","iopub.status.idle":"2025-04-22T04:51:44.337431Z","shell.execute_reply.started":"2025-04-22T04:51:44.331686Z","shell.execute_reply":"2025-04-22T04:51:44.336383Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:51:44.338465Z","iopub.execute_input":"2025-04-22T04:51:44.338853Z","iopub.status.idle":"2025-04-22T04:51:44.360858Z","shell.execute_reply.started":"2025-04-22T04:51:44.338820Z","shell.execute_reply":"2025-04-22T04:51:44.359913Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"vector = embeddings.embed_query(\"hello, world!\")\nvector[:5]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:51:46.079272Z","iopub.execute_input":"2025-04-22T04:51:46.079629Z","iopub.status.idle":"2025-04-22T04:51:46.482110Z","shell.execute_reply.started":"2025-04-22T04:51:46.079607Z","shell.execute_reply":"2025-04-22T04:51:46.481070Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"[0.05168594419956207,\n -0.030764883384108543,\n -0.03062233328819275,\n -0.02802734263241291,\n 0.01813093200325966]"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"from langchain_core.vectorstores import InMemoryVectorStore\n\nvector_store = InMemoryVectorStore(embeddings)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:51:48.350896Z","iopub.execute_input":"2025-04-22T04:51:48.352179Z","iopub.status.idle":"2025-04-22T04:51:48.361029Z","shell.execute_reply.started":"2025-04-22T04:51:48.352138Z","shell.execute_reply":"2025-04-22T04:51:48.359872Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"from uuid import uuid4\n\nfrom langchain_core.documents import Document\n\n\ndocs = [Document(page_content=result[\"abstract\"],metadata={\"title\": result[\"title\"]}) for result in results]\nuuids = [str(uuid4()) for _ in range(len(docs))]\n\nvector_store.add_documents(documents=docs, ids=uuids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:51:50.696622Z","iopub.execute_input":"2025-04-22T04:51:50.696995Z","iopub.status.idle":"2025-04-22T04:51:51.488412Z","shell.execute_reply.started":"2025-04-22T04:51:50.696970Z","shell.execute_reply":"2025-04-22T04:51:51.487298Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"['b49c69c9-b043-4b8b-9402-6a8b550162f2',\n '2276f62d-b412-42f1-96c5-71d715eb7999',\n '4e34c7a3-08df-4ab5-bf9f-65f9934f098c',\n 'f22862af-ead5-4764-b1fe-c259436e11c6',\n 'a6e3b8b1-346b-46d7-b452-2bb7e9b2c6ac',\n '7cb6b5c3-19d1-46b1-a249-7e1c6d5dbc5d',\n '4b74eb93-d340-4e8f-9a54-0555ead82d51',\n 'd24a9c4f-b28a-46bd-821f-81b6b9c76d8c',\n '5570eac2-d794-4acf-918a-8c5151949fa6',\n '94f8d757-28d5-4732-8a0c-03e91bb9789a',\n '951125b8-33e2-470f-8551-96a0767e238b',\n '84105cf6-ac3d-466c-9ca1-553d16e5673a',\n 'bca90972-2355-4abd-af75-fe277c9d7b48',\n '9d0f268d-2c6a-4396-97ef-504b38002d78',\n '283ad04e-594b-4c68-ac4a-9c42c0a30106',\n '5b14a0c9-6669-42db-bc6a-25b9113c593c',\n 'b0557a7e-697f-47f1-a26e-6deb112ea131',\n 'cac453a1-69f5-487a-8acf-64c85e805e40',\n 'ea44c6ec-fc88-4a62-b271-1d544a92f607',\n 'e76b53ed-212b-4570-ad77-0a6106f9a1bf',\n '45428f21-5d0b-4b9b-97f7-3f38246ef856',\n '1d9d286b-483c-442e-863f-1a1e7a54773b',\n '318fde7f-711f-40c0-b0d4-7abfbb79e73c',\n 'a9bb18fc-e1cc-4980-b735-406c49fff9f2',\n '5439dcd1-7e2e-4e38-9528-0b46844aabcb',\n '331363f0-1972-4319-9171-13a6ade84282',\n '89648c41-beed-4426-8367-5c8c9d7730c5',\n '16b9b67c-f301-40d9-bad9-3d0c74eba3ea',\n '343c9a95-2b3c-4d0e-8421-e4c22212f828',\n '7ddd1f43-ab70-440c-bb0a-d6059e6b4c2d',\n '99c62aaa-2c65-468e-9fc7-bcbfc2951251',\n '829bfb7a-c8d3-444c-a72d-ad306b62acc6',\n 'fe99a512-6aa9-4d35-a1b3-2b4d0fae7bb5',\n '0154ade8-35c7-4a1b-b499-99ac6a8beeac',\n '9e01783f-10b6-4fba-9c3a-e4eb574d2aca',\n '5469b832-a680-4a23-a1ef-8dd8f1265c36',\n '125afe6f-f1db-44e6-a4e5-805584cc5ba8',\n '64c9b774-6c26-4fcb-b102-02157cc4b8eb',\n 'bb4de5ec-1e6e-42ad-be86-3ff7a29f4fe1',\n '557ca95e-0385-4245-bba4-58da3d958ac0',\n '06cde352-89c4-41cf-af20-012ff61a8d30',\n '9ad14e92-c477-42bb-9cc5-a0ba636fd02e',\n '232c5297-cb50-444a-a6f2-73455cbff45a',\n '846b735d-af2e-4c7e-941c-63d759215db6',\n '7a474409-fd75-467b-8e53-e88806bee1dd',\n '088952e5-ea44-4cae-a50a-f3467c208e43',\n 'b1beeb44-c1db-47b9-b79d-e7fb9a433cde',\n 'bad4b1a2-be4a-4e36-bc0f-2b842497080a',\n 'd1c7889a-8e88-4c8c-857f-45746c7db8bd',\n 'ea49b92a-cdce-420f-8b39-61d811ee3416']"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"query = input(\"Enter your query\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:51:54.602142Z","iopub.execute_input":"2025-04-22T04:51:54.602484Z","iopub.status.idle":"2025-04-22T04:52:20.141803Z","shell.execute_reply.started":"2025-04-22T04:51:54.602463Z","shell.execute_reply":"2025-04-22T04:52:20.140904Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"Enter your query tell me about training methods in llms\n"}],"execution_count":19},{"cell_type":"code","source":"results = vector_store.similarity_search_by_vector(\n    embedding=embeddings.embed_query(query), k=3\n)\nfor doc in results:\n    print(f\"* {doc.page_content} [{doc.metadata}]\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:52:22.998575Z","iopub.execute_input":"2025-04-22T04:52:22.998943Z","iopub.status.idle":"2025-04-22T04:52:23.223527Z","shell.execute_reply.started":"2025-04-22T04:52:22.998899Z","shell.execute_reply":"2025-04-22T04:52:23.222177Z"}},"outputs":[{"name":"stdout","text":"* Large language models have led to state-of-the-art accuracies across a range of tasks. However,training large language model needs massive computing resource, as more and more open source pre-training models are available, it is worthy to study how to take full advantage of available model. We find a method to save training time and resource cost by changing the small well-trained model to large model. We initialize a larger target model from a smaller source model by copy weight values from source model and padding with zeros or small initialization values on it to make the source and target model have approximate outputs, which is valid due to block matrix multiplication and residual connection in transformer structure. We test the target model on several data sets and find it is still comparable with the source model. When we continue training the target model, the training loss can start from a smaller value. [{'title': 'Transfer training from smaller language model'}]\n* Large language models have led to state-of-the-art accuracies across a range of tasks. However, training these models efficiently is challenging for two reasons: a) GPU memory capacity is limited, making it impossible to fit large models on even a multi-GPU server, and b) the number of compute operations required to train these models can result in unrealistically long training times. Consequently, new methods of model parallelism such as tensor and pipeline parallelism have been proposed. Unfortunately, naive usage of these methods leads to fundamental scaling issues at thousands of GPUs, e.g., due to expensive cross-node communication or devices spending significant time waiting on other devices to make progress. In this paper, we show how different types of parallelism methods (tensor, pipeline, and data parallelism) can be composed to scale to thousands of GPUs and models with trillions of parameters. We survey techniques for pipeline parallelism and propose a novel interleaved pipeline parallelism schedule that can improve throughput by 10+% with memory footprint comparable to existing approaches. We quantitatively study the trade-offs between tensor, pipeline, and data parallelism, and provide intuition as to how to configure distributed training of a large model. Our approach allows us to perform training iterations on a model with 1 trillion parameters at 502 petaFLOP/s on 3072 GPUs with achieved per-GPU throughput of 52% of theoretical peak. Our code is open sourced at https://github.com/nvidia/megatron-lm. [{'title': 'Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM'}]\n* Recent developments in unsupervised representation learning have successfully established the concept of transfer learning in NLP. Mainly three forces are driving the improvements in this area of research: More elaborated architectures are making better use of contextual information. Instead of simply plugging in static pre-trained representations, these are learned based on surrounding context in end-to-end trainable models with more intelligently designed language modelling objectives. Along with this, larger corpora are used as resources for pre-training large language models in a self-supervised fashion which are afterwards fine-tuned on supervised tasks. Advances in parallel computing as well as in cloud computing, made it possible to train these models with growing capacities in the same or even in shorter time than previously established models. These three developments agglomerate in new state-of-the-art (SOTA) results being revealed in a higher and higher frequency. It is not always obvious where these improvements originate from, as it is not possible to completely disentangle the contributions of the three driving forces. We set ourselves to providing a clear and concise overview on several large pre-trained language models, which achieved SOTA results in the last two years, with respect to their use of new architectures and resources. We want to clarify for the reader where the differences between the models are and we furthermore attempt to gain some insight into the single contributions of lexical/computational improvements as well as of architectural changes. We explicitly do not intend to quantify these contributions, but rather see our work as an overview in order to identify potential starting points for benchmark comparisons. Furthermore, we tentatively want to point at potential possibilities for improvement in the field of open-sourcing and reproducible research. [{'title': 'On the comparability of Pre-trained Language Models'}]\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"retrived_context = \"\"\n\nfor doc in results:\n    retrived_context += doc.page_content + \"\\n\\n\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:52:33.368329Z","iopub.execute_input":"2025-04-22T04:52:33.368735Z","iopub.status.idle":"2025-04-22T04:52:33.374457Z","shell.execute_reply.started":"2025-04-22T04:52:33.368709Z","shell.execute_reply":"2025-04-22T04:52:33.373172Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"print(retrived_context)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:52:35.375966Z","iopub.execute_input":"2025-04-22T04:52:35.376334Z","iopub.status.idle":"2025-04-22T04:52:35.383064Z","shell.execute_reply.started":"2025-04-22T04:52:35.376308Z","shell.execute_reply":"2025-04-22T04:52:35.381975Z"}},"outputs":[{"name":"stdout","text":"Large language models have led to state-of-the-art accuracies across a range of tasks. However,training large language model needs massive computing resource, as more and more open source pre-training models are available, it is worthy to study how to take full advantage of available model. We find a method to save training time and resource cost by changing the small well-trained model to large model. We initialize a larger target model from a smaller source model by copy weight values from source model and padding with zeros or small initialization values on it to make the source and target model have approximate outputs, which is valid due to block matrix multiplication and residual connection in transformer structure. We test the target model on several data sets and find it is still comparable with the source model. When we continue training the target model, the training loss can start from a smaller value.\n\nLarge language models have led to state-of-the-art accuracies across a range of tasks. However, training these models efficiently is challenging for two reasons: a) GPU memory capacity is limited, making it impossible to fit large models on even a multi-GPU server, and b) the number of compute operations required to train these models can result in unrealistically long training times. Consequently, new methods of model parallelism such as tensor and pipeline parallelism have been proposed. Unfortunately, naive usage of these methods leads to fundamental scaling issues at thousands of GPUs, e.g., due to expensive cross-node communication or devices spending significant time waiting on other devices to make progress. In this paper, we show how different types of parallelism methods (tensor, pipeline, and data parallelism) can be composed to scale to thousands of GPUs and models with trillions of parameters. We survey techniques for pipeline parallelism and propose a novel interleaved pipeline parallelism schedule that can improve throughput by 10+% with memory footprint comparable to existing approaches. We quantitatively study the trade-offs between tensor, pipeline, and data parallelism, and provide intuition as to how to configure distributed training of a large model. Our approach allows us to perform training iterations on a model with 1 trillion parameters at 502 petaFLOP/s on 3072 GPUs with achieved per-GPU throughput of 52% of theoretical peak. Our code is open sourced at https://github.com/nvidia/megatron-lm.\n\nRecent developments in unsupervised representation learning have successfully established the concept of transfer learning in NLP. Mainly three forces are driving the improvements in this area of research: More elaborated architectures are making better use of contextual information. Instead of simply plugging in static pre-trained representations, these are learned based on surrounding context in end-to-end trainable models with more intelligently designed language modelling objectives. Along with this, larger corpora are used as resources for pre-training large language models in a self-supervised fashion which are afterwards fine-tuned on supervised tasks. Advances in parallel computing as well as in cloud computing, made it possible to train these models with growing capacities in the same or even in shorter time than previously established models. These three developments agglomerate in new state-of-the-art (SOTA) results being revealed in a higher and higher frequency. It is not always obvious where these improvements originate from, as it is not possible to completely disentangle the contributions of the three driving forces. We set ourselves to providing a clear and concise overview on several large pre-trained language models, which achieved SOTA results in the last two years, with respect to their use of new architectures and resources. We want to clarify for the reader where the differences between the models are and we furthermore attempt to gain some insight into the single contributions of lexical/computational improvements as well as of architectural changes. We explicitly do not intend to quantify these contributions, but rather see our work as an overview in order to identify potential starting points for benchmark comparisons. Furthermore, we tentatively want to point at potential possibilities for improvement in the field of open-sourcing and reproducible research.\n\n\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"output = llm.invoke(f\"Using the retrieved context : {retrived_context} answer the user's query: {query}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:52:38.812226Z","iopub.execute_input":"2025-04-22T04:52:38.813821Z","iopub.status.idle":"2025-04-22T04:52:41.173904Z","shell.execute_reply.started":"2025-04-22T04:52:38.813774Z","shell.execute_reply":"2025-04-22T04:52:41.172673Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"print(output.content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:52:50.543332Z","iopub.execute_input":"2025-04-22T04:52:50.543795Z","iopub.status.idle":"2025-04-22T04:52:50.550241Z","shell.execute_reply.started":"2025-04-22T04:52:50.543769Z","shell.execute_reply":"2025-04-22T04:52:50.548785Z"}},"outputs":[{"name":"stdout","text":"Based on the provided context, here's a summary of training methods in Large Language Models (LLMs):\n\n**Challenges and Approaches:**\n\n*   **Resource Intensive:** Training LLMs requires massive computational resources (GPUs) and can take a long time.\n*   **Memory Limitations:** The size of LLMs can exceed the memory capacity of even multi-GPU servers.\n\n**Techniques to Address Challenges:**\n\n*   **Model Parallelism:** Distributing the model across multiple GPUs to overcome memory limitations. Types include:\n    *   **Tensor Parallelism:** Dividing individual layers of the model across GPUs.\n    *   **Pipeline Parallelism:** Dividing the model into stages and processing different mini-batches concurrently. Interleaved pipeline parallelism can improve throughput.\n    *   **Data Parallelism:** Replicating the model on each GPU and processing different subsets of the training data.\n*   **Transfer Learning:** Leveraging pre-trained models to reduce training time and resource cost.\n    *   **Initializing Large Models from Smaller Ones:** Copying weights from a smaller, well-trained model to a larger target model and padding the remaining weights. This allows the larger model to start training with a lower loss.\n    *   **Fine-tuning:** Adapting a pre-trained model to a specific task using supervised learning.\n*   **Optimized Distributed Training:** Combining different parallelism methods (tensor, pipeline, and data parallelism) to achieve high throughput on a large number of GPUs.\n\n**Factors Driving Improvements:**\n\n*   **Architectural Innovations:** More sophisticated architectures that make better use of contextual information.\n*   **Larger Corpora:** Training on larger datasets in a self-supervised manner.\n*   **Advances in Computing:** Parallel and cloud computing enable the training of larger models in a reasonable timeframe.\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"# Agent","metadata":{}},{"cell_type":"code","source":"!pip install langgraph","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:52:59.315662Z","iopub.execute_input":"2025-04-22T04:52:59.316586Z","iopub.status.idle":"2025-04-22T04:53:04.908621Z","shell.execute_reply.started":"2025-04-22T04:52:59.316554Z","shell.execute_reply":"2025-04-22T04:53:04.907274Z"}},"outputs":[{"name":"stdout","text":"Collecting langgraph\n  Downloading langgraph-0.3.31-py3-none-any.whl.metadata (7.9 kB)\nRequirement already satisfied: langchain-core<0.4,>=0.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.3.55)\nCollecting langgraph-checkpoint<3.0.0,>=2.0.10 (from langgraph)\n  Downloading langgraph_checkpoint-2.0.24-py3-none-any.whl.metadata (4.6 kB)\nCollecting langgraph-prebuilt<0.2,>=0.1.8 (from langgraph)\n  Downloading langgraph_prebuilt-0.1.8-py3-none-any.whl.metadata (5.0 kB)\nCollecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n  Downloading langgraph_sdk-0.1.63-py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: xxhash<4.0.0,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\nRequirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (0.3.8)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (9.0.0)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (1.33)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (6.0.2)\nRequirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (24.2)\nRequirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (4.13.1)\nRequirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (2.11.3)\nCollecting ormsgpack<2.0.0,>=1.8.0 (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph)\n  Downloading ormsgpack-1.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\nRequirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.15)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.7.1)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (2025.1.31)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.7)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1->langgraph) (3.0.0)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (2.32.3)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (1.0.0)\nRequirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (0.23.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.1->langgraph) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.1->langgraph) (2.33.1)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.1->langgraph) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (3.4.1)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (2.3.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.3.1)\nDownloading langgraph-0.3.31-py3-none-any.whl (145 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.2/145.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langgraph_checkpoint-2.0.24-py3-none-any.whl (42 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langgraph_prebuilt-0.1.8-py3-none-any.whl (25 kB)\nDownloading langgraph_sdk-0.1.63-py3-none-any.whl (47 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ormsgpack-1.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: ormsgpack, langgraph-sdk, langgraph-checkpoint, langgraph-prebuilt, langgraph\nSuccessfully installed langgraph-0.3.31 langgraph-checkpoint-2.0.24 langgraph-prebuilt-0.1.8 langgraph-sdk-0.1.63 ormsgpack-1.9.1\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"import os\nimport requests\nimport json\nfrom uuid import uuid4\n\n\nfrom langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\nfrom langchain_core.documents import Document\nfrom langchain_core.vectorstores import InMemoryVectorStore\n\nfrom langgraph.graph import StateGraph, END\nfrom langgraph.graph.message import add_messages\nfrom langgraph.checkpoint.memory import MemorySaver\n\n\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom pydantic import BaseModel, Field\nfrom typing import List, Union\nfrom typing_extensions import TypedDict\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:53:04.910825Z","iopub.execute_input":"2025-04-22T04:53:04.911195Z","iopub.status.idle":"2025-04-22T04:53:05.004821Z","shell.execute_reply.started":"2025-04-22T04:53:04.911166Z","shell.execute_reply":"2025-04-22T04:53:05.002589Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"from typing import  Optional, Annotated\nfrom langchain_core.documents import Document\nfrom langgraph.graph.message import add_messages\nfrom langchain_core.messages import BaseMessage\n\n\nclass AgentState(TypedDict):\n    query: str                              # Original user query\n    search_query: str                       # Parsed version of the query for search\n    papers: List[dict]                      # Raw papers fetched from Papers with Code API\n    docs: List[Document]                    # LangChain Documents (abstracts with metadata)\n    results: List[Document]                 # Retrieved top-k relevant documents\n    context: str                            # Formatted context to answer questions\n    final_answer: str                       # Final synthesized answer\n    expert_opinion: str                     # Output from expert agent\n    reviewer_opinion: str                   # Output from reviewer agent\n    messages: Annotated[List[BaseMessage], add_messages]                      # ✅ LangGraph-managed chat history\n    retry_count: int\n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:53:06.814699Z","iopub.execute_input":"2025-04-22T04:53:06.815086Z","iopub.status.idle":"2025-04-22T04:53:06.822147Z","shell.execute_reply.started":"2025-04-22T04:53:06.815062Z","shell.execute_reply":"2025-04-22T04:53:06.821052Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"class SearchQuery(BaseModel):\n    search_query: str = Field(description=\"well structured Term for retrieving papers\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:53:14.406766Z","iopub.execute_input":"2025-04-22T04:53:14.407659Z","iopub.status.idle":"2025-04-22T04:53:14.415789Z","shell.execute_reply.started":"2025-04-22T04:53:14.407629Z","shell.execute_reply":"2025-04-22T04:53:14.414359Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# Instruction to parse the question\nsearch_instructions = SystemMessage(content=\"\"\"\nYou will be given a user question. Extract the relevant topic or search term \nthat best represents the topic for retrieving  relevant research papers. \nOutput JSON matching the Pydantic model SearchQuery.\n\"\"\".strip())\n\nparse_search_query = llm.with_structured_output(SearchQuery)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:53:16.231627Z","iopub.execute_input":"2025-04-22T04:53:16.232847Z","iopub.status.idle":"2025-04-22T04:53:16.243959Z","shell.execute_reply.started":"2025-04-22T04:53:16.232811Z","shell.execute_reply":"2025-04-22T04:53:16.242512Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"from langchain_core.tools import tool","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:53:20.702477Z","iopub.execute_input":"2025-04-22T04:53:20.702863Z","iopub.status.idle":"2025-04-22T04:53:20.708465Z","shell.execute_reply.started":"2025-04-22T04:53:20.702837Z","shell.execute_reply":"2025-04-22T04:53:20.707310Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"def parse_query_node(state: AgentState) -> AgentState:\n    \"\"\"Use the LLM to extract a search_query JSON via Pydantic.\"\"\"\n    try:\n        out = parse_search_query.invoke([\n            search_instructions,\n            HumanMessage(content=state[\"query\"])\n        ])\n        search = out.search_query\n    except Exception as e:\n        print(\"❗ parse_query_node error:\", e)\n        # fallback to the raw question if parsing fails\n        search = state[\"query\"]\n    print(f\"🔍 Parsed search_query = '{search}'\")\n    return {\"search_query\": search}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:53:22.658781Z","iopub.execute_input":"2025-04-22T04:53:22.659175Z","iopub.status.idle":"2025-04-22T04:53:22.665821Z","shell.execute_reply.started":"2025-04-22T04:53:22.659150Z","shell.execute_reply":"2025-04-22T04:53:22.664661Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"#@tool\ndef reparse_query_node(state: AgentState) -> AgentState:\n    \"\"\"Only reparse if we truly got 0 papers.\"\"\"\n    if len(state[\"papers\"]) > 0:\n        return state\n\n    try:\n        out = parse_search_query.invoke([\n            SystemMessage(content=\"\"\"\nThe Extracted relevant topic or search term is not returning any results.\nTry to extract a better term that best represents the topic for retrieving relevant research papers and is concise. \nOutput JSON matching the Pydantic model SearchQuery.\n\"\"\".strip()),\n            HumanMessage(content=f'The query of user: {state[\"query\"]} and the extracted old term is {state[\"search_query\"]}')\n        ])\n        search = out.search_query\n    except Exception as e:\n        print(\"❗ reparse_query_node error:\", e)\n        search = state[\"query\"]\n\n    retry_count = state['retry_count'] + 1\n    print(f\"🔍 Re Parsed search_query = '{search}', Retry Count = {retry_count}\")\n    return {\n        \"search_query\": search,\n        \"retry_count\": retry_count,\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:53:32.457652Z","iopub.execute_input":"2025-04-22T04:53:32.458016Z","iopub.status.idle":"2025-04-22T04:53:32.465682Z","shell.execute_reply.started":"2025-04-22T04:53:32.457991Z","shell.execute_reply":"2025-04-22T04:53:32.464449Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"memory = MemorySaver()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:53:35.080819Z","iopub.execute_input":"2025-04-22T04:53:35.081363Z","iopub.status.idle":"2025-04-22T04:53:35.086872Z","shell.execute_reply.started":"2025-04-22T04:53:35.081332Z","shell.execute_reply":"2025-04-22T04:53:35.085587Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"#@tool\ndef fetch_papers(state: AgentState) -> AgentState:\n    query = state[\"search_query\"]\n    url = f\"https://paperswithcode.com/api/v1/papers/?q={query}&limit=3\"\n    try:\n        response = requests.get(url)\n        papers = response.json().get(\"results\", [])\n        return {\"papers\": papers}\n    except Exception as e:\n        print(f\"Error fetching papers: {e}\")\n        return {\"papers\": []}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T06:08:51.004286Z","iopub.execute_input":"2025-04-22T06:08:51.004693Z","iopub.status.idle":"2025-04-22T06:08:51.011236Z","shell.execute_reply.started":"2025-04-22T06:08:51.004668Z","shell.execute_reply":"2025-04-22T06:08:51.010196Z"}},"outputs":[],"execution_count":170},{"cell_type":"code","source":"\ntools = [fetch_papers, parse_query_node, reparse_query_node]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:53:42.168521Z","iopub.execute_input":"2025-04-22T04:53:42.168946Z","iopub.status.idle":"2025-04-22T04:53:42.173912Z","shell.execute_reply.started":"2025-04-22T04:53:42.168901Z","shell.execute_reply":"2025-04-22T04:53:42.172699Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"from langchain_core.tools import Tool\nfrom langchain_core.tools import tool\n\nllm = ChatGoogleGenerativeAI(model=\"models/gemini-2.0-flash\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:53:44.111522Z","iopub.execute_input":"2025-04-22T04:53:44.111856Z","iopub.status.idle":"2025-04-22T04:53:44.133485Z","shell.execute_reply.started":"2025-04-22T04:53:44.111834Z","shell.execute_reply":"2025-04-22T04:53:44.132364Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"def assistant_node(state):\n    llm.bind_tools(tools)\n    \n    if not state.get(\"messages\"):\n        raise ValueError(\"State must contain non-empty 'messages'!\")\n\n    response = llm.invoke({\"messages\": state[\"messages\"]})\n    \n    return {\"messages\": state[\"messages\"] + [response]}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:54:02.109830Z","iopub.execute_input":"2025-04-22T04:54:02.110835Z","iopub.status.idle":"2025-04-22T04:54:02.116621Z","shell.execute_reply.started":"2025-04-22T04:54:02.110803Z","shell.execute_reply":"2025-04-22T04:54:02.115334Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"def no_results_node(state: AgentState) -> AgentState:\n    return {\n        \"final_answer\": \"No papers were found for the given query. Please try a different topic.\",\n        \"messages\": state[\"messages\"] + [AIMessage(content=\"No papers found.\")]\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:54:04.961194Z","iopub.execute_input":"2025-04-22T04:54:04.961583Z","iopub.status.idle":"2025-04-22T04:54:04.968226Z","shell.execute_reply.started":"2025-04-22T04:54:04.961556Z","shell.execute_reply":"2025-04-22T04:54:04.966962Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\nvector_store = InMemoryVectorStore(embeddings)\n\ndef embed_node(state: AgentState) -> AgentState:\n    papers = state[\"papers\"]\n    docs = [Document(page_content=p[\"abstract\"], metadata={\"title\": p[\"title\"]}) for p in papers]\n    uuids = [str(uuid4()) for _ in docs]\n    vector_store.add_documents(docs, ids=uuids)\n    return {\"docs\": docs}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:54:06.966180Z","iopub.execute_input":"2025-04-22T04:54:06.967021Z","iopub.status.idle":"2025-04-22T04:54:06.980051Z","shell.execute_reply.started":"2025-04-22T04:54:06.966988Z","shell.execute_reply":"2025-04-22T04:54:06.978213Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"def search_node(state: AgentState) -> AgentState:\n    query = state[\"query\"]\n    embedding = embeddings.embed_query(query)\n    results = vector_store.similarity_search_by_vector(embedding, k=3)\n    return {\"results\": results}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:54:09.299276Z","iopub.execute_input":"2025-04-22T04:54:09.299670Z","iopub.status.idle":"2025-04-22T04:54:09.305953Z","shell.execute_reply.started":"2025-04-22T04:54:09.299643Z","shell.execute_reply":"2025-04-22T04:54:09.304855Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"def generate_context(state: AgentState) -> AgentState:\n    \"\"\"\n    Build a numbered context including titles so downstream agents can cite them.\n    \"\"\"\n    parts = []\n    for i, doc in enumerate(state[\"results\"], start=1):\n        title = doc.metadata.get(\"title\", \"Untitled\")\n        parts.append(f\"[{i}] {title}\\n{doc.page_content}\")\n    ctxt = \"\\n\\n\".join(parts)\n    return {\"context\": ctxt}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:54:11.261147Z","iopub.execute_input":"2025-04-22T04:54:11.261485Z","iopub.status.idle":"2025-04-22T04:54:11.267856Z","shell.execute_reply.started":"2025-04-22T04:54:11.261463Z","shell.execute_reply":"2025-04-22T04:54:11.266704Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"def answer_node(state: AgentState) -> AgentState:\n    query = state[\"query\"]\n    context = state[\"context\"]\n    prompt = f\"Using the retrieved context:\\n{context}\\n\\nAnswer the user's query: {query}\"\n    response = llm.invoke(prompt)\n    return {\"answer\": response.content}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:54:12.761081Z","iopub.execute_input":"2025-04-22T04:54:12.761412Z","iopub.status.idle":"2025-04-22T04:54:12.767544Z","shell.execute_reply.started":"2025-04-22T04:54:12.761389Z","shell.execute_reply":"2025-04-22T04:54:12.766209Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"def expert_agent(state: AgentState) -> AgentState:\n    messages = state[\"messages\"] + [\n        AIMessage(content=\"Retrieving expert insights based on the papers and cite the papers...\"),\n        HumanMessage(content=f\"Context: {state['context']}\\n\\nQuery: {state['query']}\"),\n    ]\n    response = llm.invoke(messages)\n    return {\n        \"expert_opinion\": response.content,\n        \"messages\": messages + [AIMessage(content=response.content)],\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:54:15.634505Z","iopub.execute_input":"2025-04-22T04:54:15.635417Z","iopub.status.idle":"2025-04-22T04:54:15.642236Z","shell.execute_reply.started":"2025-04-22T04:54:15.635380Z","shell.execute_reply":"2025-04-22T04:54:15.640715Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"def reviewer_agent(state: AgentState) -> AgentState:\n    messages = state[\"messages\"] + [\n        AIMessage(content=\"🧪 Reviewing the expert's findings for validity...\"),\n        HumanMessage(content=f\"Expert said:\\n{state['expert_opinion']}\")\n    ]\n    response = llm.invoke(messages)\n\n    return {\n        \"reviewer_opinion\": response.content,\n        \"messages\": messages + [AIMessage(content=response.content)]\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:54:18.184367Z","iopub.execute_input":"2025-04-22T04:54:18.184761Z","iopub.status.idle":"2025-04-22T04:54:18.191556Z","shell.execute_reply.started":"2025-04-22T04:54:18.184735Z","shell.execute_reply":"2025-04-22T04:54:18.190219Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"def synthesizer_agent(state: AgentState) -> AgentState:\n    messages = state[\"messages\"] + [\n        AIMessage(content=\"🧠 Synthesizing expert and reviewer input...\"),\n        HumanMessage(content=(\n            f\"Expert Opinion:\\n{state['expert_opinion']}\\n\\n\"\n            f\"Reviewer Feedback:\\n{state['reviewer_opinion']}\\n\\n\"\n            f\"Final Question: {state['query']}\"\n        ))\n    ]\n    response = llm.invoke(messages)\n\n    return {\n        \"final_answer\": response.content,\n        \"messages\": messages + [AIMessage(content=response.content)]\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:54:20.042452Z","iopub.execute_input":"2025-04-22T04:54:20.042838Z","iopub.status.idle":"2025-04-22T04:54:20.051047Z","shell.execute_reply.started":"2025-04-22T04:54:20.042813Z","shell.execute_reply":"2025-04-22T04:54:20.049792Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"def final_human_reply(state: AgentState) -> AgentState:\n    return {\n        \"messages\": state[\"messages\"] + [HumanMessage(content=\"Thank you!\")],\n        \"final_answer\": state[\"final_answer\"]\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:54:21.836964Z","iopub.execute_input":"2025-04-22T04:54:21.837629Z","iopub.status.idle":"2025-04-22T04:54:21.844225Z","shell.execute_reply.started":"2025-04-22T04:54:21.837448Z","shell.execute_reply":"2025-04-22T04:54:21.843060Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"# Condition: decide next step based on number of papers and retries\ndef papers_condition(state: AgentState) -> str:\n    if len(state.get(\"papers\", [])) > 0:\n        return \"continue\"\n    elif state.get(\"retry_count\", 0) < 3:\n        return \"reparse_query\"\n    else:\n        return \"no_results\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T04:54:28.579824Z","iopub.execute_input":"2025-04-22T04:54:28.580185Z","iopub.status.idle":"2025-04-22T04:54:28.586835Z","shell.execute_reply.started":"2025-04-22T04:54:28.580162Z","shell.execute_reply":"2025-04-22T04:54:28.585487Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"# Graph definition\nfrom langgraph.graph import StateGraph, END, START\n\nbuilder = StateGraph(AgentState)\n\n\n\n\n\n\n\n\n\n\n\nbuilder.add_node('parse_query', parse_query_node)\nbuilder.add_node('fetch_papers_node', fetch_papers)  \nbuilder.add_node('reparse_query_node', reparse_query_node)\nbuilder.add_node('embed_docs', embed_node)\nbuilder.add_node('search_docs', search_node)\nbuilder.add_node('generate_context', generate_context)\nbuilder.add_node('expert_agent', expert_agent)\nbuilder.add_node('reviewer_agent', reviewer_agent)\nbuilder.add_node('synthesizer_agent', synthesizer_agent)\nbuilder.add_node('final_human_reply', final_human_reply)\nbuilder.add_node('no_results', no_results_node)\n\n\n\nbuilder.set_entry_point('parse_query')\nbuilder.add_edge(START, 'parse_query')\nbuilder.add_edge('parse_query', 'fetch_papers_node')\n\nbuilder.add_conditional_edges(\n    'fetch_papers_node',\n    papers_condition,\n    { 'continue': 'embed_docs', 'reparse_query': 'reparse_query_node', 'no_results': 'no_results' }\n)\n\nbuilder.add_edge('reparse_query_node', 'fetch_papers_node')\n\nbuilder.add_edge('embed_docs', 'search_docs')\nbuilder.add_edge('search_docs', 'generate_context')\nbuilder.add_edge('generate_context', 'expert_agent')\nbuilder.add_edge('expert_agent', 'reviewer_agent')\nbuilder.add_edge('reviewer_agent', 'synthesizer_agent')\nbuilder.add_edge('synthesizer_agent', 'final_human_reply')\nbuilder.add_edge('final_human_reply', END)\nbuilder.add_edge('no_results', END)\n\nrunnable = builder.compile(checkpointer = memory,\n    interrupt_before=['fetch_papers_node']\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:50:37.059716Z","iopub.execute_input":"2025-04-22T05:50:37.060707Z","iopub.status.idle":"2025-04-22T05:50:37.080220Z","shell.execute_reply.started":"2025-04-22T05:50:37.060678Z","shell.execute_reply":"2025-04-22T05:50:37.079106Z"}},"outputs":[],"execution_count":158},{"cell_type":"code","source":"thread = {\"configurable\": {\"thread_id\": \"6\"}}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:47:08.544268Z","iopub.execute_input":"2025-04-22T05:47:08.544700Z","iopub.status.idle":"2025-04-22T05:47:08.549774Z","shell.execute_reply.started":"2025-04-22T05:47:08.544677Z","shell.execute_reply":"2025-04-22T05:47:08.548743Z"}},"outputs":[],"execution_count":149},{"cell_type":"code","source":"output = runnable.invoke({\"query\":query,\"retry_count\":0},thread)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:51:39.156242Z","iopub.execute_input":"2025-04-22T05:51:39.156642Z","iopub.status.idle":"2025-04-22T05:51:39.781114Z","shell.execute_reply.started":"2025-04-22T05:51:39.156615Z","shell.execute_reply":"2025-04-22T05:51:39.780101Z"}},"outputs":[{"name":"stdout","text":"🔍 Parsed search_query = 'multimodal large language model'\n","output_type":"stream"}],"execution_count":159},{"cell_type":"code","source":"output['final_answer']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:51:41.236578Z","iopub.execute_input":"2025-04-22T05:51:41.236902Z","iopub.status.idle":"2025-04-22T05:51:41.243632Z","shell.execute_reply.started":"2025-04-22T05:51:41.236882Z","shell.execute_reply":"2025-04-22T05:51:41.242698Z"}},"outputs":[{"execution_count":160,"output_type":"execute_result","data":{"text/plain":"'Based on the provided papers, two distinct approaches to multimodal LLMs are presented:\\n\\n*   **DreamLLM:** Focuses on generative modeling within the raw multimodal space, enabling interleaved content creation (text and images) and achieving strong zero-shot performance. This approach emphasizes learning the joint distributions of modalities directly, avoiding reliance on external feature extractors [1].\\n\\n*   **AnyMAL:** Employs a pre-trained aligner to map various modalities (text, image, video, audio, IMU) into a shared textual space, leveraging the reasoning capabilities of existing LLMs like LLaMA-2. Fine-tuning with a multimodal instruction set enhances its performance on diverse multimodal tasks [2, 3].\\n\\nIn essence, DreamLLM prioritizes direct multimodal generation and learning, while AnyMAL focuses on aligning different modalities to a textual representation for reasoning with a powerful LLM. Both achieve state-of-the-art results in their respective approaches.'"},"metadata":{}}],"execution_count":160},{"cell_type":"code","source":"query = input(\"Enter the query\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T06:10:48.692701Z","iopub.execute_input":"2025-04-22T06:10:48.693109Z","iopub.status.idle":"2025-04-22T06:10:59.252804Z","shell.execute_reply.started":"2025-04-22T06:10:48.693083Z","shell.execute_reply":"2025-04-22T06:10:59.251804Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"Enter the query latest advances in multimodal learning\n"}],"execution_count":178},{"cell_type":"code","source":"# Run the graph until the first interruption\nfor event in runnable.stream({\"query\":query,\"retry_count\":0}, thread, stream_mode=\"values\"):\n    print(event)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:14:02.342162Z","iopub.execute_input":"2025-04-22T05:14:02.342515Z","iopub.status.idle":"2025-04-22T05:14:03.056858Z","shell.execute_reply.started":"2025-04-22T05:14:02.342493Z","shell.execute_reply":"2025-04-22T05:14:03.055743Z"}},"outputs":[{"name":"stdout","text":"{'query': 'Tell me about advances in multimodal', 'messages': [], 'retry_count': 0}\n🔍 Parsed search_query = 'advances in multimodal learning'\n{'query': 'Tell me about advances in multimodal', 'search_query': 'advances in multimodal learning', 'messages': [], 'retry_count': 0}\n","output_type":"stream"}],"execution_count":78},{"cell_type":"code","source":"state = runnable.get_state(thread)\nstate.next","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:14:48.213236Z","iopub.execute_input":"2025-04-22T05:14:48.213650Z","iopub.status.idle":"2025-04-22T05:14:48.222124Z","shell.execute_reply.started":"2025-04-22T05:14:48.213612Z","shell.execute_reply":"2025-04-22T05:14:48.220794Z"}},"outputs":[{"execution_count":79,"output_type":"execute_result","data":{"text/plain":"('fetch_papers_node',)"},"metadata":{}}],"execution_count":79},{"cell_type":"code","source":"state","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:14:53.992138Z","iopub.execute_input":"2025-04-22T05:14:53.992532Z","iopub.status.idle":"2025-04-22T05:14:53.999117Z","shell.execute_reply.started":"2025-04-22T05:14:53.992507Z","shell.execute_reply":"2025-04-22T05:14:53.998060Z"}},"outputs":[{"execution_count":80,"output_type":"execute_result","data":{"text/plain":"StateSnapshot(values={'query': 'Tell me about advances in multimodal', 'search_query': 'advances in multimodal learning', 'messages': [], 'retry_count': 0}, next=('fetch_papers_node',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f01f389-b2cd-64e5-8005-247b0058a6f8'}}, metadata={'source': 'loop', 'writes': {'parse_query': {'search_query': 'advances in multimodal learning'}}, 'step': 5, 'parents': {}, 'thread_id': '1'}, created_at='2025-04-22T05:14:03.052132+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f01f389-ac15-6d07-8004-6b3767183bb7'}}, tasks=(PregelTask(id='602120fc-2ea7-9060-8dcc-11225650df60', name='fetch_papers_node', path=('__pregel_pull', 'fetch_papers_node'), error=None, interrupts=(), state=None, result=None),))"},"metadata":{}}],"execution_count":80},{"cell_type":"code","source":"for event in runnable.stream(None, thread, stream_mode=\"values\"):\n    print(event)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:15:48.275041Z","iopub.execute_input":"2025-04-22T05:15:48.275432Z","iopub.status.idle":"2025-04-22T05:15:49.330237Z","shell.execute_reply.started":"2025-04-22T05:15:48.275407Z","shell.execute_reply":"2025-04-22T05:15:49.329273Z"}},"outputs":[{"name":"stdout","text":"{'query': 'Tell me about advances in multimodal', 'search_query': 'advances in multimodal learning', 'messages': [], 'retry_count': 0}\n{'query': 'Tell me about advances in multimodal', 'search_query': 'advances in multimodal learning', 'papers': [], 'messages': [], 'retry_count': 0}\n🔍 Re Parsed search_query = 'multimodal learning advances', Retry Count = 1\n{'query': 'Tell me about advances in multimodal', 'search_query': 'multimodal learning advances', 'papers': [], 'messages': [], 'retry_count': 1}\n","output_type":"stream"}],"execution_count":82},{"cell_type":"code","source":"state = runnable.get_state(thread)\nstate.next","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:16:11.684470Z","iopub.execute_input":"2025-04-22T05:16:11.684861Z","iopub.status.idle":"2025-04-22T05:16:11.692857Z","shell.execute_reply.started":"2025-04-22T05:16:11.684834Z","shell.execute_reply":"2025-04-22T05:16:11.691789Z"}},"outputs":[{"execution_count":83,"output_type":"execute_result","data":{"text/plain":"('fetch_papers_node',)"},"metadata":{}}],"execution_count":83},{"cell_type":"code","source":"for event in runnable.stream(None, thread, stream_mode=\"values\"):\n    print(event)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:16:20.900164Z","iopub.execute_input":"2025-04-22T05:16:20.900487Z","iopub.status.idle":"2025-04-22T05:16:21.752033Z","shell.execute_reply.started":"2025-04-22T05:16:20.900466Z","shell.execute_reply":"2025-04-22T05:16:21.751013Z"}},"outputs":[{"name":"stdout","text":"{'query': 'Tell me about advances in multimodal', 'search_query': 'multimodal learning advances', 'papers': [], 'messages': [], 'retry_count': 1}\n{'query': 'Tell me about advances in multimodal', 'search_query': 'multimodal learning advances', 'papers': [], 'messages': [], 'retry_count': 1}\n🔍 Re Parsed search_query = 'multimodal learning advancements', Retry Count = 2\n{'query': 'Tell me about advances in multimodal', 'search_query': 'multimodal learning advancements', 'papers': [], 'messages': [], 'retry_count': 2}\n","output_type":"stream"}],"execution_count":84},{"cell_type":"code","source":"state = runnable.get_state(thread)\nstate.next","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:16:37.237557Z","iopub.execute_input":"2025-04-22T05:16:37.238439Z","iopub.status.idle":"2025-04-22T05:16:37.245272Z","shell.execute_reply.started":"2025-04-22T05:16:37.238392Z","shell.execute_reply":"2025-04-22T05:16:37.244377Z"}},"outputs":[{"execution_count":85,"output_type":"execute_result","data":{"text/plain":"('fetch_papers_node',)"},"metadata":{}}],"execution_count":85},{"cell_type":"code","source":"for event in runnable.stream(None, thread, stream_mode=\"values\"):\n    print(event)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:16:44.612538Z","iopub.execute_input":"2025-04-22T05:16:44.612879Z","iopub.status.idle":"2025-04-22T05:16:45.313133Z","shell.execute_reply.started":"2025-04-22T05:16:44.612857Z","shell.execute_reply":"2025-04-22T05:16:45.312025Z"}},"outputs":[{"name":"stdout","text":"{'query': 'Tell me about advances in multimodal', 'search_query': 'multimodal learning advancements', 'papers': [], 'messages': [], 'retry_count': 2}\n{'query': 'Tell me about advances in multimodal', 'search_query': 'multimodal learning advancements', 'papers': [], 'messages': [], 'retry_count': 2}\n🔍 Re Parsed search_query = 'multimodal learning advances', Retry Count = 3\n{'query': 'Tell me about advances in multimodal', 'search_query': 'multimodal learning advances', 'papers': [], 'messages': [], 'retry_count': 3}\n","output_type":"stream"}],"execution_count":86},{"cell_type":"code","source":"state = runnable.get_state(thread)\nstate.next","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:16:56.547318Z","iopub.execute_input":"2025-04-22T05:16:56.547648Z","iopub.status.idle":"2025-04-22T05:16:56.554960Z","shell.execute_reply.started":"2025-04-22T05:16:56.547627Z","shell.execute_reply":"2025-04-22T05:16:56.553895Z"}},"outputs":[{"execution_count":87,"output_type":"execute_result","data":{"text/plain":"('fetch_papers_node',)"},"metadata":{}}],"execution_count":87},{"cell_type":"code","source":"for event in runnable.stream(None, thread, stream_mode=\"values\"):\n    print(event)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:17:04.510668Z","iopub.execute_input":"2025-04-22T05:17:04.512193Z","iopub.status.idle":"2025-04-22T05:17:04.662000Z","shell.execute_reply.started":"2025-04-22T05:17:04.512146Z","shell.execute_reply":"2025-04-22T05:17:04.660591Z"}},"outputs":[{"name":"stdout","text":"{'query': 'Tell me about advances in multimodal', 'search_query': 'multimodal learning advances', 'papers': [], 'messages': [], 'retry_count': 3}\n{'query': 'Tell me about advances in multimodal', 'search_query': 'multimodal learning advances', 'papers': [], 'messages': [], 'retry_count': 3}\n{'query': 'Tell me about advances in multimodal', 'search_query': 'multimodal learning advances', 'papers': [], 'final_answer': 'No papers were found for the given query. Please try a different topic.', 'messages': [AIMessage(content='No papers found.', additional_kwargs={}, response_metadata={}, id='4530571f-26ac-4805-91e0-662717cfad37')], 'retry_count': 3}\n","output_type":"stream"}],"execution_count":88},{"cell_type":"markdown","source":"# Human In The Loop","metadata":{}},{"cell_type":"code","source":"def run_events(query, thread):\n    for event in runnable.stream({\"query\":query,\"retry_count\":0}, thread, stream_mode=\"values\"):\n        print(event)        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T06:22:02.784772Z","iopub.execute_input":"2025-04-22T06:22:02.785140Z","iopub.status.idle":"2025-04-22T06:22:02.790502Z","shell.execute_reply.started":"2025-04-22T06:22:02.785119Z","shell.execute_reply":"2025-04-22T06:22:02.789493Z"}},"outputs":[],"execution_count":200},{"cell_type":"code","source":"def resume_run(thread):\n    for event in runnable.stream(None, thread, stream_mode=\"values\"):\n        print(event)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T06:22:04.278991Z","iopub.execute_input":"2025-04-22T06:22:04.279280Z","iopub.status.idle":"2025-04-22T06:22:04.284457Z","shell.execute_reply.started":"2025-04-22T06:22:04.279262Z","shell.execute_reply":"2025-04-22T06:22:04.283313Z"}},"outputs":[],"execution_count":201},{"cell_type":"code","source":"thread = {\"configurable\": {\"thread_id\": \"17\"}}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T06:25:46.319035Z","iopub.execute_input":"2025-04-22T06:25:46.319423Z","iopub.status.idle":"2025-04-22T06:25:46.325253Z","shell.execute_reply.started":"2025-04-22T06:25:46.319398Z","shell.execute_reply":"2025-04-22T06:25:46.323784Z"}},"outputs":[],"execution_count":211},{"cell_type":"code","source":"run_events(query=\" Latest advancements in multimodal learning\", thread= thread)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T06:25:47.989636Z","iopub.execute_input":"2025-04-22T06:25:47.990004Z","iopub.status.idle":"2025-04-22T06:25:48.453232Z","shell.execute_reply.started":"2025-04-22T06:25:47.989980Z","shell.execute_reply":"2025-04-22T06:25:48.452062Z"}},"outputs":[{"name":"stdout","text":"{'query': ' Latest advancements in multimodal learning', 'messages': [], 'retry_count': 0}\n🔍 Parsed search_query = 'Latest advancements in multimodal learning'\n{'query': ' Latest advancements in multimodal learning', 'search_query': 'Latest advancements in multimodal learning', 'messages': [], 'retry_count': 0}\n","output_type":"stream"}],"execution_count":212},{"cell_type":"markdown","source":"## Previous response","metadata":{}},{"cell_type":"markdown","source":"Enter if the search_query is apt no\nEnter the feedback I dont want to know about latest advancements, I want to know about multimodal llms\n{'query': \"latest advances in multimodal learning Consider User's response for the search query: I dont want to know about latest advancements, I want to know about multimodal llms\", 'search_query': 'Multimodal learning advancements', 'papers': [], 'messages': [], 'retry_count': 1}\n{'query': \"latest advances in multimodal learning Consider User's response for the search query: I dont want to know about latest advancements, I want to know about multimodal llms\", 'search_query': 'Multimodal learning advancements', 'papers': [], 'messages': [], 'retry_count': 1}\n🔍 Re Parsed search_query = 'multimodal large language models', Retry Count = 2\n{'query': \"latest advances in multimodal learning Consider User's response for the search query: I dont want to know about latest advancements, I want to know about multimodal llms\", 'search_query': 'multimodal large language models', 'papers': [], 'messages': [], 'retry_count': 2}","metadata":{}},{"cell_type":"code","source":"user_approval = input(f\"Enter if the search_query is apt\")\nif user_approval.lower() == \"yes\":\n    resume_run(thread)\nelse:\n    runnable.update_state(thread, {'query': query + f\" Consider User's response for the search query: {input('Enter the feedback')}\"})\n    resume_run(thread)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T06:26:55.681834Z","iopub.execute_input":"2025-04-22T06:26:55.682184Z","iopub.status.idle":"2025-04-22T06:27:03.912294Z","shell.execute_reply.started":"2025-04-22T06:26:55.682162Z","shell.execute_reply":"2025-04-22T06:27:03.910686Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"Enter if the search_query is apt yes\n"},{"name":"stdout","text":"{'query': \"latest advances in multimodal learning Consider User's response for the search query: I dont want to know about latest advancements, I want to know about multimodal llms\", 'search_query': 'multimodal large language models', 'papers': [], 'messages': [], 'retry_count': 2}\n{'query': \"latest advances in multimodal learning Consider User's response for the search query: I dont want to know about latest advancements, I want to know about multimodal llms\", 'search_query': 'multimodal large language models', 'papers': [{'id': 'multimodal-large-language-models-for', 'arxiv_id': None, 'nips_id': None, 'url_abs': 'https://aclanthology.org/2022.naacl-srw.26', 'url_pdf': 'https://aclanthology.org/2022.naacl-srw.26.pdf', 'title': 'Multimodal large language models for inclusive collaboration learning tasks', 'abstract': 'This PhD project leverages advancements in multimodal large language models to build an inclusive collaboration feedback loop, in order to facilitate the automated detection, modeling, and feedback for participants developing general collaboration skills. This topic is important given the role of collaboration as an essential 21st century skill, the potential to ground large language models within learning theory and real-world practice, and the expressive potential of transformer models to support equity and inclusion. We address some concerns of integrating advances in natural language processing into downstream tasks such as the learning analytics feedback loop.', 'authors': ['Armanda Lewis'], 'published': None, 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': 'naacl-acl-2022-7'}, {'id': 'personality-aware-human-centric-multimodal', 'arxiv_id': '2304.02313', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2304.02313v2', 'url_pdf': 'https://arxiv.org/pdf/2304.02313v2.pdf', 'title': 'Personality-aware Human-centric Multimodal Reasoning: A New Task, Dataset and Baselines', 'abstract': \"Personality traits, emotions, and beliefs shape individuals' behavioral choices and decision-making processes. However, for one thing, the affective computing community normally focused on predicting personality traits but overlooks their application in behavior prediction. For another, the multimodal reasoning task emphasized the prediction of future states and behaviors but often neglected the incorporation of individual personality traits. In this work, we introduce a new task called Personality-aware Human-centric Multimodal Reasoning (PHMR) (T1), with the goal of forecasting the future behavior of a particular individual using multimodal information from past instances, while integrating personality factors. We accordingly construct a new dataset based on six television shows, encompassing 225 characters and 12k samples. To establish a benchmark for the task, we propose seven baseline methods: three adapted from related tasks, two pre-trained model, and two multimodal large language models. The experimental results demonstrate that incorporating personality traits enhances human-centric multimodal reasoning performance. To further solve the lack of personality annotation in real-life scenes, we introduce an extension task called Personality-predicted Human-centric Multimodal Reasoning task (T2) along with the corresponding dataset and method. We will make our dataset and code available on GitHub.\", 'authors': ['Rui Xia', 'Xiangqing Shen', 'Yaochen Zhu'], 'published': '2023-04-05', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'visual-transformation-telling', 'arxiv_id': '2305.01928', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2305.01928v2', 'url_pdf': 'https://arxiv.org/pdf/2305.01928v2.pdf', 'title': 'Visual Transformation Telling', 'abstract': 'Humans can naturally reason from superficial state differences (e.g. ground wetness) to transformations descriptions (e.g. raining) according to their life experience. In this paper, we propose a new visual reasoning task to test this transformation reasoning ability in real-world scenarios, called \\\\textbf{V}isual \\\\textbf{T}ransformation \\\\textbf{T}elling (VTT). Given a series of states (i.e. images), VTT requires to describe the transformation occurring between every two adjacent states. Different from existing visual reasoning tasks that focus on surface state reasoning, the advantage of VTT is that it captures the underlying causes, e.g. actions or events, behind the differences among states. We collect a novel dataset to support the study of transformation reasoning from two existing instructional video datasets, CrossTask and COIN, comprising 13,547 samples. Each sample involves the key state images along with their transformation descriptions. Our dataset covers diverse real-world activities, providing a rich resource for training and evaluation. To construct an initial benchmark for VTT, we test several models, including traditional visual storytelling methods (CST, GLACNet, Densecap) and advanced multimodal large language models (LLaVA v1.5-7B, Qwen-VL-chat, Gemini Pro Vision, GPT-4o, and GPT-4). Experimental results reveal that even state-of-the-art models still face challenges in VTT, highlighting substantial areas for improvement.', 'authors': ['Liang Pang', 'Xin Hong', 'Wanqing Cui', 'Xueqi Cheng', 'Jiafeng Guo', 'Yanyan Lan'], 'published': '2023-05-03', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'listen-think-and-understand', 'arxiv_id': '2305.10790', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2305.10790v3', 'url_pdf': 'https://arxiv.org/pdf/2305.10790v3.pdf', 'title': 'Listen, Think, and Understand', 'abstract': 'The ability of artificial intelligence (AI) systems to perceive and comprehend audio signals is crucial for many applications. Although significant progress has been made in this area since the development of AudioSet, most existing models are designed to map audio inputs to pre-defined, discrete sound label sets. In contrast, humans possess the ability to not only classify sounds into general categories, but also to listen to the finer details of the sounds, explain the reason for the predictions, think about what the sound infers, and understand the scene and what action needs to be taken, if any. Such capabilities beyond perception are not yet present in existing audio models. On the other hand, modern large language models (LLMs) exhibit emerging reasoning ability but they lack audio perception capabilities. Therefore, we ask the question: can we build a model that has both audio perception and a reasoning ability? In this paper, we propose a new audio foundation model, called LTU (Listen, Think, and Understand). To train LTU, we created a new OpenAQA-5M dataset consisting of 1.9 million closed-ended and 3.7 million open-ended, diverse (audio, question, answer) tuples, and have used an autoregressive training framework with a perception-to-understanding curriculum. LTU demonstrates strong performance and generalization ability on conventional audio tasks such as classification and captioning. More importantly, it exhibits emerging audio reasoning and comprehension abilities that are absent in existing audio models. To the best of our knowledge, LTU is one of the first multimodal large language models that focus on general audio (rather than just speech) understanding.', 'authors': ['James Glass', 'Leonid Karlinsky', 'Alexander H. Liu', 'Hongyin Luo', 'Yuan Gong'], 'published': '2023-05-18', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'what-makes-for-good-visual-tokenizers-for', 'arxiv_id': '2305.12223', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2305.12223v2', 'url_pdf': 'https://arxiv.org/pdf/2305.12223v2.pdf', 'title': 'What Makes for Good Visual Tokenizers for Large Language Models?', 'abstract': 'We empirically investigate proper pre-training methods to build good visual tokenizers, making Large Language Models (LLMs) powerful Multimodal Large Language Models (MLLMs). In our benchmark, which is curated to evaluate MLLMs visual semantic understanding and fine-grained perception capabilities, we discussed different visual tokenizers pre-trained with dominant methods (i.e., DeiT, CLIP, MAE, DINO), and observe that: i) Fully/weakly supervised models capture more semantics than self-supervised models, but the gap is narrowed by scaling up the pre-training dataset. ii) Self-supervised models are better at fine-grained perception, where patch-level supervision is particularly effective. iii) Tuning the visual tokenizer leads to the loss of semantics obtained from large-scale pretraining, which is unfavorable with relatively small-scale instruction-tuning dataset. Given the findings, we reviewed methods that attempted to unify semantics and fine-grained visual understanding, e.g., patch-level feature distillation with semantically-rich targets. We obtain an intriguing insight mask-based strategies that were once all the rage may not be applicable for obtaining good visual tokenizers. Based on this critical observation, we obtain a new MLLM equipped with a tailored Good Visual Tokenizer (GVT), which exhibits strong visual comprehension capability at multiple scales. In particular, without introducing extra parameters and task-specific fine-tuning, GVT achieves superior performance on visual question answering, image captioning, and other fine-grained visual understanding tasks such as object counting and multi-class identification.', 'authors': ['Ying Shan', 'Mohan Kankanhalli', 'Xiaohan Ding', 'Yixiao Ge', 'Guangzhi Wang'], 'published': '2023-05-20', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'interpretable-by-design-visual-question', 'arxiv_id': '2305.14882', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2305.14882v2', 'url_pdf': 'https://arxiv.org/pdf/2305.14882v2.pdf', 'title': 'Dynamic Clue Bottlenecks: Towards Interpretable-by-Design Visual Question Answering', 'abstract': 'Recent advances in multimodal large language models (LLMs) have shown extreme effectiveness in visual question answering (VQA). However, the design nature of these end-to-end models prevents them from being interpretable to humans, undermining trust and applicability in critical domains. While post-hoc rationales offer certain insight into understanding model behavior, these explanations are not guaranteed to be faithful to the model. In this paper, we address these shortcomings by introducing an interpretable by design model that factors model decisions into intermediate human-legible explanations, and allows people to easily understand why a model fails or succeeds. We propose the Dynamic Clue Bottleneck Model ( (DCLUB), a method that is designed towards an inherently interpretable VQA system. DCLUB provides an explainable intermediate space before the VQA decision and is faithful from the beginning, while maintaining comparable performance to black-box systems. Given a question, DCLUB first returns a set of visual clues: natural language statements of visually salient evidence from the image, and then generates the output based solely on the visual clues. To supervise and evaluate the generation of VQA explanations within DCLUB, we collect a dataset of 1.7k reasoning-focused questions with visual clues. Evaluations show that our inherently interpretable system can improve 4.64% over a comparable black-box system in reasoning-focused questions while preserving 99.43% of performance on VQA-v2.', 'authors': ['Dan Roth', 'Mark Yatskar', 'Sihao Chen', 'Ben Zhou', 'Xingyu Fu'], 'published': '2023-05-24', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'pathasst-redefining-pathology-through', 'arxiv_id': '2305.15072', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2305.15072v2', 'url_pdf': 'https://arxiv.org/pdf/2305.15072v2.pdf', 'title': 'PathAsst: A Generative Foundation AI Assistant Towards Artificial General Intelligence of Pathology', 'abstract': \"As advances in large language models (LLMs) and multimodal techniques continue to mature, the development of general-purpose multimodal large language models (MLLMs) has surged, offering significant applications in interpreting natural images. However, the field of pathology has largely remained untapped, particularly in gathering high-quality data and designing comprehensive model frameworks. To bridge the gap in pathology MLLMs, we present PathAsst, a multimodal generative foundation AI assistant to revolutionize diagnostic and predictive analytics in pathology. The development of PathAsst involves three pivotal steps: data acquisition, CLIP model adaptation, and the training of PathAsst's multimodal generative capabilities. Firstly, we collect over 207K high-quality pathology image-text pairs from authoritative sources. Leveraging the advanced power of ChatGPT, we generate over 180K instruction-following samples. Furthermore, we devise additional instruction-following data specifically tailored for invoking eight pathology-specific sub-models we prepared, allowing the PathAsst to effectively collaborate with these models, enhancing its diagnostic ability. Secondly, by leveraging the collected data, we construct PathCLIP, a pathology-dedicated CLIP, to enhance PathAsst's capabilities in interpreting pathology images. Finally, we integrate PathCLIP with the Vicuna-13b and utilize pathology-specific instruction-tuning data to enhance the multimodal generation capacity of PathAsst and bolster its synergistic interactions with sub-models. The experimental results of PathAsst show the potential of harnessing AI-powered generative foundation model to improve pathology diagnosis and treatment processes.\", 'authors': ['Yunlong Zhang', 'Zhongyi Shui', 'Lin Sun', 'Lin Yang', 'Honglin Li', 'Kai Zhang', 'Sunyi Zheng', 'Chenglu Zhu', 'Yuxuan Sun'], 'published': '2023-05-24', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'contextual-object-detection-with-multimodal', 'arxiv_id': '2305.18279', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2305.18279v2', 'url_pdf': 'https://arxiv.org/pdf/2305.18279v2.pdf', 'title': 'Contextual Object Detection with Multimodal Large Language Models', 'abstract': 'Recent Multimodal Large Language Models (MLLMs) are remarkable in vision-language tasks, such as image captioning and question answering, but lack the essential perception ability, i.e., object detection. In this work, we address this limitation by introducing a novel research problem of contextual object detection -- understanding visible objects within different human-AI interactive contexts. Three representative scenarios are investigated, including the language cloze test, visual captioning, and question answering. Moreover, we present ContextDET, a unified multimodal model that is capable of end-to-end differentiable modeling of visual-language contexts, so as to locate, identify, and associate visual objects with language inputs for human-AI interaction. Our ContextDET involves three key submodels: (i) a visual encoder for extracting visual representations, (ii) a pre-trained LLM for multimodal context decoding, and (iii) a visual decoder for predicting bounding boxes given contextual object words. The new generate-then-detect framework enables us to detect object words within human vocabulary. Extensive experiments show the advantages of ContextDET on our proposed CODE benchmark, open-vocabulary detection, and referring image segmentation. Github: https://github.com/yuhangzang/ContextDET.', 'authors': ['Chen Change Loy', 'Kaiyang Zhou', 'Jun Han', 'Wei Li', 'Yuhang Zang'], 'published': '2023-05-29', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'a-survey-on-multimodal-large-language-models', 'arxiv_id': '2306.13549', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2306.13549v4', 'url_pdf': 'https://arxiv.org/pdf/2306.13549v4.pdf', 'title': 'A Survey on Multimodal Large Language Models', 'abstract': 'Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.', 'authors': ['Enhong Chen', 'Tong Xu', 'Xing Sun', 'Ke Li', 'Sirui Zhao', 'Chaoyou Fu', 'Shukang Yin'], 'published': '2023-06-23', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'mme-a-comprehensive-evaluation-benchmark-for', 'arxiv_id': '2306.13394', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2306.13394v4', 'url_pdf': 'https://arxiv.org/pdf/2306.13394v4.pdf', 'title': 'MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models', 'abstract': 'Multimodal Large Language Model (MLLM) relies on the powerful LLM to perform multimodal tasks, showing amazing emergent abilities in recent studies, such as writing poems based on an image. However, it is difficult for these case studies to fully reflect the performance of MLLM, lacking a comprehensive evaluation. In this paper, we fill in this blank, presenting the first comprehensive MLLM Evaluation benchmark MME. It measures both perception and cognition abilities on a total of 14 subtasks. In order to avoid data leakage that may arise from direct use of public datasets for evaluation, the annotations of instruction-answer pairs are all manually designed. The concise instruction design allows us to fairly compare MLLMs, instead of struggling in prompt engineering. Besides, with such an instruction, we can also easily carry out quantitative statistics. A total of 30 advanced MLLMs are comprehensively evaluated on our MME, which not only suggests that existing MLLMs still have a large room for improvement, but also reveals the potential directions for the subsequent model optimization. The data application manner and online leaderboards are released at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation.', 'authors': ['Rongrong Ji', 'Yunsheng Wu', 'Jinrui Yang', 'Xing Sun', 'Ke Li', 'Xiawu Zheng', 'Xu Lin', 'Mengdan Zhang', 'Yulei Qin', 'Yunhang Shen', 'Peixian Chen', 'Chaoyou Fu'], 'published': '2023-06-23', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'kosmos-2-grounding-multimodal-large-language', 'arxiv_id': '2306.14824', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2306.14824v3', 'url_pdf': 'https://arxiv.org/pdf/2306.14824v3.pdf', 'title': 'Kosmos-2: Grounding Multimodal Large Language Models to the World', 'abstract': \"We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. Specifically, we represent refer expressions as links in Markdown, i.e., ``[text span](bounding boxes)'', where object descriptions are sequences of location tokens. Together with multimodal corpora, we construct large-scale data of grounded image-text pairs (called GrIT) to train the model. In addition to the existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), Kosmos-2 integrates the grounding capability into downstream applications. We evaluate Kosmos-2 on a wide range of tasks, including (i) multimodal grounding, such as referring expression comprehension, and phrase grounding, (ii) multimodal referring, such as referring expression generation, (iii) perception-language tasks, and (iv) language understanding and generation. This work lays out the foundation for the development of Embodiment AI and sheds light on the big convergence of language, multimodal perception, action, and world modeling, which is a key step toward artificial general intelligence. Code and pretrained models are available at https://aka.ms/kosmos-2.\", 'authors': ['Furu Wei', 'Shuming Ma', 'Shaohan Huang', 'Yaru Hao', 'Li Dong', 'Wenhui Wang', 'Zhiliang Peng'], 'published': '2023-06-26', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'shikra-unleashing-multimodal-llm-s', 'arxiv_id': '2306.15195', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2306.15195v2', 'url_pdf': 'https://arxiv.org/pdf/2306.15195v2.pdf', 'title': \"Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic\", 'abstract': \"In human conversations, individuals can indicate relevant regions within a scene while addressing others. In turn, the other person can then respond by referring to specific regions if necessary. This natural referential ability in dialogue remains absent in current Multimodal Large Language Models (MLLMs). To fill this gap, this paper proposes an MLLM called Shikra, which can handle spatial coordinate inputs and outputs in natural language. Its architecture consists of a vision encoder, an alignment layer, and a LLM. It is designed to be straightforward and simple, without the need for extra vocabularies, position encoder, pre-/post-detection modules, or external plug-in models. All inputs and outputs are in natural language form. Referential dialogue is a superset of various vision-language (VL) tasks. Shikra can naturally handle location-related tasks like REC and PointQA, as well as conventional VL tasks such as Image Captioning and VQA. Experimental results showcase Shikra's promising performance. Furthermore, it enables numerous exciting applications, like providing mentioned objects' coordinates in chains of thoughts and comparing user-pointed regions similarities. Our code, model and dataset are accessed at https://github.com/shikras/shikra.\", 'authors': ['Rui Zhao', 'Feng Zhu', 'Richong Zhang', 'Weili Zeng', 'Zhao Zhang', 'Keqin Chen'], 'published': '2023-06-27', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'svit-scaling-up-visual-instruction-tuning', 'arxiv_id': '2307.04087', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2307.04087v3', 'url_pdf': 'https://arxiv.org/pdf/2307.04087v3.pdf', 'title': 'SVIT: Scaling up Visual Instruction Tuning', 'abstract': \"Thanks to the emerging of foundation models, the large language and vision models are integrated to acquire the multimodal ability of visual captioning, question answering, etc. Although existing multimodal models present impressive performance of visual understanding and reasoning, their limits are still largely under-explored due to the scarcity of high-quality instruction tuning data. To push the limits of multimodal capability, we Scale up Visual Instruction Tuning (SVIT) by constructing a dataset of 4.2 million visual instruction tuning data including 1.6M conversation question-answer (QA) pairs, 1.6M complex reasoning QA pairs, 1.0M referring QA pairs and 106K detailed image descriptions. Besides the volume, the proposed dataset is also featured by the high quality and rich diversity, which is generated by prompting GPT-4 with the abundant manual annotations of images. We also propose a new data recipe to select subset with better diversity and balance, which evokes model's superior capabilities. Extensive experiments verify that SVIT-v1.5, trained on the proposed dataset, outperforms state-of-the-art Multimodal Large Language Models on popular benchmarks. The data and code are publicly available at https://github.com/BAAI-DCAI/Visual-Instruction-Tuning.\", 'authors': ['Tiejun Huang', 'Muyang He', 'Boya Wu', 'Bo Zhao'], 'published': '2023-07-09', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'chatspot-bootstrapping-multimodal-llms-via', 'arxiv_id': '2307.09474', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2307.09474v1', 'url_pdf': 'https://arxiv.org/pdf/2307.09474v1.pdf', 'title': 'ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning', 'abstract': \"Human-AI interactivity is a critical aspect that reflects the usability of multimodal large language models (MLLMs). However, existing end-to-end MLLMs only allow users to interact with them through language instructions, leading to the limitation of the interactive accuracy and efficiency. In this study, we present precise referring instructions that utilize diverse reference representations such as points and boxes as referring prompts to refer to the special region. This enables MLLMs to focus on the region of interest and achieve finer-grained interaction. Based on precise referring instruction, we propose ChatSpot, a unified end-to-end multimodal large language model that supports diverse forms of interactivity including mouse clicks, drag-and-drop, and drawing boxes, which provides a more flexible and seamless interactive experience. We also construct a multi-grained vision-language instruction-following dataset based on existing datasets and GPT-4 generating. Furthermore, we design a series of evaluation tasks to assess the effectiveness of region recognition and interaction. Experimental results showcase ChatSpot's promising performance.\", 'authors': ['Xiangyu Zhang', 'Chunrui Han', 'Runpei Dong', 'Yuang Peng', 'Jianjian Sun', 'HongYu Zhou', 'Haoran Wei', 'Jinrong Yang', 'Zheng Ge', 'En Yu', 'Liang Zhao'], 'published': '2023-07-18', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'seed-bench-benchmarking-multimodal-llms-with', 'arxiv_id': '2307.16125', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2307.16125v2', 'url_pdf': 'https://arxiv.org/pdf/2307.16125v2.pdf', 'title': 'SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension', 'abstract': 'Based on powerful Large Language Models (LLMs), recent generative Multimodal Large Language Models (MLLMs) have gained prominence as a pivotal research area, exhibiting remarkable capability for both comprehension and generation. In this work, we address the evaluation of generative comprehension in MLLMs as a preliminary step towards a comprehensive assessment of generative models, by introducing a benchmark named SEED-Bench. SEED-Bench consists of 19K multiple choice questions with accurate human annotations (x 6 larger than existing benchmarks), which spans 12 evaluation dimensions including the comprehension of both the image and video modality. We develop an advanced pipeline for generating multiple-choice questions that target specific evaluation dimensions, integrating both automatic filtering and manual verification processes. Multiple-choice questions with groundtruth options derived from human annotation enables an objective and efficient assessment of model performance, eliminating the need for human or GPT intervention during evaluation. We further evaluate the performance of 18 models across all 12 dimensions, covering both the spatial and temporal understanding. By revealing the limitations of existing MLLMs through evaluation results, we aim for SEED-Bench to provide insights for motivating future research. We will launch and consistently maintain a leaderboard to provide a platform for the community to assess and investigate model capability.', 'authors': ['Ying Shan', 'Yixiao Ge', 'Yuying Ge', 'Guangzhi Wang', 'Rui Wang', 'Bohao Li'], 'published': '2023-07-30', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'lisa-reasoning-segmentation-via-large', 'arxiv_id': '2308.00692', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.00692v3', 'url_pdf': 'https://arxiv.org/pdf/2308.00692v3.pdf', 'title': 'LISA: Reasoning Segmentation via Large Language Model', 'abstract': 'Although perception systems have made remarkable advancements in recent years, they still rely on explicit human instruction or pre-defined categories to identify the target objects before executing visual recognition tasks. Such systems cannot actively reason and comprehend implicit user intention. In this work, we propose a new segmentation task -- reasoning segmentation. The task is designed to output a segmentation mask given a complex and implicit query text. Furthermore, we establish a benchmark comprising over one thousand image-instruction-mask data samples, incorporating intricate reasoning and world knowledge for evaluation purposes. Finally, we present LISA: large Language Instructed Segmentation Assistant, which inherits the language generation capabilities of multimodal Large Language Models (LLMs) while also possessing the ability to produce segmentation masks. We expand the original vocabulary with a <SEG> token and propose the embedding-as-mask paradigm to unlock the segmentation capability. Remarkably, LISA can handle cases involving complex reasoning and world knowledge. Also, it demonstrates robust zero-shot capability when trained exclusively on reasoning-free datasets. In addition, fine-tuning the model with merely 239 reasoning segmentation data samples results in further performance enhancement. Both quantitative and qualitative experiments show our method effectively unlocks new reasoning segmentation capabilities for multimodal LLMs. Code, models, and data are available at https://github.com/dvlab-research/LISA.', 'authors': ['Jiaya Jia', 'Shu Liu', 'Yuhui Yuan', 'Yanwei Li', 'Yukang Chen', 'Zhuotao Tian', 'Xin Lai'], 'published': '2023-08-01', 'conference': None, 'conference_url_abs': 'http://openaccess.thecvf.com//content/CVPR2024/html/Lai_LISA_Reasoning_Segmentation_via_Large_Language_Model_CVPR_2024_paper.html', 'conference_url_pdf': 'http://openaccess.thecvf.com//content/CVPR2024/papers/Lai_LISA_Reasoning_Segmentation_via_Large_Language_Model_CVPR_2024_paper.pdf', 'proceeding': 'cvpr-2024-1'}, {'id': 'empowering-vision-language-models-to-follow', 'arxiv_id': '2308.04152', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.04152v4', 'url_pdf': 'https://arxiv.org/pdf/2308.04152v4.pdf', 'title': 'Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions', 'abstract': \"Recent advancements in Multimodal Large Language Models (MLLMs) have been utilizing Visual Prompt Generators (VPGs) to convert visual features into tokens that LLMs can recognize. This is achieved by training the VPGs on millions of image-caption pairs, where the VPG-generated tokens of images are fed into a frozen LLM to generate the corresponding captions. However, this image-captioning based training objective inherently biases the VPG to concentrate solely on the primary visual contents sufficient for caption generation, often neglecting other visual details. This shortcoming results in MLLMs' underperformance in comprehending demonstrative instructions consisting of multiple, interleaved, and multimodal instructions that demonstrate the required context to complete a task. To address this issue, we introduce a generic and lightweight Visual Prompt Generator Complete module (VPG-C), which can infer and complete the missing details essential for comprehending demonstrative instructions. Further, we propose a synthetic discriminative training strategy to fine-tune VPG-C, eliminating the need for supervised demonstrative instructions. As for evaluation, we build DEMON, a comprehensive benchmark for demonstrative instruction understanding. Synthetically trained with the proposed strategy, VPG-C achieves significantly stronger zero-shot performance across all tasks of DEMON. Further evaluation on the MME and OwlEval benchmarks also demonstrate the superiority of VPG-C. Our benchmark, code, and pre-trained models are available at https://github.com/DCDmllm/Cheetah.\", 'authors': ['Wenqiao Zhang', 'Wei Ji', 'Yueting Zhuang', 'Siliang Tang', 'Tat-Seng Chua', 'Hanwang Zhang', 'Minghe Gao', 'Zhiqi Ge', 'Kaihang Pan', 'Juncheng Li'], 'published': '2023-08-08', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'link-context-learning-for-multimodal-llms', 'arxiv_id': '2308.07891', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.07891v1', 'url_pdf': 'https://arxiv.org/pdf/2308.07891v1.pdf', 'title': 'Link-Context Learning for Multimodal LLMs', 'abstract': 'The ability to learn from context with novel concepts, and deliver appropriate responses are essential in human conversations. Despite current Multimodal Large Language Models (MLLMs) and Large Language Models (LLMs) being trained on mega-scale datasets, recognizing unseen images or understanding novel concepts in a training-free manner remains a challenge. In-Context Learning (ICL) explores training-free few-shot learning, where models are encouraged to ``learn to learn\" from limited tasks and generalize to unseen tasks. In this work, we propose link-context learning (LCL), which emphasizes \"reasoning from cause and effect\" to augment the learning capabilities of MLLMs. LCL goes beyond traditional ICL by explicitly strengthening the causal relationship between the support set and the query set. By providing demonstrations with causal links, LCL guides the model to discern not only the analogy but also the underlying causal associations between data points, which empowers MLLMs to recognize unseen images and understand novel concepts more effectively. To facilitate the evaluation of this novel approach, we introduce the ISEKAI dataset, comprising exclusively of unseen generated image-label pairs designed for link-context learning. Extensive experiments show that our LCL-MLLM exhibits strong link-context learning capabilities to novel concepts over vanilla MLLMs. Code and data will be released at https://github.com/isekai-portal/Link-Context-Learning.', 'authors': ['Ziwei Liu', 'Rui Zhao', 'Feng Zhu', 'Zhao Zhang', 'Weichen Fan', 'Yan Tai'], 'published': '2023-08-15', 'conference': None, 'conference_url_abs': 'http://openaccess.thecvf.com//content/CVPR2024/html/Tai_Link-Context_Learning_for_Multimodal_LLMs_CVPR_2024_paper.html', 'conference_url_pdf': 'http://openaccess.thecvf.com//content/CVPR2024/papers/Tai_Link-Context_Learning_for_Multimodal_LLMs_CVPR_2024_paper.pdf', 'proceeding': 'cvpr-2024-1'}, {'id': 'fashionlogo-prompting-multimodal-large', 'arxiv_id': '2308.09012', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.09012v2', 'url_pdf': 'https://arxiv.org/pdf/2308.09012v2.pdf', 'title': 'FashionLOGO: Prompting Multimodal Large Language Models for Fashion Logo Embeddings', 'abstract': \"Logo embedding models convert the product logos in images into vectors, enabling their utilization for logo recognition and detection within e-commerce platforms. This facilitates the enforcement of intellectual property rights and enhances product search capabilities. However, current methods treat logo embedding as a purely visual problem. A noteworthy issue is that visual models capture features more than logos. Instead, we view this as a multimodal task, using text as auxiliary information to facilitate the visual model's understanding of the logo. The emerging Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in both visual and textual understanding. Inspired by this, we propose an approach, \\\\textbf{FashionLOGO}, to explore how to prompt MLLMs to generate appropriate text for product images, which can help visual models achieve better logo embeddings. We adopt a cross-attention transformer block that enables visual embedding to automatically learn supplementary knowledge from textual embedding. Our extensive experiments on real-world datasets prove that FashionLOGO is capable of generating generic and robust logo embeddings, achieving state-of-the-art performance in all benchmarks.\", 'authors': ['Walton Wang', 'Minghui Qiu', 'Min Yang', 'Yulin Su', 'Da Li', 'Zhen Wang'], 'published': '2023-08-17', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'wanjuan-a-comprehensive-multimodal-dataset', 'arxiv_id': '2308.10755', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.10755v3', 'url_pdf': 'https://arxiv.org/pdf/2308.10755v3.pdf', 'title': 'WanJuan: A Comprehensive Multimodal Dataset for Advancing English and Chinese Large Models', 'abstract': 'The rise in popularity of ChatGPT and GPT-4 has significantly accelerated the development of large models, leading to the creation of numerous impressive large language models(LLMs) and multimodal large language models (MLLMs). These cutting-edge models owe their remarkable performance to high-quality data. However, the details of the training data used in leading paradigms are often kept confidential. This lack of transparency, coupled with the scarcity of open-source data, impedes further developments within the community. As a response, this paper presents \"Wan Juan\", a large-scale multimodal dataset composed of both Chinese and English data, collected from a wide range of web sources. The dataset incorporates text, image-text, and video modalities, with a total volume exceeding 2TB. It was utilized in the training of InternLM, a model that demonstrated significant advantages in multi-dimensional evaluations when compared to models of a similar scale. All data can be accessed at https://opendatalab.org.cn/WanJuan1.0.', 'authors': ['Dahua Lin', 'Jiaqi Wang', 'Hang Yan', 'Wei Li', 'Bin Wang', 'Jiantao Qiu', 'Chao Xu', 'Zhenjiang Jin', 'Conghui He'], 'published': '2023-08-21', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'stablellava-enhanced-visual-instruction', 'arxiv_id': '2308.10253', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.10253v2', 'url_pdf': 'https://arxiv.org/pdf/2308.10253v2.pdf', 'title': 'StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data', 'abstract': \"The remarkable multimodal capabilities demonstrated by OpenAI's GPT-4 have sparked significant interest in the development of multimodal Large Language Models (LLMs). A primary research objective of such models is to align visual and textual modalities effectively while comprehending human instructions. Current methodologies often rely on annotations derived from benchmark datasets to construct image-dialogue datasets for training purposes, akin to instruction tuning in LLMs. However, these datasets often exhibit domain bias, potentially constraining the generative capabilities of the models. In an effort to mitigate these limitations, we propose a novel data collection methodology that synchronously synthesizes images and dialogues for visual instruction tuning. This approach harnesses the power of generative models, marrying the abilities of ChatGPT and text-to-image generative models to yield a diverse and controllable dataset with varied image content. Additionally, datasets can be arbitrarily scaled. This not only provides greater flexibility compared to existing methodologies but also significantly enhances several model capabilities. Our research includes comprehensive experiments conducted on various datasets. The results emphasize substantial enhancements in more than ten commonly assessed capabilities. Additionally, our model achieves state-of-the-art results across multiple widely recognized multimodal benchmarks.\", 'authors': ['Yunchao Wei', 'Ling Chen', 'Chunhua Shen', 'Guosheng Lin', 'Bin Fu', 'Zhibin Wang', 'Gang Yu', 'Chi Zhang', 'Yanda Li'], 'published': '2023-08-20', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'instructiongpt-4-a-200-instruction-paradigm', 'arxiv_id': '2308.12067', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.12067v2', 'url_pdf': 'https://arxiv.org/pdf/2308.12067v2.pdf', 'title': 'InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4', 'abstract': 'Multimodal large language models are typically trained in two stages: first pre-training on image-text pairs, and then fine-tuning using supervised vision-language instruction data. Recent studies have shown that large language models can achieve satisfactory results even with a limited amount of high-quality instruction-following data. In this paper, we introduce InstructionGPT-4, which is fine-tuned on a small dataset comprising only 200 examples, amounting to approximately 6\\\\% of the instruction-following data used in the alignment dataset for MiniGPT-4. To achieve this, we first propose several metrics to access the quality of multimodal instruction data. Based on these metrics, we present an effective and trainable data selector to automatically identify and filter low-quality vision-language data. By employing this method, InstructionGPT-4 outperforms the original MiniGPT-4 on various evaluations. Overall, our findings demonstrate that less but high-quality instruction tuning data is efficient in enabling multimodal large language models to generate better output. Our code is available at https://github.com/waltonfuture/InstructionGPT-4.', 'authors': ['Lichao Sun', 'Weiran Huang', 'Zihao Jiang', 'Lai Wei'], 'published': '2023-08-23', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'vigc-visual-instruction-generation-and', 'arxiv_id': '2308.12714', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.12714v3', 'url_pdf': 'https://arxiv.org/pdf/2308.12714v3.pdf', 'title': 'VIGC: Visual Instruction Generation and Correction', 'abstract': \"The integration of visual encoders and large language models (LLMs) has driven recent progress in multimodal large language models (MLLMs). However, the scarcity of high-quality instruction-tuning data for vision-language tasks remains a challenge. The current leading paradigm, such as LLaVA, relies on language-only GPT-4 to generate data, which requires pre-annotated image captions and detection bounding boxes, suffering from understanding image details. A practical solution to this problem would be to utilize the available multimodal large language models (MLLMs) to generate instruction data for vision-language tasks. However, it's worth noting that the currently accessible MLLMs are not as powerful as their LLM counterparts, as they tend to produce inadequate responses and generate false information. As a solution for addressing the current issue, this paper proposes the Visual Instruction Generation and Correction (VIGC) framework that enables multimodal large language models to generate instruction-tuning data and progressively enhance its quality on-the-fly. Specifically, Visual Instruction Generation (VIG) guides the vision-language model to generate diverse instruction-tuning data. To ensure generation quality, Visual Instruction Correction (VIC) adopts an iterative update mechanism to correct any inaccuracies in data produced by VIG, effectively reducing the risk of hallucination. Leveraging the diverse, high-quality data generated by VIGC, we finetune mainstream models and validate data quality based on various evaluations. Experimental results demonstrate that VIGC not only compensates for the shortcomings of language-only data generation methods, but also effectively enhances the benchmark performance. The models, datasets, and code are available at https://opendatalab.github.io/VIGC.\", 'authors': ['Conghui He', 'Jiaqi Wang', 'Wei Li', 'Weijia Li', 'Xiaoyi Dong', 'Pan Zhang', 'Huaping Zhong', 'Jiahui Peng', 'Xiao Han', 'Fan Wu', 'Bin Wang'], 'published': '2023-08-24', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'position-enhanced-visual-instruction-tuning', 'arxiv_id': '2308.13437', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.13437v2', 'url_pdf': 'https://arxiv.org/pdf/2308.13437v2.pdf', 'title': 'Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models', 'abstract': 'Recently, Multimodal Large Language Models (MLLMs) that enable Large Language Models (LLMs) to interpret images through visual instruction tuning have achieved significant success. However, existing visual instruction tuning methods only utilize image-language instruction data to align the language and image modalities, lacking a more fine-grained cross-modal alignment. In this paper, we propose Position-enhanced Visual Instruction Tuning (PVIT), which extends the functionality of MLLMs by integrating an additional region-level vision encoder. This integration promotes a more detailed comprehension of images for the MLLM. In addition, to efficiently achieve a fine-grained alignment between the vision modules and the LLM, we design multiple data generation strategies to construct an image-region-language instruction dataset. Finally, we present both quantitative experiments and qualitative analysis that demonstrate the superiority of the proposed model. Code and data will be released at https://github.com/PVIT-official/PVIT.', 'authors': ['Yang Liu', 'Maosong Sun', 'Peng Li', 'Xiaoyue Mi', 'Fuwen Luo', 'Ruoyu Qin', 'Chi Chen'], 'published': '2023-08-25', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'mllm-dataengine-an-iterative-refinement', 'arxiv_id': '2308.13566', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.13566v2', 'url_pdf': 'https://arxiv.org/pdf/2308.13566v2.pdf', 'title': 'MLLM-DataEngine: An Iterative Refinement Approach for MLLM', 'abstract': 'Despite the great advance of Multimodal Large Language Models (MLLMs) in both instruction dataset building and benchmarking, the independence of training and evaluation makes current MLLMs hard to further improve their capability under the guidance of evaluation results with a relatively low human cost. In this paper, we propose MLLM-DataEngine, a novel closed-loop system that bridges data generation, model training, and evaluation. Within each loop iteration, the MLLM-DataEngine first analyze the weakness of the model based on the evaluation results, then generate a proper incremental dataset for the next training iteration and enhance the model capability iteratively. Compared with previous data collection methods which are separate from the benchmarking, the data generated by MLLM-DataEngine shows better targeting, quality, and correctness. For targeting, we propose an Adaptive Bad-case Sampling module, which adjusts the ratio of different types of data within each incremental dataset based on the benchmarking results. For quality, we resort to GPT-4 to generate high-quality data with each given data type. For correctness, prompt design is critical for the data generation results. Rather than previous hand-crafted prompt, we propose an Interactive Prompt Optimization strategy, which optimizes the prompt with the multi-round interaction between human and GPT, and improve the correctness of generated data greatly. Through extensive experiments, we find our MLLM-DataEngine could boost the MLLM capability in a targeted and automatic manner, with only a few human participation. We hope it could be a general solution for the following MLLMs building. The MLLM-DataEngine has been open-sourced and is now available at https://github.com/opendatalab/MLLM-DataEngine.', 'authors': ['Conghui He', 'Jiaqi Wang', 'Xiaoyi Dong', 'Pan Zhang', 'Siyuan Huang', 'Bin Wang', 'Linke Ouyang', 'Zhiyuan Zhao'], 'published': '2023-08-25', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'socratis-are-large-multimodal-models', 'arxiv_id': '2308.16741', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.16741v3', 'url_pdf': 'https://arxiv.org/pdf/2308.16741v3.pdf', 'title': 'Socratis: Are large multimodal models emotionally aware?', 'abstract': 'Existing emotion prediction benchmarks contain coarse emotion labels which do not consider the diversity of emotions that an image and text can elicit in humans due to various reasons. Learning diverse reactions to multimodal content is important as intelligent machines take a central role in generating and delivering content to society. To address this gap, we propose Socratis, a societal reactions benchmark, where each image-caption (IC) pair is annotated with multiple emotions and the reasons for feeling them. Socratis contains 18K free-form reactions for 980 emotions on 2075 image-caption pairs from 5 widely-read news and image-caption (IC) datasets. We benchmark the capability of state-of-the-art multimodal large language models to generate the reasons for feeling an emotion given an IC pair. Based on a preliminary human study, we observe that humans prefer human-written reasons over 2 times more often than machine-generated ones. This shows our task is harder than standard generation tasks because it starkly contrasts recent findings where humans cannot tell apart machine vs human-written news articles, for instance. We further see that current captioning metrics based on large vision-language models also fail to correlate with human preferences. We hope that these findings and our benchmark will inspire further research on training emotionally aware models.', 'authors': ['Kate Saenko', 'Bryan A. Plummer', 'Saadia Gabriel', 'Reuben Tan', 'Arijit Ray', 'Katherine Deng'], 'published': '2023-08-31', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'multiway-adapater-adapting-large-scale-multi', 'arxiv_id': '2309.01516', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.01516v3', 'url_pdf': 'https://arxiv.org/pdf/2309.01516v3.pdf', 'title': 'MultiWay-Adapater: Adapting large-scale multi-modal models for scalable image-text retrieval', 'abstract': \"As Multimodal Large Language Models (MLLMs) grow in size, adapting them to specialized tasks becomes increasingly challenging due to high computational and memory demands. Indeed, traditional fine-tuning methods are costly, due to the need for extensive, task-specific training. While efficient adaptation methods exist that aim to reduce these costs, in practice they suffer from shallow inter-modal alignment, which severely hurts model effectiveness. To tackle these computational challenges and improve inter-modal alignment, we introduce the MultiWay-Adapter (MWA), a novel framework featuring an 'Alignment Enhancer'. This enhancer deepens inter-modal alignment, enabling high transferability with minimal tuning effort. Our experiments show that unlike prior efficient tuning approaches, MWA maintains model effectiveness, while reducing training time by up-to 57%. MWA is also lightweight, increasing model size by only 2-3% (in terms of parameters) for state-of-the-art foundation models like BEiT-3 Large. These results demonstrate that MWA provides an efficient and effective adaptation method for MLLMs, significantly broadening their applicability.\", 'authors': ['Gerardo Aragon Camarasa', 'Richard McCreadie', 'George Killick', 'Zijun Long'], 'published': '2023-09-04', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'next-gpt-any-to-any-multimodal-llm', 'arxiv_id': '2309.05519', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.05519v3', 'url_pdf': 'https://arxiv.org/pdf/2309.05519v3.pdf', 'title': 'NExT-GPT: Any-to-Any Multimodal LLM', 'abstract': 'While recently Multimodal Large Language Models (MM-LLMs) have made exciting strides, they mostly fall prey to the limitation of only input-side multimodal understanding, without the ability to produce content in multiple modalities. As we humans always perceive the world and communicate with people through various modalities, developing any-to-any MM-LLMs capable of accepting and delivering content in any modality becomes essential to human-level AI. To fill the gap, we present an end-to-end general-purpose any-to-any MM-LLM system, NExT-GPT. We connect an LLM with multimodal adaptors and different diffusion decoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio. By leveraging the existing well-trained highly-performing encoders and decoders, NExT-GPT is tuned with only a small amount of parameter (1%) of certain projection layers, which not only benefits low-cost training and also facilitates convenient expansion to more potential modalities. Moreover, we introduce a modality-switching instruction tuning (MosIT) and manually curate a high-quality dataset for MosIT, based on which NExT-GPT is empowered with complex cross-modal semantic understanding and content generation. Overall, our research showcases the promising possibility of building an AI agent capable of modeling universal modalities, paving the way for more human-like AI research in the community. Project page: https://next-gpt.github.io/', 'authors': ['Tat-Seng Chua', 'Wei Ji', 'Leigang Qu', 'Hao Fei', 'Shengqiong Wu'], 'published': '2023-09-11', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'hilm-d-towards-high-resolution-understanding', 'arxiv_id': '2309.05186', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.05186v1', 'url_pdf': 'https://arxiv.org/pdf/2309.05186v1.pdf', 'title': 'HiLM-D: Towards High-Resolution Understanding in Multimodal Large Language Models for Autonomous Driving', 'abstract': \"Autonomous driving systems generally employ separate models for different tasks resulting in intricate designs. For the first time, we leverage singular multimodal large language models (MLLMs) to consolidate multiple autonomous driving tasks from videos, i.e., the Risk Object Localization and Intention and Suggestion Prediction (ROLISP) task. ROLISP uses natural language to simultaneously identify and interpret risk objects, understand ego-vehicle intentions, and provide motion suggestions, eliminating the necessity for task-specific architectures. However, lacking high-resolution (HR) information, existing MLLMs often miss small objects (e.g., traffic cones) and overly focus on salient ones (e.g., large trucks) when applied to ROLISP. We propose HiLM-D (Towards High-Resolution Understanding in MLLMs for Autonomous Driving), an efficient method to incorporate HR information into MLLMs for the ROLISP task. Especially, HiLM-D integrates two branches: (i) the low-resolution reasoning branch, can be any MLLMs, processes low-resolution videos to caption risk objects and discern ego-vehicle intentions/suggestions; (ii) the high-resolution perception branch (HR-PB), prominent to HiLM-D,, ingests HR images to enhance detection by capturing vision-specific HR feature maps and prioritizing all potential risks over merely salient objects. Our HR-PB serves as a plug-and-play module, seamlessly fitting into current MLLMs. Experiments on the ROLISP benchmark reveal HiLM-D's notable advantage over leading MLLMs, with improvements of 4.8% in BLEU-4 for captioning and 17.2% in mIoU for detection.\", 'authors': ['Xiaomeng Li', 'Wei zhang', 'Hang Xu', 'Jianhua Han', 'Xinpeng Ding'], 'published': '2023-09-11', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'investigating-the-catastrophic-forgetting-in', 'arxiv_id': '2309.10313', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.10313v4', 'url_pdf': 'https://arxiv.org/pdf/2309.10313v4.pdf', 'title': 'Investigating the Catastrophic Forgetting in Multimodal Large Language Models', 'abstract': 'Following the success of GPT4, there has been a surge in interest in multimodal large language model (MLLM) research. This line of research focuses on developing general-purpose LLMs through fine-tuning pre-trained LLMs and vision models. However, catastrophic forgetting, a notorious phenomenon where the fine-tuned model fails to retain similar performance compared to the pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM). In this paper, we introduce EMT: Evaluating MulTimodality for evaluating the catastrophic forgetting in MLLMs, by treating each MLLM as an image classifier. We first apply EMT to evaluate several open-source fine-tuned MLLMs and we discover that almost all evaluated MLLMs fail to retain the same performance levels as their vision encoders on standard image classification tasks. Moreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess performance throughout the fine-tuning. Interestingly, our results suggest that early-stage fine-tuning on an image dataset improves performance across other image datasets, by enhancing the alignment of text and visual features. However, as fine-tuning proceeds, the MLLMs begin to hallucinate, resulting in a significant loss of generalizability, even when the image encoder remains frozen. Our results suggest that MLLMs have yet to demonstrate performance on par with their vision models on standard image classification tasks and the current MLLM fine-tuning procedure still has room for improvement.', 'authors': ['Yi Ma', 'Yong Jae Lee', 'Qing Qu', 'Mu Cai', 'Xiao Li', 'Shengbang Tong', 'Yuexiang Zhai'], 'published': '2023-09-19', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'dreamllm-synergistic-multimodal-comprehension', 'arxiv_id': '2309.11499', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.11499v2', 'url_pdf': 'https://arxiv.org/pdf/2309.11499v2.pdf', 'title': 'DreamLLM: Synergistic Multimodal Comprehension and Creation', 'abstract': \"This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy. Project page: https://dreamllm.github.io.\", 'authors': ['Li Yi', 'Kaisheng Ma', 'Xiangyu Zhang', 'Xiangwen Kong', 'Haoran Wei', 'HongYu Zhou', 'Jianjian Sun', 'Liang Zhao', 'Jinrong Yang', 'Zheng Ge', 'Zekun Qi', 'Yuang Peng', 'Chunrui Han', 'Runpei Dong'], 'published': '2023-09-20', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'how-robust-is-google-s-bard-to-adversarial', 'arxiv_id': '2309.11751', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.11751v2', 'url_pdf': 'https://arxiv.org/pdf/2309.11751v2.pdf', 'title': \"How Robust is Google's Bard to Adversarial Image Attacks?\", 'abstract': \"Multimodal Large Language Models (MLLMs) that integrate text and other modalities (especially vision) have achieved unprecedented performance in various multimodal tasks. However, due to the unsolved adversarial robustness problem of vision models, MLLMs can have more severe safety and security risks by introducing the vision inputs. In this work, we study the adversarial robustness of Google's Bard, a competitive chatbot to ChatGPT that released its multimodal capability recently, to better understand the vulnerabilities of commercial MLLMs. By attacking white-box surrogate vision encoders or MLLMs, the generated adversarial examples can mislead Bard to output wrong image descriptions with a 22% success rate based solely on the transferability. We show that the adversarial examples can also attack other MLLMs, e.g., a 26% attack success rate against Bing Chat and a 86% attack success rate against ERNIE bot. Moreover, we identify two defense mechanisms of Bard, including face detection and toxicity detection of images. We design corresponding attacks to evade these defenses, demonstrating that the current defenses of Bard are also vulnerable. We hope this work can deepen our understanding on the robustness of MLLMs and facilitate future research on defenses. Our code is available at https://github.com/thu-ml/Attack-Bard. Update: GPT-4V is available at October 2023. We further evaluate its robustness under the same set of adversarial examples, achieving a 45% attack success rate.\", 'authors': ['Jun Zhu', 'Hang Su', 'Yu Tian', 'Yichi Zhang', 'Xiao Yang', 'Zhengwei Fang', 'Jiawei Chen', 'Huanran Chen', 'Yinpeng Dong'], 'published': '2023-09-21', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'vdc-versatile-data-cleanser-for-detecting', 'arxiv_id': '2309.16211', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.16211v2', 'url_pdf': 'https://arxiv.org/pdf/2309.16211v2.pdf', 'title': 'VDC: Versatile Data Cleanser based on Visual-Linguistic Inconsistency by Multimodal Large Language Models', 'abstract': 'The role of data in building AI systems has recently been emphasized by the emerging concept of data-centric AI. Unfortunately, in the real-world, datasets may contain dirty samples, such as poisoned samples from backdoor attack, noisy labels in crowdsourcing, and even hybrids of them. The presence of such dirty samples makes the DNNs vunerable and unreliable.Hence, it is critical to detect dirty samples to improve the quality and realiability of dataset. Existing detectors only focus on detecting poisoned samples or noisy labels, that are often prone to weak generalization when dealing with dirty samples from other domains.In this paper, we find a commonality of various dirty samples is visual-linguistic inconsistency between images and associated labels. To capture the semantic inconsistency between modalities, we propose versatile data cleanser (VDC) leveraging the surpassing capabilities of multimodal large language models (MLLM) in cross-modal alignment and reasoning.It consists of three consecutive modules: the visual question generation module to generate insightful questions about the image; the visual question answering module to acquire the semantics of the visual content by answering the questions with MLLM; followed by the visual answer evaluation module to evaluate the inconsistency.Extensive experiments demonstrate its superior performance and generalization to various categories and types of dirty samples. The code is available at \\\\url{https://github.com/zihao-ai/vdc}.', 'authors': ['Baoyuan Wu', 'Bingzhe Wu', 'Shaokui Wei', 'Mingda Zhang', 'Zihao Zhu'], 'published': '2023-09-28', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'guiding-instruction-based-image-editing-via', 'arxiv_id': '2309.17102', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.17102v2', 'url_pdf': 'https://arxiv.org/pdf/2309.17102v2.pdf', 'title': 'Guiding Instruction-based Image Editing via Multimodal Large Language Models', 'abstract': 'Instruction-based image editing improves the controllability and flexibility of image manipulation via natural commands without elaborate descriptions or regional masks. However, human instructions are sometimes too brief for current methods to capture and follow. Multimodal large language models (MLLMs) show promising capabilities in cross-modal understanding and visual-aware response generation via LMs. We investigate how MLLMs facilitate edit instructions and present MLLM-Guided Image Editing (MGIE). MGIE learns to derive expressive instructions and provides explicit guidance. The editing model jointly captures this visual imagination and performs manipulation through end-to-end training. We evaluate various aspects of Photoshop-style modification, global photo optimization, and local editing. Extensive experimental results demonstrate that expressive instructions are crucial to instruction-based image editing, and our MGIE can lead to a notable improvement in automatic metrics and human evaluation while maintaining competitive inference efficiency.', 'authors': ['Zhe Gan', 'Yinfei Yang', 'William Yang Wang', 'Xianzhi Du', 'Wenze Hu', 'Tsu-Jui Fu'], 'published': '2023-09-29', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'reformulating-vision-language-foundation', 'arxiv_id': '2310.00653', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.00653v1', 'url_pdf': 'https://arxiv.org/pdf/2310.00653v1.pdf', 'title': 'Reformulating Vision-Language Foundation Models and Datasets Towards Universal Multimodal Assistants', 'abstract': \"Recent Multimodal Large Language Models (MLLMs) exhibit impressive abilities to perceive images and follow open-ended instructions. The capabilities of MLLMs depend on two crucial factors: the model architecture to facilitate the feature alignment of visual modules and large language models; the multimodal instruction tuning datasets for human instruction following. (i) For the model architecture, most existing models introduce an external bridge module to connect vision encoders with language models, which needs an additional feature-alignment pre-training. In this work, we discover that compact pre-trained vision language models can inherently serve as ``out-of-the-box'' bridges between vision and language. Based on this, we propose Muffin framework, which directly employs pre-trained vision-language models to act as providers of visual signals. (ii) For the multimodal instruction tuning datasets, existing methods omit the complementary relationship between different datasets and simply mix datasets from different tasks. Instead, we propose UniMM-Chat dataset which explores the complementarities of datasets to generate 1.1M high-quality and diverse multimodal instructions. We merge information describing the same image from diverse datasets and transforms it into more knowledge-intensive conversation data. Experimental results demonstrate the effectiveness of the Muffin framework and UniMM-Chat dataset. Muffin achieves state-of-the-art performance on a wide range of vision-language tasks, significantly surpassing state-of-the-art models like LLaVA and InstructBLIP. Our model and dataset are all accessible at https://github.com/thunlp/muffin.\", 'authors': ['Maosong Sun', 'Hai-Tao Zheng', 'Zhiyuan Liu', 'Dahai Li', 'Jiao Xue', 'Yinxv Pan', 'Shan Wang', 'Chongyi Wang', 'Yue Zhao', 'Haoye Zhang', 'Yuan YAO', 'Jinyi Hu', 'Tianyu Yu'], 'published': '2023-10-01', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'drivegpt4-interpretable-end-to-end-autonomous', 'arxiv_id': '2310.01412', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.01412v5', 'url_pdf': 'https://arxiv.org/pdf/2310.01412v5.pdf', 'title': 'DriveGPT4: Interpretable End-to-end Autonomous Driving via Large Language Model', 'abstract': 'Multimodal large language models (MLLMs) have emerged as a prominent area of interest within the research community, given their proficiency in handling and reasoning with non-textual data, including images and videos. This study seeks to extend the application of MLLMs to the realm of autonomous driving by introducing DriveGPT4, a novel interpretable end-to-end autonomous driving system based on LLMs. Capable of processing multi-frame video inputs and textual queries, DriveGPT4 facilitates the interpretation of vehicle actions, offers pertinent reasoning, and effectively addresses a diverse range of questions posed by users. Furthermore, DriveGPT4 predicts low-level vehicle control signals in an end-to-end fashion.These advanced capabilities are achieved through the utilization of a bespoke visual instruction tuning dataset, specifically tailored for autonomous driving applications, in conjunction with a mix-finetuning training strategy. DriveGPT4 represents the pioneering effort to leverage LLMs for the development of an interpretable end-to-end autonomous driving solution. Evaluations conducted on the BDD-X dataset showcase the superior qualitative and quantitative performance of DriveGPT4. Additionally, the fine-tuning of domain-specific data enables DriveGPT4 to yield close or even improved results in terms of autonomous driving grounding when contrasted with GPT4-V.', 'authors': ['Kwan-Yee. K. Wong', 'Hengshuang Zhao', 'Zhenguo Li', 'Yong Guo', 'Zhen Zhao', 'Enze Xie', 'Yujia Zhang', 'Zhenhua Xu'], 'published': '2023-10-02', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'minigpt-5-interleaved-vision-and-language', 'arxiv_id': '2310.02239', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.02239v3', 'url_pdf': 'https://arxiv.org/pdf/2310.02239v3.pdf', 'title': 'MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens', 'abstract': 'The effectiveness of Multimodal Large Language Models (MLLMs) demonstrates a profound capability in multimodal understanding. However, the simultaneous generation of images with coherent texts is still underdeveloped. Addressing this, we introduce a novel interleaved vision-and-language generation method, centered around the concept of ``generative vokens\". These vokens serve as pivotal elements contributing to coherent image-text outputs. Our method is marked by a unique two-stage training strategy for description-free multimodal generation, which does not necessitate extensive descriptions of images. We integrate classifier-free guidance to enhance the alignment of generated images and texts, ensuring more seamless and contextually relevant multimodal interactions. Our model, MiniGPT-5, exhibits substantial improvement over the baseline models on multimodal generation datasets, including MMDialog and VIST. The human evaluation shows MiniGPT-5 is better than the baseline model on more than 56\\\\% cases for multimodal generation, highlighting its efficacy across diverse benchmarks.', 'authors': ['Xin Eric Wang', 'Xuehai He', 'Kaizhi Zheng'], 'published': '2023-10-03', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'towards-end-to-end-embodied-decision-making', 'arxiv_id': '2310.02071', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.02071v4', 'url_pdf': 'https://arxiv.org/pdf/2310.02071v4.pdf', 'title': 'Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond', 'abstract': 'In this study, we explore the potential of Multimodal Large Language Models (MLLMs) in improving embodied decision-making processes for agents. While Large Language Models (LLMs) have been widely used due to their advanced reasoning skills and vast world knowledge, MLLMs like GPT4-Vision offer enhanced visual understanding and reasoning capabilities. We investigate whether state-of-the-art MLLMs can handle embodied decision-making in an end-to-end manner and whether collaborations between LLMs and MLLMs can enhance decision-making. To address these questions, we introduce a new benchmark called PCA-EVAL, which evaluates embodied decision-making from the perspectives of Perception, Cognition, and Action. Additionally, we propose HOLMES, a multi-agent cooperation framework that allows LLMs to leverage MLLMs and APIs to gather multimodal information for informed decision-making. We compare end-to-end embodied decision-making and HOLMES on our benchmark and find that the GPT4-Vision model demonstrates strong end-to-end embodied decision-making abilities, outperforming GPT4-HOLMES in terms of average decision accuracy (+3%). However, this performance is exclusive to the latest GPT4-Vision model, surpassing the open-source state-of-the-art MLLM by 26%. Our results indicate that powerful MLLMs like GPT4-Vision hold promise for decision-making in embodied agents, offering new avenues for MLLM research. Code and data are open at https://github.com/pkunlp-icler/PCA-EVAL/.', 'authors': ['Peiyi Wang', 'Baobao Chang', 'Tianyu Liu', 'Yuchi Wang', 'Zefan Cai', 'Haozhe Zhao', 'Shuhuai Ren', 'Yichi Zhang', 'Liang Chen'], 'published': '2023-10-03', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'kosmos-g-generating-images-in-context-with', 'arxiv_id': '2310.02992', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.02992v3', 'url_pdf': 'https://arxiv.org/pdf/2310.02992v3.pdf', 'title': 'Kosmos-G: Generating Images in Context with Multimodal Large Language Models', 'abstract': 'Recent advancements in subject-driven image generation have made significant strides. However, current methods still fall short in diverse application scenarios, as they require test-time tuning and cannot accept interleaved multi-image and text input. These limitations keep them far from the ultimate goal of \"image as a foreign language in image generation.\" This paper presents Kosmos-G, a model that leverages the advanced multimodal perception capabilities of Multimodal Large Language Models (MLLMs) to tackle the aforementioned challenge. Our approach aligns the output space of MLLM with CLIP using the textual modality as an anchor and performs compositional instruction tuning on curated data. Kosmos-G demonstrates an impressive capability of zero-shot subject-driven generation with interleaved multi-image and text input. Notably, the score distillation instruction tuning requires no modifications to the image decoder. This allows for a seamless substitution of CLIP and effortless integration with a myriad of U-Net techniques ranging from fine-grained controls to personalized image decoder variants. We posit Kosmos-G as an initial attempt towards the goal of \"image as a foreign language in image generation.\" The code can be found at https://aka.ms/Kosmos-G', 'authors': ['Furu Wei', 'Wenhu Chen', 'Zhiliang Peng', 'Shaohan Huang', 'Li Dong', 'Xichen Pan'], 'published': '2023-10-04', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'fine-grained-audio-visual-joint', 'arxiv_id': '2310.05863', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.05863v2', 'url_pdf': 'https://arxiv.org/pdf/2310.05863v2.pdf', 'title': 'Fine-grained Audio-Visual Joint Representations for Multimodal Large Language Models', 'abstract': 'Audio-visual large language models (LLM) have drawn significant attention, yet the fine-grained combination of both input streams is rather under-explored, which is challenging but necessary for LLMs to understand general video inputs. To this end, a fine-grained audio-visual joint representation (FAVOR) learning framework for multimodal LLMs is proposed in this paper, which extends a text-based LLM to simultaneously perceive speech and audio events in the audio input stream and images or videos in the visual input stream, at the frame level. To fuse the audio and visual feature streams into joint representations and to align the joint space with the LLM input embedding space, we propose a causal Q-Former structure with a causal attention module to enhance the capture of causal relations of the audio-visual frames across time. An audio-visual evaluation benchmark (AVEB) is also proposed which comprises six representative single-modal tasks with five cross-modal tasks reflecting audio-visual co-reasoning abilities. While achieving competitive single-modal performance on audio, speech and image tasks in AVEB, FAVOR achieved over 20% accuracy improvements on the video question-answering task when fine-grained information or temporal causal reasoning is required. FAVOR, in addition, demonstrated remarkable video comprehension and reasoning abilities on tasks that are unprecedented by other multimodal LLMs. An interactive demo of FAVOR is available at https://github.com/BriansIDP/AudioVisualLLM.git, and the training code and model checkpoints will be released soon.', 'authors': ['Chao Zhang', 'Zejun Ma', 'Lu Lu', 'Wei Li', 'Tian Tan', 'Xianzhao Chen', 'Changli Tang', 'Wenyi Yu', 'Guangzhi Sun'], 'published': '2023-10-09', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'can-we-edit-multimodal-large-language-models', 'arxiv_id': '2310.08475', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.08475v5', 'url_pdf': 'https://arxiv.org/pdf/2310.08475v5.pdf', 'title': 'Can We Edit Multimodal Large Language Models?', 'abstract': 'In this paper, we focus on editing Multimodal Large Language Models (MLLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights. Code and dataset are available in https://github.com/zjunlp/EasyEdit.', 'authors': ['Ningyu Zhang', 'Huajun Chen', 'Yongheng Wang', 'Xi Chen', 'Qingbin Liu', 'Bozhong Tian', 'Siyuan Cheng'], 'published': '2023-10-12', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'autorepo-a-general-framework-for-multi-modal', 'arxiv_id': '2310.07944', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.07944v2', 'url_pdf': 'https://arxiv.org/pdf/2310.07944v2.pdf', 'title': 'AutoRepo: A general framework for multi-modal LLM-based automated construction reporting', 'abstract': 'Ensuring the safety, quality, and timely completion of construction projects is paramount, with construction inspections serving as a vital instrument towards these goals. Nevertheless, the predominantly manual approach of present-day inspections frequently results in inefficiencies and inadequate information management. Such methods often fall short of providing holistic, exhaustive assessments, consequently engendering regulatory oversights and potential safety hazards. To address this issue, this paper presents a novel framework named AutoRepo for automated generation of construction inspection reports. The unmanned vehicles efficiently perform construction inspections and collect scene information, while the multimodal large language models (LLMs) are leveraged to automatically generate the inspection reports. The framework was applied and tested on a real-world construction site, demonstrating its potential to expedite the inspection process, significantly reduce resource allocation, and produce high-quality, regulatory standard-compliant inspection reports. This research thus underscores the immense potential of multimodal large language models in revolutionizing construction inspection practices, signaling a significant leap forward towards a more efficient and safer construction management paradigm.', 'authors': ['Heng Li', 'Runhao Guo', 'Jing Li', 'Xincong Yang', 'Hongxu Pu'], 'published': '2023-10-11', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'mm-bigbench-evaluating-multimodal-models-on', 'arxiv_id': '2310.09036', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.09036v1', 'url_pdf': 'https://arxiv.org/pdf/2310.09036v1.pdf', 'title': 'MM-BigBench: Evaluating Multimodal Models on Multimodal Content Comprehension Tasks', 'abstract': \"The popularity of multimodal large language models (MLLMs) has triggered a recent surge in research efforts dedicated to evaluating these models. Nevertheless, existing evaluation studies of MLLMs primarily focus on the comprehension and reasoning of unimodal (vision) content, neglecting performance evaluations in the domain of multimodal (vision-language) content understanding. Beyond multimodal reasoning, tasks related to multimodal content comprehension necessitate a profound understanding of multimodal contexts, achieved through the multimodal interaction to obtain a final answer. In this paper, we introduce a comprehensive assessment framework called MM-BigBench, which incorporates a diverse range of metrics to offer an extensive evaluation of the performance of various models and instructions across a wide spectrum of diverse multimodal content comprehension tasks. Consequently, our work complements research on the performance of MLLMs in multimodal comprehension tasks, achieving a more comprehensive and holistic evaluation of MLLMs. To begin, we employ the Best Performance metric to ascertain each model's performance upper bound on different datasets. Subsequently, the Mean Relative Gain metric offers an assessment of the overall performance of various models and instructions, while the Stability metric measures their sensitivity. Furthermore, previous research centers on evaluating models independently or solely assessing instructions, neglecting the adaptability between models and instructions. We propose the Adaptability metric to quantify the adaptability between models and instructions. Our paper evaluates a total of 20 language models (14 MLLMs) on 14 multimodal datasets spanning 6 tasks, with 10 instructions for each task, and derives novel insights. Our code will be released at https://github.com/declare-lab/MM-BigBench.\", 'authors': ['Soujanya Poria', 'XiaoMing Fu', 'Yifei Zhang', 'Qi Sun', 'Yang Li', 'Daling Wang', 'Ming Wang', 'Shi Feng', 'Wenfang Wu', 'Xiaocui Yang'], 'published': '2023-10-13', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'robollm-robotic-vision-tasks-grounded-on', 'arxiv_id': '2310.10221', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.10221v2', 'url_pdf': 'https://arxiv.org/pdf/2310.10221v2.pdf', 'title': 'RoboLLM: Robotic Vision Tasks Grounded on Multimodal Large Language Models', 'abstract': 'Robotic vision applications often necessitate a wide range of visual perception tasks, such as object detection, segmentation, and identification. While there have been substantial advances in these individual tasks, integrating specialized models into a unified vision pipeline presents significant engineering challenges and costs. Recently, Multimodal Large Language Models (MLLMs) have emerged as novel backbones for various downstream tasks. We argue that leveraging the pre-training capabilities of MLLMs enables the creation of a simplified framework, thus mitigating the need for task-specific encoders. Specifically, the large-scale pretrained knowledge in MLLMs allows for easier fine-tuning to downstream robotic vision tasks and yields superior performance. We introduce the RoboLLM framework, equipped with a BEiT-3 backbone, to address all visual perception tasks in the ARMBench challenge-a large-scale robotic manipulation dataset about real-world warehouse scenarios. RoboLLM not only outperforms existing baselines but also substantially reduces the engineering burden associated with model selection and tuning. The source code is publicly available at https://github.com/longkukuhi/armbench.', 'authors': ['Gerardo Aragon Camarasa', 'Richard McCreadie', 'George Killick', 'Zijun Long'], 'published': '2023-10-16', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'benchmarking-sequential-visual-input', 'arxiv_id': '2310.13473', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.13473v1', 'url_pdf': 'https://arxiv.org/pdf/2310.13473v1.pdf', 'title': 'Benchmarking Sequential Visual Input Reasoning and Prediction in Multimodal Large Language Models', 'abstract': \"Multimodal large language models (MLLMs) have shown great potential in perception and interpretation tasks, but their capabilities in predictive reasoning remain under-explored. To address this gap, we introduce a novel benchmark that assesses the predictive reasoning capabilities of MLLMs across diverse scenarios. Our benchmark targets three important domains: abstract pattern reasoning, human activity prediction, and physical interaction prediction. We further develop three evaluation methods powered by large language model to robustly quantify a model's performance in predicting and reasoning the future based on multi-visual context. Empirical experiments confirm the soundness of the proposed benchmark and evaluation methods via rigorous testing and reveal pros and cons of current popular MLLMs in the task of predictive reasoning. Lastly, our proposed benchmark provides a standardized evaluation framework for MLLMs and can facilitate the development of more advanced models that can reason and predict over complex long sequence of multimodal input.\", 'authors': ['Jianwei Yin', 'Tiancheng Zhao', 'Kangjia Zhao', 'Yu Shu', 'Leigang Sha', 'Mingwei Zhu'], 'published': '2023-10-20', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'woodpecker-hallucination-correction-for', 'arxiv_id': '2310.16045', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.16045v2', 'url_pdf': 'https://arxiv.org/pdf/2310.16045v2.pdf', 'title': 'Woodpecker: Hallucination Correction for Multimodal Large Language Models', 'abstract': 'Hallucination is a big shadow hanging over the rapidly evolving Multimodal Large Language Models (MLLMs), referring to the phenomenon that the generated text is inconsistent with the image content. In order to mitigate hallucinations, existing studies mainly resort to an instruction-tuning manner that requires retraining the models with specific data. In this paper, we pave a different way, introducing a training-free method named Woodpecker. Like a woodpecker heals trees, it picks out and corrects hallucinations from the generated text. Concretely, Woodpecker consists of five stages: key concept extraction, question formulation, visual knowledge validation, visual claim generation, and hallucination correction. Implemented in a post-remedy manner, Woodpecker can easily serve different MLLMs, while being interpretable by accessing intermediate outputs of the five stages. We evaluate Woodpecker both quantitatively and qualitatively and show the huge potential of this new paradigm. On the POPE benchmark, our method obtains a 30.66%/24.33% improvement in accuracy over the baseline MiniGPT-4/mPLUG-Owl. The source code is released at https://github.com/BradyFU/Woodpecker.', 'authors': ['Enhong Chen', 'Xing Sun', 'Ke Li', 'Yunhang Shen', 'Dianbo Sui', 'Hao Wang', 'Tong Xu', 'Sirui Zhao', 'Chaoyou Fu', 'Shukang Yin'], 'published': '2023-10-24', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'visual-cropping-improves-zero-shot-question', 'arxiv_id': '2310.16033', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.16033v3', 'url_pdf': 'https://arxiv.org/pdf/2310.16033v3.pdf', 'title': 'Towards Perceiving Small Visual Details in Zero-shot Visual Question Answering with Multimodal LLMs', 'abstract': \"Multimodal Large Language Models (MLLMs) have recently achieved promising zero-shot accuracy on visual question answering (VQA) -- a fundamental task affecting various downstream applications and domains. Given the great potential for the broad use of these models, it is important to investigate their limitations in dealing with different image and question properties. In this work, we investigate whether MLLMs can perceive small details as well as large details in images. In particular, we show that their zero-shot accuracy in answering visual questions is very sensitive to the size of the visual subject of the question, declining up to 46% with size. Furthermore, we show that this effect is causal by observing that human visual cropping can significantly mitigate their sensitivity to size. Inspired by the usefulness of human cropping, we then propose five automatic visual cropping methods -- leveraging either external localization models or the decision process of the given MLLM itself -- as inference time mechanisms to improve the zero-shot performance of MLLMs. We study their effectiveness on four popular VQA datasets, and a subset of the VQAv2 dataset tailored towards fine visual details. Our findings suggest that MLLMs should be used with caution in detail-sensitive VQA applications, and that visual cropping is a promising direction to improve their zero-shot performance. To facilitate further investigation of MLLMs' behaviors, our code and data are publicly released.\", 'authors': ['Filip Ilievski', 'Prateek Chhikara', 'Mahyar Khayatkhoei', 'Jiarui Zhang'], 'published': '2023-10-24', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'chef-a-comprehensive-evaluation-framework-for', 'arxiv_id': '2311.02692', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2311.02692v1', 'url_pdf': 'https://arxiv.org/pdf/2311.02692v1.pdf', 'title': 'ChEF: A Comprehensive Evaluation Framework for Standardized Assessment of Multimodal Large Language Models', 'abstract': \"Multimodal Large Language Models (MLLMs) have shown impressive abilities in interacting with visual content with myriad potential downstream tasks. However, even though a list of benchmarks has been proposed, the capabilities and limitations of MLLMs are still not comprehensively understood, due to a lack of a standardized and holistic evaluation framework. To this end, we present the first Comprehensive Evaluation Framework (ChEF) that can holistically profile each MLLM and fairly compare different MLLMs. First, we structure ChEF as four modular components, i.e., Scenario as scalable multimodal datasets, Instruction as flexible instruction retrieving formulae, Inferencer as reliable question answering strategies, and Metric as indicative task-specific score functions. Based on them, ChEF facilitates versatile evaluations in a standardized framework, and new evaluations can be built by designing new Recipes (systematic selection of these four components). Notably, current MLLM benchmarks can be readily summarized as recipes of ChEF. Second, we introduce 6 new recipes to quantify competent MLLMs' desired capabilities (or called desiderata, i.e., calibration, in-context learning, instruction following, language performance, hallucination, and robustness) as reliable agents that can perform real-world multimodal interactions. Third, we conduct a large-scale evaluation of 9 prominent MLLMs on 9 scenarios and 6 desiderata. Our evaluation summarized over 20 valuable observations concerning the generalizability of MLLMs across various scenarios and the composite capability of MLLMs required for multimodal interactions. We will publicly release all the detailed implementations for further analysis, as well as an easy-to-use modular toolkit for the integration of new recipes and models, so that ChEF can be a growing evaluation framework for the MLLM community.\", 'authors': ['Jing Shao', 'Yu Qiao', 'Lu Sheng', 'Zhenfei Yin', 'Hongxing Fan', 'Zhipin Wang', 'Zhelun Shi'], 'published': '2023-11-05', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'octavius-mitigating-task-interference-in', 'arxiv_id': '2311.02684', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2311.02684v3', 'url_pdf': 'https://arxiv.org/pdf/2311.02684v3.pdf', 'title': 'Octavius: Mitigating Task Interference in MLLMs via LoRA-MoE', 'abstract': 'Recent studies have demonstrated Large Language Models (LLMs) can extend their zero-shot generalization capabilities to multimodal learning through instruction tuning. As more modalities and downstream tasks are introduced, negative conflicts and interference may have a worse impact on performance. While this phenomenon has been overlooked in previous work, we propose a novel and extensible framework, called Octavius, for comprehensive studies and experimentation on multimodal learning with Multimodal Large Language Models (MLLMs). Specifically, we combine the well-known Mixture-of-Experts (MoE) and one of the representative PEFT techniques, i.e., LoRA, designing a novel LLM-based decoder, called LoRA-MoE, for multimodal learning. To the best of our knowledge, we are one of the pioneering efforts to introduce MoE into MLLMs to address this problem. The experimental results (about 20% improvement) have shown the effectiveness and versatility of our design in various 2D and 3D downstream tasks. Code and datasets are available at https://openlamm.github.io/tutorial/.', 'authors': ['Jing Shao', 'Yu Qiao', 'Wanli Ouyang', 'Lu Sheng', 'Si Liu', 'Zhenfei Yin', 'Huayang Liu', 'Zhen Wang', 'Ziqin Wang', 'Zeren Chen'], 'published': '2023-11-05', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'what-large-language-models-bring-to-text-rich', 'arxiv_id': '2311.07306', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2311.07306v1', 'url_pdf': 'https://arxiv.org/pdf/2311.07306v1.pdf', 'title': 'What Large Language Models Bring to Text-rich VQA?', 'abstract': \"Text-rich VQA, namely Visual Question Answering based on text recognition in the images, is a cross-modal task that requires both image comprehension and text recognition. In this work, we focus on investigating the advantages and bottlenecks of LLM-based approaches in addressing this problem. To address the above concern, we separate the vision and language modules, where we leverage external OCR models to recognize texts in the image and Large Language Models (LLMs) to answer the question given texts. The whole framework is training-free benefiting from the in-context ability of LLMs. This pipeline achieved superior performance compared to the majority of existing Multimodal Large Language Models (MLLM) on four text-rich VQA datasets. Besides, based on the ablation study, we find that LLM brings stronger comprehension ability and may introduce helpful knowledge for the VQA problem. The bottleneck for LLM to address text-rich VQA problems may primarily lie in visual part. We also combine the OCR module with MLLMs and pleasantly find that the combination of OCR module with MLLM also works. It's worth noting that not all MLLMs can comprehend the OCR information, which provides insights into how to train an MLLM that preserves the abilities of LLM.\", 'authors': ['Fei Tan', 'Zechao Li', 'Rui Zhao', 'Jinghui Lu', 'Xinzhe Ni', 'Wei Tang', 'Xuejing Liu'], 'published': '2023-11-13', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}], 'messages': [], 'retry_count': 2}\n{'query': \"latest advances in multimodal learning Consider User's response for the search query: I dont want to know about latest advancements, I want to know about multimodal llms\", 'search_query': 'multimodal large language models', 'papers': [{'id': 'multimodal-large-language-models-for', 'arxiv_id': None, 'nips_id': None, 'url_abs': 'https://aclanthology.org/2022.naacl-srw.26', 'url_pdf': 'https://aclanthology.org/2022.naacl-srw.26.pdf', 'title': 'Multimodal large language models for inclusive collaboration learning tasks', 'abstract': 'This PhD project leverages advancements in multimodal large language models to build an inclusive collaboration feedback loop, in order to facilitate the automated detection, modeling, and feedback for participants developing general collaboration skills. This topic is important given the role of collaboration as an essential 21st century skill, the potential to ground large language models within learning theory and real-world practice, and the expressive potential of transformer models to support equity and inclusion. We address some concerns of integrating advances in natural language processing into downstream tasks such as the learning analytics feedback loop.', 'authors': ['Armanda Lewis'], 'published': None, 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': 'naacl-acl-2022-7'}, {'id': 'personality-aware-human-centric-multimodal', 'arxiv_id': '2304.02313', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2304.02313v2', 'url_pdf': 'https://arxiv.org/pdf/2304.02313v2.pdf', 'title': 'Personality-aware Human-centric Multimodal Reasoning: A New Task, Dataset and Baselines', 'abstract': \"Personality traits, emotions, and beliefs shape individuals' behavioral choices and decision-making processes. However, for one thing, the affective computing community normally focused on predicting personality traits but overlooks their application in behavior prediction. For another, the multimodal reasoning task emphasized the prediction of future states and behaviors but often neglected the incorporation of individual personality traits. In this work, we introduce a new task called Personality-aware Human-centric Multimodal Reasoning (PHMR) (T1), with the goal of forecasting the future behavior of a particular individual using multimodal information from past instances, while integrating personality factors. We accordingly construct a new dataset based on six television shows, encompassing 225 characters and 12k samples. To establish a benchmark for the task, we propose seven baseline methods: three adapted from related tasks, two pre-trained model, and two multimodal large language models. The experimental results demonstrate that incorporating personality traits enhances human-centric multimodal reasoning performance. To further solve the lack of personality annotation in real-life scenes, we introduce an extension task called Personality-predicted Human-centric Multimodal Reasoning task (T2) along with the corresponding dataset and method. We will make our dataset and code available on GitHub.\", 'authors': ['Rui Xia', 'Xiangqing Shen', 'Yaochen Zhu'], 'published': '2023-04-05', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'visual-transformation-telling', 'arxiv_id': '2305.01928', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2305.01928v2', 'url_pdf': 'https://arxiv.org/pdf/2305.01928v2.pdf', 'title': 'Visual Transformation Telling', 'abstract': 'Humans can naturally reason from superficial state differences (e.g. ground wetness) to transformations descriptions (e.g. raining) according to their life experience. In this paper, we propose a new visual reasoning task to test this transformation reasoning ability in real-world scenarios, called \\\\textbf{V}isual \\\\textbf{T}ransformation \\\\textbf{T}elling (VTT). Given a series of states (i.e. images), VTT requires to describe the transformation occurring between every two adjacent states. Different from existing visual reasoning tasks that focus on surface state reasoning, the advantage of VTT is that it captures the underlying causes, e.g. actions or events, behind the differences among states. We collect a novel dataset to support the study of transformation reasoning from two existing instructional video datasets, CrossTask and COIN, comprising 13,547 samples. Each sample involves the key state images along with their transformation descriptions. Our dataset covers diverse real-world activities, providing a rich resource for training and evaluation. To construct an initial benchmark for VTT, we test several models, including traditional visual storytelling methods (CST, GLACNet, Densecap) and advanced multimodal large language models (LLaVA v1.5-7B, Qwen-VL-chat, Gemini Pro Vision, GPT-4o, and GPT-4). Experimental results reveal that even state-of-the-art models still face challenges in VTT, highlighting substantial areas for improvement.', 'authors': ['Liang Pang', 'Xin Hong', 'Wanqing Cui', 'Xueqi Cheng', 'Jiafeng Guo', 'Yanyan Lan'], 'published': '2023-05-03', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'listen-think-and-understand', 'arxiv_id': '2305.10790', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2305.10790v3', 'url_pdf': 'https://arxiv.org/pdf/2305.10790v3.pdf', 'title': 'Listen, Think, and Understand', 'abstract': 'The ability of artificial intelligence (AI) systems to perceive and comprehend audio signals is crucial for many applications. Although significant progress has been made in this area since the development of AudioSet, most existing models are designed to map audio inputs to pre-defined, discrete sound label sets. In contrast, humans possess the ability to not only classify sounds into general categories, but also to listen to the finer details of the sounds, explain the reason for the predictions, think about what the sound infers, and understand the scene and what action needs to be taken, if any. Such capabilities beyond perception are not yet present in existing audio models. On the other hand, modern large language models (LLMs) exhibit emerging reasoning ability but they lack audio perception capabilities. Therefore, we ask the question: can we build a model that has both audio perception and a reasoning ability? In this paper, we propose a new audio foundation model, called LTU (Listen, Think, and Understand). To train LTU, we created a new OpenAQA-5M dataset consisting of 1.9 million closed-ended and 3.7 million open-ended, diverse (audio, question, answer) tuples, and have used an autoregressive training framework with a perception-to-understanding curriculum. LTU demonstrates strong performance and generalization ability on conventional audio tasks such as classification and captioning. More importantly, it exhibits emerging audio reasoning and comprehension abilities that are absent in existing audio models. To the best of our knowledge, LTU is one of the first multimodal large language models that focus on general audio (rather than just speech) understanding.', 'authors': ['James Glass', 'Leonid Karlinsky', 'Alexander H. Liu', 'Hongyin Luo', 'Yuan Gong'], 'published': '2023-05-18', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'what-makes-for-good-visual-tokenizers-for', 'arxiv_id': '2305.12223', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2305.12223v2', 'url_pdf': 'https://arxiv.org/pdf/2305.12223v2.pdf', 'title': 'What Makes for Good Visual Tokenizers for Large Language Models?', 'abstract': 'We empirically investigate proper pre-training methods to build good visual tokenizers, making Large Language Models (LLMs) powerful Multimodal Large Language Models (MLLMs). In our benchmark, which is curated to evaluate MLLMs visual semantic understanding and fine-grained perception capabilities, we discussed different visual tokenizers pre-trained with dominant methods (i.e., DeiT, CLIP, MAE, DINO), and observe that: i) Fully/weakly supervised models capture more semantics than self-supervised models, but the gap is narrowed by scaling up the pre-training dataset. ii) Self-supervised models are better at fine-grained perception, where patch-level supervision is particularly effective. iii) Tuning the visual tokenizer leads to the loss of semantics obtained from large-scale pretraining, which is unfavorable with relatively small-scale instruction-tuning dataset. Given the findings, we reviewed methods that attempted to unify semantics and fine-grained visual understanding, e.g., patch-level feature distillation with semantically-rich targets. We obtain an intriguing insight mask-based strategies that were once all the rage may not be applicable for obtaining good visual tokenizers. Based on this critical observation, we obtain a new MLLM equipped with a tailored Good Visual Tokenizer (GVT), which exhibits strong visual comprehension capability at multiple scales. In particular, without introducing extra parameters and task-specific fine-tuning, GVT achieves superior performance on visual question answering, image captioning, and other fine-grained visual understanding tasks such as object counting and multi-class identification.', 'authors': ['Ying Shan', 'Mohan Kankanhalli', 'Xiaohan Ding', 'Yixiao Ge', 'Guangzhi Wang'], 'published': '2023-05-20', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'interpretable-by-design-visual-question', 'arxiv_id': '2305.14882', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2305.14882v2', 'url_pdf': 'https://arxiv.org/pdf/2305.14882v2.pdf', 'title': 'Dynamic Clue Bottlenecks: Towards Interpretable-by-Design Visual Question Answering', 'abstract': 'Recent advances in multimodal large language models (LLMs) have shown extreme effectiveness in visual question answering (VQA). However, the design nature of these end-to-end models prevents them from being interpretable to humans, undermining trust and applicability in critical domains. While post-hoc rationales offer certain insight into understanding model behavior, these explanations are not guaranteed to be faithful to the model. In this paper, we address these shortcomings by introducing an interpretable by design model that factors model decisions into intermediate human-legible explanations, and allows people to easily understand why a model fails or succeeds. We propose the Dynamic Clue Bottleneck Model ( (DCLUB), a method that is designed towards an inherently interpretable VQA system. DCLUB provides an explainable intermediate space before the VQA decision and is faithful from the beginning, while maintaining comparable performance to black-box systems. Given a question, DCLUB first returns a set of visual clues: natural language statements of visually salient evidence from the image, and then generates the output based solely on the visual clues. To supervise and evaluate the generation of VQA explanations within DCLUB, we collect a dataset of 1.7k reasoning-focused questions with visual clues. Evaluations show that our inherently interpretable system can improve 4.64% over a comparable black-box system in reasoning-focused questions while preserving 99.43% of performance on VQA-v2.', 'authors': ['Dan Roth', 'Mark Yatskar', 'Sihao Chen', 'Ben Zhou', 'Xingyu Fu'], 'published': '2023-05-24', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'pathasst-redefining-pathology-through', 'arxiv_id': '2305.15072', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2305.15072v2', 'url_pdf': 'https://arxiv.org/pdf/2305.15072v2.pdf', 'title': 'PathAsst: A Generative Foundation AI Assistant Towards Artificial General Intelligence of Pathology', 'abstract': \"As advances in large language models (LLMs) and multimodal techniques continue to mature, the development of general-purpose multimodal large language models (MLLMs) has surged, offering significant applications in interpreting natural images. However, the field of pathology has largely remained untapped, particularly in gathering high-quality data and designing comprehensive model frameworks. To bridge the gap in pathology MLLMs, we present PathAsst, a multimodal generative foundation AI assistant to revolutionize diagnostic and predictive analytics in pathology. The development of PathAsst involves three pivotal steps: data acquisition, CLIP model adaptation, and the training of PathAsst's multimodal generative capabilities. Firstly, we collect over 207K high-quality pathology image-text pairs from authoritative sources. Leveraging the advanced power of ChatGPT, we generate over 180K instruction-following samples. Furthermore, we devise additional instruction-following data specifically tailored for invoking eight pathology-specific sub-models we prepared, allowing the PathAsst to effectively collaborate with these models, enhancing its diagnostic ability. Secondly, by leveraging the collected data, we construct PathCLIP, a pathology-dedicated CLIP, to enhance PathAsst's capabilities in interpreting pathology images. Finally, we integrate PathCLIP with the Vicuna-13b and utilize pathology-specific instruction-tuning data to enhance the multimodal generation capacity of PathAsst and bolster its synergistic interactions with sub-models. The experimental results of PathAsst show the potential of harnessing AI-powered generative foundation model to improve pathology diagnosis and treatment processes.\", 'authors': ['Yunlong Zhang', 'Zhongyi Shui', 'Lin Sun', 'Lin Yang', 'Honglin Li', 'Kai Zhang', 'Sunyi Zheng', 'Chenglu Zhu', 'Yuxuan Sun'], 'published': '2023-05-24', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'contextual-object-detection-with-multimodal', 'arxiv_id': '2305.18279', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2305.18279v2', 'url_pdf': 'https://arxiv.org/pdf/2305.18279v2.pdf', 'title': 'Contextual Object Detection with Multimodal Large Language Models', 'abstract': 'Recent Multimodal Large Language Models (MLLMs) are remarkable in vision-language tasks, such as image captioning and question answering, but lack the essential perception ability, i.e., object detection. In this work, we address this limitation by introducing a novel research problem of contextual object detection -- understanding visible objects within different human-AI interactive contexts. Three representative scenarios are investigated, including the language cloze test, visual captioning, and question answering. Moreover, we present ContextDET, a unified multimodal model that is capable of end-to-end differentiable modeling of visual-language contexts, so as to locate, identify, and associate visual objects with language inputs for human-AI interaction. Our ContextDET involves three key submodels: (i) a visual encoder for extracting visual representations, (ii) a pre-trained LLM for multimodal context decoding, and (iii) a visual decoder for predicting bounding boxes given contextual object words. The new generate-then-detect framework enables us to detect object words within human vocabulary. Extensive experiments show the advantages of ContextDET on our proposed CODE benchmark, open-vocabulary detection, and referring image segmentation. Github: https://github.com/yuhangzang/ContextDET.', 'authors': ['Chen Change Loy', 'Kaiyang Zhou', 'Jun Han', 'Wei Li', 'Yuhang Zang'], 'published': '2023-05-29', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'a-survey-on-multimodal-large-language-models', 'arxiv_id': '2306.13549', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2306.13549v4', 'url_pdf': 'https://arxiv.org/pdf/2306.13549v4.pdf', 'title': 'A Survey on Multimodal Large Language Models', 'abstract': 'Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.', 'authors': ['Enhong Chen', 'Tong Xu', 'Xing Sun', 'Ke Li', 'Sirui Zhao', 'Chaoyou Fu', 'Shukang Yin'], 'published': '2023-06-23', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'mme-a-comprehensive-evaluation-benchmark-for', 'arxiv_id': '2306.13394', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2306.13394v4', 'url_pdf': 'https://arxiv.org/pdf/2306.13394v4.pdf', 'title': 'MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models', 'abstract': 'Multimodal Large Language Model (MLLM) relies on the powerful LLM to perform multimodal tasks, showing amazing emergent abilities in recent studies, such as writing poems based on an image. However, it is difficult for these case studies to fully reflect the performance of MLLM, lacking a comprehensive evaluation. In this paper, we fill in this blank, presenting the first comprehensive MLLM Evaluation benchmark MME. It measures both perception and cognition abilities on a total of 14 subtasks. In order to avoid data leakage that may arise from direct use of public datasets for evaluation, the annotations of instruction-answer pairs are all manually designed. The concise instruction design allows us to fairly compare MLLMs, instead of struggling in prompt engineering. Besides, with such an instruction, we can also easily carry out quantitative statistics. A total of 30 advanced MLLMs are comprehensively evaluated on our MME, which not only suggests that existing MLLMs still have a large room for improvement, but also reveals the potential directions for the subsequent model optimization. The data application manner and online leaderboards are released at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation.', 'authors': ['Rongrong Ji', 'Yunsheng Wu', 'Jinrui Yang', 'Xing Sun', 'Ke Li', 'Xiawu Zheng', 'Xu Lin', 'Mengdan Zhang', 'Yulei Qin', 'Yunhang Shen', 'Peixian Chen', 'Chaoyou Fu'], 'published': '2023-06-23', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'kosmos-2-grounding-multimodal-large-language', 'arxiv_id': '2306.14824', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2306.14824v3', 'url_pdf': 'https://arxiv.org/pdf/2306.14824v3.pdf', 'title': 'Kosmos-2: Grounding Multimodal Large Language Models to the World', 'abstract': \"We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. Specifically, we represent refer expressions as links in Markdown, i.e., ``[text span](bounding boxes)'', where object descriptions are sequences of location tokens. Together with multimodal corpora, we construct large-scale data of grounded image-text pairs (called GrIT) to train the model. In addition to the existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), Kosmos-2 integrates the grounding capability into downstream applications. We evaluate Kosmos-2 on a wide range of tasks, including (i) multimodal grounding, such as referring expression comprehension, and phrase grounding, (ii) multimodal referring, such as referring expression generation, (iii) perception-language tasks, and (iv) language understanding and generation. This work lays out the foundation for the development of Embodiment AI and sheds light on the big convergence of language, multimodal perception, action, and world modeling, which is a key step toward artificial general intelligence. Code and pretrained models are available at https://aka.ms/kosmos-2.\", 'authors': ['Furu Wei', 'Shuming Ma', 'Shaohan Huang', 'Yaru Hao', 'Li Dong', 'Wenhui Wang', 'Zhiliang Peng'], 'published': '2023-06-26', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'shikra-unleashing-multimodal-llm-s', 'arxiv_id': '2306.15195', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2306.15195v2', 'url_pdf': 'https://arxiv.org/pdf/2306.15195v2.pdf', 'title': \"Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic\", 'abstract': \"In human conversations, individuals can indicate relevant regions within a scene while addressing others. In turn, the other person can then respond by referring to specific regions if necessary. This natural referential ability in dialogue remains absent in current Multimodal Large Language Models (MLLMs). To fill this gap, this paper proposes an MLLM called Shikra, which can handle spatial coordinate inputs and outputs in natural language. Its architecture consists of a vision encoder, an alignment layer, and a LLM. It is designed to be straightforward and simple, without the need for extra vocabularies, position encoder, pre-/post-detection modules, or external plug-in models. All inputs and outputs are in natural language form. Referential dialogue is a superset of various vision-language (VL) tasks. Shikra can naturally handle location-related tasks like REC and PointQA, as well as conventional VL tasks such as Image Captioning and VQA. Experimental results showcase Shikra's promising performance. Furthermore, it enables numerous exciting applications, like providing mentioned objects' coordinates in chains of thoughts and comparing user-pointed regions similarities. Our code, model and dataset are accessed at https://github.com/shikras/shikra.\", 'authors': ['Rui Zhao', 'Feng Zhu', 'Richong Zhang', 'Weili Zeng', 'Zhao Zhang', 'Keqin Chen'], 'published': '2023-06-27', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'svit-scaling-up-visual-instruction-tuning', 'arxiv_id': '2307.04087', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2307.04087v3', 'url_pdf': 'https://arxiv.org/pdf/2307.04087v3.pdf', 'title': 'SVIT: Scaling up Visual Instruction Tuning', 'abstract': \"Thanks to the emerging of foundation models, the large language and vision models are integrated to acquire the multimodal ability of visual captioning, question answering, etc. Although existing multimodal models present impressive performance of visual understanding and reasoning, their limits are still largely under-explored due to the scarcity of high-quality instruction tuning data. To push the limits of multimodal capability, we Scale up Visual Instruction Tuning (SVIT) by constructing a dataset of 4.2 million visual instruction tuning data including 1.6M conversation question-answer (QA) pairs, 1.6M complex reasoning QA pairs, 1.0M referring QA pairs and 106K detailed image descriptions. Besides the volume, the proposed dataset is also featured by the high quality and rich diversity, which is generated by prompting GPT-4 with the abundant manual annotations of images. We also propose a new data recipe to select subset with better diversity and balance, which evokes model's superior capabilities. Extensive experiments verify that SVIT-v1.5, trained on the proposed dataset, outperforms state-of-the-art Multimodal Large Language Models on popular benchmarks. The data and code are publicly available at https://github.com/BAAI-DCAI/Visual-Instruction-Tuning.\", 'authors': ['Tiejun Huang', 'Muyang He', 'Boya Wu', 'Bo Zhao'], 'published': '2023-07-09', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'chatspot-bootstrapping-multimodal-llms-via', 'arxiv_id': '2307.09474', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2307.09474v1', 'url_pdf': 'https://arxiv.org/pdf/2307.09474v1.pdf', 'title': 'ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning', 'abstract': \"Human-AI interactivity is a critical aspect that reflects the usability of multimodal large language models (MLLMs). However, existing end-to-end MLLMs only allow users to interact with them through language instructions, leading to the limitation of the interactive accuracy and efficiency. In this study, we present precise referring instructions that utilize diverse reference representations such as points and boxes as referring prompts to refer to the special region. This enables MLLMs to focus on the region of interest and achieve finer-grained interaction. Based on precise referring instruction, we propose ChatSpot, a unified end-to-end multimodal large language model that supports diverse forms of interactivity including mouse clicks, drag-and-drop, and drawing boxes, which provides a more flexible and seamless interactive experience. We also construct a multi-grained vision-language instruction-following dataset based on existing datasets and GPT-4 generating. Furthermore, we design a series of evaluation tasks to assess the effectiveness of region recognition and interaction. Experimental results showcase ChatSpot's promising performance.\", 'authors': ['Xiangyu Zhang', 'Chunrui Han', 'Runpei Dong', 'Yuang Peng', 'Jianjian Sun', 'HongYu Zhou', 'Haoran Wei', 'Jinrong Yang', 'Zheng Ge', 'En Yu', 'Liang Zhao'], 'published': '2023-07-18', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'seed-bench-benchmarking-multimodal-llms-with', 'arxiv_id': '2307.16125', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2307.16125v2', 'url_pdf': 'https://arxiv.org/pdf/2307.16125v2.pdf', 'title': 'SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension', 'abstract': 'Based on powerful Large Language Models (LLMs), recent generative Multimodal Large Language Models (MLLMs) have gained prominence as a pivotal research area, exhibiting remarkable capability for both comprehension and generation. In this work, we address the evaluation of generative comprehension in MLLMs as a preliminary step towards a comprehensive assessment of generative models, by introducing a benchmark named SEED-Bench. SEED-Bench consists of 19K multiple choice questions with accurate human annotations (x 6 larger than existing benchmarks), which spans 12 evaluation dimensions including the comprehension of both the image and video modality. We develop an advanced pipeline for generating multiple-choice questions that target specific evaluation dimensions, integrating both automatic filtering and manual verification processes. Multiple-choice questions with groundtruth options derived from human annotation enables an objective and efficient assessment of model performance, eliminating the need for human or GPT intervention during evaluation. We further evaluate the performance of 18 models across all 12 dimensions, covering both the spatial and temporal understanding. By revealing the limitations of existing MLLMs through evaluation results, we aim for SEED-Bench to provide insights for motivating future research. We will launch and consistently maintain a leaderboard to provide a platform for the community to assess and investigate model capability.', 'authors': ['Ying Shan', 'Yixiao Ge', 'Yuying Ge', 'Guangzhi Wang', 'Rui Wang', 'Bohao Li'], 'published': '2023-07-30', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'lisa-reasoning-segmentation-via-large', 'arxiv_id': '2308.00692', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.00692v3', 'url_pdf': 'https://arxiv.org/pdf/2308.00692v3.pdf', 'title': 'LISA: Reasoning Segmentation via Large Language Model', 'abstract': 'Although perception systems have made remarkable advancements in recent years, they still rely on explicit human instruction or pre-defined categories to identify the target objects before executing visual recognition tasks. Such systems cannot actively reason and comprehend implicit user intention. In this work, we propose a new segmentation task -- reasoning segmentation. The task is designed to output a segmentation mask given a complex and implicit query text. Furthermore, we establish a benchmark comprising over one thousand image-instruction-mask data samples, incorporating intricate reasoning and world knowledge for evaluation purposes. Finally, we present LISA: large Language Instructed Segmentation Assistant, which inherits the language generation capabilities of multimodal Large Language Models (LLMs) while also possessing the ability to produce segmentation masks. We expand the original vocabulary with a <SEG> token and propose the embedding-as-mask paradigm to unlock the segmentation capability. Remarkably, LISA can handle cases involving complex reasoning and world knowledge. Also, it demonstrates robust zero-shot capability when trained exclusively on reasoning-free datasets. In addition, fine-tuning the model with merely 239 reasoning segmentation data samples results in further performance enhancement. Both quantitative and qualitative experiments show our method effectively unlocks new reasoning segmentation capabilities for multimodal LLMs. Code, models, and data are available at https://github.com/dvlab-research/LISA.', 'authors': ['Jiaya Jia', 'Shu Liu', 'Yuhui Yuan', 'Yanwei Li', 'Yukang Chen', 'Zhuotao Tian', 'Xin Lai'], 'published': '2023-08-01', 'conference': None, 'conference_url_abs': 'http://openaccess.thecvf.com//content/CVPR2024/html/Lai_LISA_Reasoning_Segmentation_via_Large_Language_Model_CVPR_2024_paper.html', 'conference_url_pdf': 'http://openaccess.thecvf.com//content/CVPR2024/papers/Lai_LISA_Reasoning_Segmentation_via_Large_Language_Model_CVPR_2024_paper.pdf', 'proceeding': 'cvpr-2024-1'}, {'id': 'empowering-vision-language-models-to-follow', 'arxiv_id': '2308.04152', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.04152v4', 'url_pdf': 'https://arxiv.org/pdf/2308.04152v4.pdf', 'title': 'Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions', 'abstract': \"Recent advancements in Multimodal Large Language Models (MLLMs) have been utilizing Visual Prompt Generators (VPGs) to convert visual features into tokens that LLMs can recognize. This is achieved by training the VPGs on millions of image-caption pairs, where the VPG-generated tokens of images are fed into a frozen LLM to generate the corresponding captions. However, this image-captioning based training objective inherently biases the VPG to concentrate solely on the primary visual contents sufficient for caption generation, often neglecting other visual details. This shortcoming results in MLLMs' underperformance in comprehending demonstrative instructions consisting of multiple, interleaved, and multimodal instructions that demonstrate the required context to complete a task. To address this issue, we introduce a generic and lightweight Visual Prompt Generator Complete module (VPG-C), which can infer and complete the missing details essential for comprehending demonstrative instructions. Further, we propose a synthetic discriminative training strategy to fine-tune VPG-C, eliminating the need for supervised demonstrative instructions. As for evaluation, we build DEMON, a comprehensive benchmark for demonstrative instruction understanding. Synthetically trained with the proposed strategy, VPG-C achieves significantly stronger zero-shot performance across all tasks of DEMON. Further evaluation on the MME and OwlEval benchmarks also demonstrate the superiority of VPG-C. Our benchmark, code, and pre-trained models are available at https://github.com/DCDmllm/Cheetah.\", 'authors': ['Wenqiao Zhang', 'Wei Ji', 'Yueting Zhuang', 'Siliang Tang', 'Tat-Seng Chua', 'Hanwang Zhang', 'Minghe Gao', 'Zhiqi Ge', 'Kaihang Pan', 'Juncheng Li'], 'published': '2023-08-08', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'link-context-learning-for-multimodal-llms', 'arxiv_id': '2308.07891', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.07891v1', 'url_pdf': 'https://arxiv.org/pdf/2308.07891v1.pdf', 'title': 'Link-Context Learning for Multimodal LLMs', 'abstract': 'The ability to learn from context with novel concepts, and deliver appropriate responses are essential in human conversations. Despite current Multimodal Large Language Models (MLLMs) and Large Language Models (LLMs) being trained on mega-scale datasets, recognizing unseen images or understanding novel concepts in a training-free manner remains a challenge. In-Context Learning (ICL) explores training-free few-shot learning, where models are encouraged to ``learn to learn\" from limited tasks and generalize to unseen tasks. In this work, we propose link-context learning (LCL), which emphasizes \"reasoning from cause and effect\" to augment the learning capabilities of MLLMs. LCL goes beyond traditional ICL by explicitly strengthening the causal relationship between the support set and the query set. By providing demonstrations with causal links, LCL guides the model to discern not only the analogy but also the underlying causal associations between data points, which empowers MLLMs to recognize unseen images and understand novel concepts more effectively. To facilitate the evaluation of this novel approach, we introduce the ISEKAI dataset, comprising exclusively of unseen generated image-label pairs designed for link-context learning. Extensive experiments show that our LCL-MLLM exhibits strong link-context learning capabilities to novel concepts over vanilla MLLMs. Code and data will be released at https://github.com/isekai-portal/Link-Context-Learning.', 'authors': ['Ziwei Liu', 'Rui Zhao', 'Feng Zhu', 'Zhao Zhang', 'Weichen Fan', 'Yan Tai'], 'published': '2023-08-15', 'conference': None, 'conference_url_abs': 'http://openaccess.thecvf.com//content/CVPR2024/html/Tai_Link-Context_Learning_for_Multimodal_LLMs_CVPR_2024_paper.html', 'conference_url_pdf': 'http://openaccess.thecvf.com//content/CVPR2024/papers/Tai_Link-Context_Learning_for_Multimodal_LLMs_CVPR_2024_paper.pdf', 'proceeding': 'cvpr-2024-1'}, {'id': 'fashionlogo-prompting-multimodal-large', 'arxiv_id': '2308.09012', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.09012v2', 'url_pdf': 'https://arxiv.org/pdf/2308.09012v2.pdf', 'title': 'FashionLOGO: Prompting Multimodal Large Language Models for Fashion Logo Embeddings', 'abstract': \"Logo embedding models convert the product logos in images into vectors, enabling their utilization for logo recognition and detection within e-commerce platforms. This facilitates the enforcement of intellectual property rights and enhances product search capabilities. However, current methods treat logo embedding as a purely visual problem. A noteworthy issue is that visual models capture features more than logos. Instead, we view this as a multimodal task, using text as auxiliary information to facilitate the visual model's understanding of the logo. The emerging Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in both visual and textual understanding. Inspired by this, we propose an approach, \\\\textbf{FashionLOGO}, to explore how to prompt MLLMs to generate appropriate text for product images, which can help visual models achieve better logo embeddings. We adopt a cross-attention transformer block that enables visual embedding to automatically learn supplementary knowledge from textual embedding. Our extensive experiments on real-world datasets prove that FashionLOGO is capable of generating generic and robust logo embeddings, achieving state-of-the-art performance in all benchmarks.\", 'authors': ['Walton Wang', 'Minghui Qiu', 'Min Yang', 'Yulin Su', 'Da Li', 'Zhen Wang'], 'published': '2023-08-17', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'wanjuan-a-comprehensive-multimodal-dataset', 'arxiv_id': '2308.10755', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.10755v3', 'url_pdf': 'https://arxiv.org/pdf/2308.10755v3.pdf', 'title': 'WanJuan: A Comprehensive Multimodal Dataset for Advancing English and Chinese Large Models', 'abstract': 'The rise in popularity of ChatGPT and GPT-4 has significantly accelerated the development of large models, leading to the creation of numerous impressive large language models(LLMs) and multimodal large language models (MLLMs). These cutting-edge models owe their remarkable performance to high-quality data. However, the details of the training data used in leading paradigms are often kept confidential. This lack of transparency, coupled with the scarcity of open-source data, impedes further developments within the community. As a response, this paper presents \"Wan Juan\", a large-scale multimodal dataset composed of both Chinese and English data, collected from a wide range of web sources. The dataset incorporates text, image-text, and video modalities, with a total volume exceeding 2TB. It was utilized in the training of InternLM, a model that demonstrated significant advantages in multi-dimensional evaluations when compared to models of a similar scale. All data can be accessed at https://opendatalab.org.cn/WanJuan1.0.', 'authors': ['Dahua Lin', 'Jiaqi Wang', 'Hang Yan', 'Wei Li', 'Bin Wang', 'Jiantao Qiu', 'Chao Xu', 'Zhenjiang Jin', 'Conghui He'], 'published': '2023-08-21', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'stablellava-enhanced-visual-instruction', 'arxiv_id': '2308.10253', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.10253v2', 'url_pdf': 'https://arxiv.org/pdf/2308.10253v2.pdf', 'title': 'StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data', 'abstract': \"The remarkable multimodal capabilities demonstrated by OpenAI's GPT-4 have sparked significant interest in the development of multimodal Large Language Models (LLMs). A primary research objective of such models is to align visual and textual modalities effectively while comprehending human instructions. Current methodologies often rely on annotations derived from benchmark datasets to construct image-dialogue datasets for training purposes, akin to instruction tuning in LLMs. However, these datasets often exhibit domain bias, potentially constraining the generative capabilities of the models. In an effort to mitigate these limitations, we propose a novel data collection methodology that synchronously synthesizes images and dialogues for visual instruction tuning. This approach harnesses the power of generative models, marrying the abilities of ChatGPT and text-to-image generative models to yield a diverse and controllable dataset with varied image content. Additionally, datasets can be arbitrarily scaled. This not only provides greater flexibility compared to existing methodologies but also significantly enhances several model capabilities. Our research includes comprehensive experiments conducted on various datasets. The results emphasize substantial enhancements in more than ten commonly assessed capabilities. Additionally, our model achieves state-of-the-art results across multiple widely recognized multimodal benchmarks.\", 'authors': ['Yunchao Wei', 'Ling Chen', 'Chunhua Shen', 'Guosheng Lin', 'Bin Fu', 'Zhibin Wang', 'Gang Yu', 'Chi Zhang', 'Yanda Li'], 'published': '2023-08-20', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'instructiongpt-4-a-200-instruction-paradigm', 'arxiv_id': '2308.12067', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.12067v2', 'url_pdf': 'https://arxiv.org/pdf/2308.12067v2.pdf', 'title': 'InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4', 'abstract': 'Multimodal large language models are typically trained in two stages: first pre-training on image-text pairs, and then fine-tuning using supervised vision-language instruction data. Recent studies have shown that large language models can achieve satisfactory results even with a limited amount of high-quality instruction-following data. In this paper, we introduce InstructionGPT-4, which is fine-tuned on a small dataset comprising only 200 examples, amounting to approximately 6\\\\% of the instruction-following data used in the alignment dataset for MiniGPT-4. To achieve this, we first propose several metrics to access the quality of multimodal instruction data. Based on these metrics, we present an effective and trainable data selector to automatically identify and filter low-quality vision-language data. By employing this method, InstructionGPT-4 outperforms the original MiniGPT-4 on various evaluations. Overall, our findings demonstrate that less but high-quality instruction tuning data is efficient in enabling multimodal large language models to generate better output. Our code is available at https://github.com/waltonfuture/InstructionGPT-4.', 'authors': ['Lichao Sun', 'Weiran Huang', 'Zihao Jiang', 'Lai Wei'], 'published': '2023-08-23', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'vigc-visual-instruction-generation-and', 'arxiv_id': '2308.12714', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.12714v3', 'url_pdf': 'https://arxiv.org/pdf/2308.12714v3.pdf', 'title': 'VIGC: Visual Instruction Generation and Correction', 'abstract': \"The integration of visual encoders and large language models (LLMs) has driven recent progress in multimodal large language models (MLLMs). However, the scarcity of high-quality instruction-tuning data for vision-language tasks remains a challenge. The current leading paradigm, such as LLaVA, relies on language-only GPT-4 to generate data, which requires pre-annotated image captions and detection bounding boxes, suffering from understanding image details. A practical solution to this problem would be to utilize the available multimodal large language models (MLLMs) to generate instruction data for vision-language tasks. However, it's worth noting that the currently accessible MLLMs are not as powerful as their LLM counterparts, as they tend to produce inadequate responses and generate false information. As a solution for addressing the current issue, this paper proposes the Visual Instruction Generation and Correction (VIGC) framework that enables multimodal large language models to generate instruction-tuning data and progressively enhance its quality on-the-fly. Specifically, Visual Instruction Generation (VIG) guides the vision-language model to generate diverse instruction-tuning data. To ensure generation quality, Visual Instruction Correction (VIC) adopts an iterative update mechanism to correct any inaccuracies in data produced by VIG, effectively reducing the risk of hallucination. Leveraging the diverse, high-quality data generated by VIGC, we finetune mainstream models and validate data quality based on various evaluations. Experimental results demonstrate that VIGC not only compensates for the shortcomings of language-only data generation methods, but also effectively enhances the benchmark performance. The models, datasets, and code are available at https://opendatalab.github.io/VIGC.\", 'authors': ['Conghui He', 'Jiaqi Wang', 'Wei Li', 'Weijia Li', 'Xiaoyi Dong', 'Pan Zhang', 'Huaping Zhong', 'Jiahui Peng', 'Xiao Han', 'Fan Wu', 'Bin Wang'], 'published': '2023-08-24', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'position-enhanced-visual-instruction-tuning', 'arxiv_id': '2308.13437', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.13437v2', 'url_pdf': 'https://arxiv.org/pdf/2308.13437v2.pdf', 'title': 'Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models', 'abstract': 'Recently, Multimodal Large Language Models (MLLMs) that enable Large Language Models (LLMs) to interpret images through visual instruction tuning have achieved significant success. However, existing visual instruction tuning methods only utilize image-language instruction data to align the language and image modalities, lacking a more fine-grained cross-modal alignment. In this paper, we propose Position-enhanced Visual Instruction Tuning (PVIT), which extends the functionality of MLLMs by integrating an additional region-level vision encoder. This integration promotes a more detailed comprehension of images for the MLLM. In addition, to efficiently achieve a fine-grained alignment between the vision modules and the LLM, we design multiple data generation strategies to construct an image-region-language instruction dataset. Finally, we present both quantitative experiments and qualitative analysis that demonstrate the superiority of the proposed model. Code and data will be released at https://github.com/PVIT-official/PVIT.', 'authors': ['Yang Liu', 'Maosong Sun', 'Peng Li', 'Xiaoyue Mi', 'Fuwen Luo', 'Ruoyu Qin', 'Chi Chen'], 'published': '2023-08-25', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'mllm-dataengine-an-iterative-refinement', 'arxiv_id': '2308.13566', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.13566v2', 'url_pdf': 'https://arxiv.org/pdf/2308.13566v2.pdf', 'title': 'MLLM-DataEngine: An Iterative Refinement Approach for MLLM', 'abstract': 'Despite the great advance of Multimodal Large Language Models (MLLMs) in both instruction dataset building and benchmarking, the independence of training and evaluation makes current MLLMs hard to further improve their capability under the guidance of evaluation results with a relatively low human cost. In this paper, we propose MLLM-DataEngine, a novel closed-loop system that bridges data generation, model training, and evaluation. Within each loop iteration, the MLLM-DataEngine first analyze the weakness of the model based on the evaluation results, then generate a proper incremental dataset for the next training iteration and enhance the model capability iteratively. Compared with previous data collection methods which are separate from the benchmarking, the data generated by MLLM-DataEngine shows better targeting, quality, and correctness. For targeting, we propose an Adaptive Bad-case Sampling module, which adjusts the ratio of different types of data within each incremental dataset based on the benchmarking results. For quality, we resort to GPT-4 to generate high-quality data with each given data type. For correctness, prompt design is critical for the data generation results. Rather than previous hand-crafted prompt, we propose an Interactive Prompt Optimization strategy, which optimizes the prompt with the multi-round interaction between human and GPT, and improve the correctness of generated data greatly. Through extensive experiments, we find our MLLM-DataEngine could boost the MLLM capability in a targeted and automatic manner, with only a few human participation. We hope it could be a general solution for the following MLLMs building. The MLLM-DataEngine has been open-sourced and is now available at https://github.com/opendatalab/MLLM-DataEngine.', 'authors': ['Conghui He', 'Jiaqi Wang', 'Xiaoyi Dong', 'Pan Zhang', 'Siyuan Huang', 'Bin Wang', 'Linke Ouyang', 'Zhiyuan Zhao'], 'published': '2023-08-25', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'socratis-are-large-multimodal-models', 'arxiv_id': '2308.16741', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.16741v3', 'url_pdf': 'https://arxiv.org/pdf/2308.16741v3.pdf', 'title': 'Socratis: Are large multimodal models emotionally aware?', 'abstract': 'Existing emotion prediction benchmarks contain coarse emotion labels which do not consider the diversity of emotions that an image and text can elicit in humans due to various reasons. Learning diverse reactions to multimodal content is important as intelligent machines take a central role in generating and delivering content to society. To address this gap, we propose Socratis, a societal reactions benchmark, where each image-caption (IC) pair is annotated with multiple emotions and the reasons for feeling them. Socratis contains 18K free-form reactions for 980 emotions on 2075 image-caption pairs from 5 widely-read news and image-caption (IC) datasets. We benchmark the capability of state-of-the-art multimodal large language models to generate the reasons for feeling an emotion given an IC pair. Based on a preliminary human study, we observe that humans prefer human-written reasons over 2 times more often than machine-generated ones. This shows our task is harder than standard generation tasks because it starkly contrasts recent findings where humans cannot tell apart machine vs human-written news articles, for instance. We further see that current captioning metrics based on large vision-language models also fail to correlate with human preferences. We hope that these findings and our benchmark will inspire further research on training emotionally aware models.', 'authors': ['Kate Saenko', 'Bryan A. Plummer', 'Saadia Gabriel', 'Reuben Tan', 'Arijit Ray', 'Katherine Deng'], 'published': '2023-08-31', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'multiway-adapater-adapting-large-scale-multi', 'arxiv_id': '2309.01516', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.01516v3', 'url_pdf': 'https://arxiv.org/pdf/2309.01516v3.pdf', 'title': 'MultiWay-Adapater: Adapting large-scale multi-modal models for scalable image-text retrieval', 'abstract': \"As Multimodal Large Language Models (MLLMs) grow in size, adapting them to specialized tasks becomes increasingly challenging due to high computational and memory demands. Indeed, traditional fine-tuning methods are costly, due to the need for extensive, task-specific training. While efficient adaptation methods exist that aim to reduce these costs, in practice they suffer from shallow inter-modal alignment, which severely hurts model effectiveness. To tackle these computational challenges and improve inter-modal alignment, we introduce the MultiWay-Adapter (MWA), a novel framework featuring an 'Alignment Enhancer'. This enhancer deepens inter-modal alignment, enabling high transferability with minimal tuning effort. Our experiments show that unlike prior efficient tuning approaches, MWA maintains model effectiveness, while reducing training time by up-to 57%. MWA is also lightweight, increasing model size by only 2-3% (in terms of parameters) for state-of-the-art foundation models like BEiT-3 Large. These results demonstrate that MWA provides an efficient and effective adaptation method for MLLMs, significantly broadening their applicability.\", 'authors': ['Gerardo Aragon Camarasa', 'Richard McCreadie', 'George Killick', 'Zijun Long'], 'published': '2023-09-04', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'next-gpt-any-to-any-multimodal-llm', 'arxiv_id': '2309.05519', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.05519v3', 'url_pdf': 'https://arxiv.org/pdf/2309.05519v3.pdf', 'title': 'NExT-GPT: Any-to-Any Multimodal LLM', 'abstract': 'While recently Multimodal Large Language Models (MM-LLMs) have made exciting strides, they mostly fall prey to the limitation of only input-side multimodal understanding, without the ability to produce content in multiple modalities. As we humans always perceive the world and communicate with people through various modalities, developing any-to-any MM-LLMs capable of accepting and delivering content in any modality becomes essential to human-level AI. To fill the gap, we present an end-to-end general-purpose any-to-any MM-LLM system, NExT-GPT. We connect an LLM with multimodal adaptors and different diffusion decoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio. By leveraging the existing well-trained highly-performing encoders and decoders, NExT-GPT is tuned with only a small amount of parameter (1%) of certain projection layers, which not only benefits low-cost training and also facilitates convenient expansion to more potential modalities. Moreover, we introduce a modality-switching instruction tuning (MosIT) and manually curate a high-quality dataset for MosIT, based on which NExT-GPT is empowered with complex cross-modal semantic understanding and content generation. Overall, our research showcases the promising possibility of building an AI agent capable of modeling universal modalities, paving the way for more human-like AI research in the community. Project page: https://next-gpt.github.io/', 'authors': ['Tat-Seng Chua', 'Wei Ji', 'Leigang Qu', 'Hao Fei', 'Shengqiong Wu'], 'published': '2023-09-11', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'hilm-d-towards-high-resolution-understanding', 'arxiv_id': '2309.05186', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.05186v1', 'url_pdf': 'https://arxiv.org/pdf/2309.05186v1.pdf', 'title': 'HiLM-D: Towards High-Resolution Understanding in Multimodal Large Language Models for Autonomous Driving', 'abstract': \"Autonomous driving systems generally employ separate models for different tasks resulting in intricate designs. For the first time, we leverage singular multimodal large language models (MLLMs) to consolidate multiple autonomous driving tasks from videos, i.e., the Risk Object Localization and Intention and Suggestion Prediction (ROLISP) task. ROLISP uses natural language to simultaneously identify and interpret risk objects, understand ego-vehicle intentions, and provide motion suggestions, eliminating the necessity for task-specific architectures. However, lacking high-resolution (HR) information, existing MLLMs often miss small objects (e.g., traffic cones) and overly focus on salient ones (e.g., large trucks) when applied to ROLISP. We propose HiLM-D (Towards High-Resolution Understanding in MLLMs for Autonomous Driving), an efficient method to incorporate HR information into MLLMs for the ROLISP task. Especially, HiLM-D integrates two branches: (i) the low-resolution reasoning branch, can be any MLLMs, processes low-resolution videos to caption risk objects and discern ego-vehicle intentions/suggestions; (ii) the high-resolution perception branch (HR-PB), prominent to HiLM-D,, ingests HR images to enhance detection by capturing vision-specific HR feature maps and prioritizing all potential risks over merely salient objects. Our HR-PB serves as a plug-and-play module, seamlessly fitting into current MLLMs. Experiments on the ROLISP benchmark reveal HiLM-D's notable advantage over leading MLLMs, with improvements of 4.8% in BLEU-4 for captioning and 17.2% in mIoU for detection.\", 'authors': ['Xiaomeng Li', 'Wei zhang', 'Hang Xu', 'Jianhua Han', 'Xinpeng Ding'], 'published': '2023-09-11', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'investigating-the-catastrophic-forgetting-in', 'arxiv_id': '2309.10313', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.10313v4', 'url_pdf': 'https://arxiv.org/pdf/2309.10313v4.pdf', 'title': 'Investigating the Catastrophic Forgetting in Multimodal Large Language Models', 'abstract': 'Following the success of GPT4, there has been a surge in interest in multimodal large language model (MLLM) research. This line of research focuses on developing general-purpose LLMs through fine-tuning pre-trained LLMs and vision models. However, catastrophic forgetting, a notorious phenomenon where the fine-tuned model fails to retain similar performance compared to the pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM). In this paper, we introduce EMT: Evaluating MulTimodality for evaluating the catastrophic forgetting in MLLMs, by treating each MLLM as an image classifier. We first apply EMT to evaluate several open-source fine-tuned MLLMs and we discover that almost all evaluated MLLMs fail to retain the same performance levels as their vision encoders on standard image classification tasks. Moreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess performance throughout the fine-tuning. Interestingly, our results suggest that early-stage fine-tuning on an image dataset improves performance across other image datasets, by enhancing the alignment of text and visual features. However, as fine-tuning proceeds, the MLLMs begin to hallucinate, resulting in a significant loss of generalizability, even when the image encoder remains frozen. Our results suggest that MLLMs have yet to demonstrate performance on par with their vision models on standard image classification tasks and the current MLLM fine-tuning procedure still has room for improvement.', 'authors': ['Yi Ma', 'Yong Jae Lee', 'Qing Qu', 'Mu Cai', 'Xiao Li', 'Shengbang Tong', 'Yuexiang Zhai'], 'published': '2023-09-19', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'dreamllm-synergistic-multimodal-comprehension', 'arxiv_id': '2309.11499', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.11499v2', 'url_pdf': 'https://arxiv.org/pdf/2309.11499v2.pdf', 'title': 'DreamLLM: Synergistic Multimodal Comprehension and Creation', 'abstract': \"This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy. Project page: https://dreamllm.github.io.\", 'authors': ['Li Yi', 'Kaisheng Ma', 'Xiangyu Zhang', 'Xiangwen Kong', 'Haoran Wei', 'HongYu Zhou', 'Jianjian Sun', 'Liang Zhao', 'Jinrong Yang', 'Zheng Ge', 'Zekun Qi', 'Yuang Peng', 'Chunrui Han', 'Runpei Dong'], 'published': '2023-09-20', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'how-robust-is-google-s-bard-to-adversarial', 'arxiv_id': '2309.11751', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.11751v2', 'url_pdf': 'https://arxiv.org/pdf/2309.11751v2.pdf', 'title': \"How Robust is Google's Bard to Adversarial Image Attacks?\", 'abstract': \"Multimodal Large Language Models (MLLMs) that integrate text and other modalities (especially vision) have achieved unprecedented performance in various multimodal tasks. However, due to the unsolved adversarial robustness problem of vision models, MLLMs can have more severe safety and security risks by introducing the vision inputs. In this work, we study the adversarial robustness of Google's Bard, a competitive chatbot to ChatGPT that released its multimodal capability recently, to better understand the vulnerabilities of commercial MLLMs. By attacking white-box surrogate vision encoders or MLLMs, the generated adversarial examples can mislead Bard to output wrong image descriptions with a 22% success rate based solely on the transferability. We show that the adversarial examples can also attack other MLLMs, e.g., a 26% attack success rate against Bing Chat and a 86% attack success rate against ERNIE bot. Moreover, we identify two defense mechanisms of Bard, including face detection and toxicity detection of images. We design corresponding attacks to evade these defenses, demonstrating that the current defenses of Bard are also vulnerable. We hope this work can deepen our understanding on the robustness of MLLMs and facilitate future research on defenses. Our code is available at https://github.com/thu-ml/Attack-Bard. Update: GPT-4V is available at October 2023. We further evaluate its robustness under the same set of adversarial examples, achieving a 45% attack success rate.\", 'authors': ['Jun Zhu', 'Hang Su', 'Yu Tian', 'Yichi Zhang', 'Xiao Yang', 'Zhengwei Fang', 'Jiawei Chen', 'Huanran Chen', 'Yinpeng Dong'], 'published': '2023-09-21', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'vdc-versatile-data-cleanser-for-detecting', 'arxiv_id': '2309.16211', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.16211v2', 'url_pdf': 'https://arxiv.org/pdf/2309.16211v2.pdf', 'title': 'VDC: Versatile Data Cleanser based on Visual-Linguistic Inconsistency by Multimodal Large Language Models', 'abstract': 'The role of data in building AI systems has recently been emphasized by the emerging concept of data-centric AI. Unfortunately, in the real-world, datasets may contain dirty samples, such as poisoned samples from backdoor attack, noisy labels in crowdsourcing, and even hybrids of them. The presence of such dirty samples makes the DNNs vunerable and unreliable.Hence, it is critical to detect dirty samples to improve the quality and realiability of dataset. Existing detectors only focus on detecting poisoned samples or noisy labels, that are often prone to weak generalization when dealing with dirty samples from other domains.In this paper, we find a commonality of various dirty samples is visual-linguistic inconsistency between images and associated labels. To capture the semantic inconsistency between modalities, we propose versatile data cleanser (VDC) leveraging the surpassing capabilities of multimodal large language models (MLLM) in cross-modal alignment and reasoning.It consists of three consecutive modules: the visual question generation module to generate insightful questions about the image; the visual question answering module to acquire the semantics of the visual content by answering the questions with MLLM; followed by the visual answer evaluation module to evaluate the inconsistency.Extensive experiments demonstrate its superior performance and generalization to various categories and types of dirty samples. The code is available at \\\\url{https://github.com/zihao-ai/vdc}.', 'authors': ['Baoyuan Wu', 'Bingzhe Wu', 'Shaokui Wei', 'Mingda Zhang', 'Zihao Zhu'], 'published': '2023-09-28', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'guiding-instruction-based-image-editing-via', 'arxiv_id': '2309.17102', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.17102v2', 'url_pdf': 'https://arxiv.org/pdf/2309.17102v2.pdf', 'title': 'Guiding Instruction-based Image Editing via Multimodal Large Language Models', 'abstract': 'Instruction-based image editing improves the controllability and flexibility of image manipulation via natural commands without elaborate descriptions or regional masks. However, human instructions are sometimes too brief for current methods to capture and follow. Multimodal large language models (MLLMs) show promising capabilities in cross-modal understanding and visual-aware response generation via LMs. We investigate how MLLMs facilitate edit instructions and present MLLM-Guided Image Editing (MGIE). MGIE learns to derive expressive instructions and provides explicit guidance. The editing model jointly captures this visual imagination and performs manipulation through end-to-end training. We evaluate various aspects of Photoshop-style modification, global photo optimization, and local editing. Extensive experimental results demonstrate that expressive instructions are crucial to instruction-based image editing, and our MGIE can lead to a notable improvement in automatic metrics and human evaluation while maintaining competitive inference efficiency.', 'authors': ['Zhe Gan', 'Yinfei Yang', 'William Yang Wang', 'Xianzhi Du', 'Wenze Hu', 'Tsu-Jui Fu'], 'published': '2023-09-29', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'reformulating-vision-language-foundation', 'arxiv_id': '2310.00653', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.00653v1', 'url_pdf': 'https://arxiv.org/pdf/2310.00653v1.pdf', 'title': 'Reformulating Vision-Language Foundation Models and Datasets Towards Universal Multimodal Assistants', 'abstract': \"Recent Multimodal Large Language Models (MLLMs) exhibit impressive abilities to perceive images and follow open-ended instructions. The capabilities of MLLMs depend on two crucial factors: the model architecture to facilitate the feature alignment of visual modules and large language models; the multimodal instruction tuning datasets for human instruction following. (i) For the model architecture, most existing models introduce an external bridge module to connect vision encoders with language models, which needs an additional feature-alignment pre-training. In this work, we discover that compact pre-trained vision language models can inherently serve as ``out-of-the-box'' bridges between vision and language. Based on this, we propose Muffin framework, which directly employs pre-trained vision-language models to act as providers of visual signals. (ii) For the multimodal instruction tuning datasets, existing methods omit the complementary relationship between different datasets and simply mix datasets from different tasks. Instead, we propose UniMM-Chat dataset which explores the complementarities of datasets to generate 1.1M high-quality and diverse multimodal instructions. We merge information describing the same image from diverse datasets and transforms it into more knowledge-intensive conversation data. Experimental results demonstrate the effectiveness of the Muffin framework and UniMM-Chat dataset. Muffin achieves state-of-the-art performance on a wide range of vision-language tasks, significantly surpassing state-of-the-art models like LLaVA and InstructBLIP. Our model and dataset are all accessible at https://github.com/thunlp/muffin.\", 'authors': ['Maosong Sun', 'Hai-Tao Zheng', 'Zhiyuan Liu', 'Dahai Li', 'Jiao Xue', 'Yinxv Pan', 'Shan Wang', 'Chongyi Wang', 'Yue Zhao', 'Haoye Zhang', 'Yuan YAO', 'Jinyi Hu', 'Tianyu Yu'], 'published': '2023-10-01', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'drivegpt4-interpretable-end-to-end-autonomous', 'arxiv_id': '2310.01412', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.01412v5', 'url_pdf': 'https://arxiv.org/pdf/2310.01412v5.pdf', 'title': 'DriveGPT4: Interpretable End-to-end Autonomous Driving via Large Language Model', 'abstract': 'Multimodal large language models (MLLMs) have emerged as a prominent area of interest within the research community, given their proficiency in handling and reasoning with non-textual data, including images and videos. This study seeks to extend the application of MLLMs to the realm of autonomous driving by introducing DriveGPT4, a novel interpretable end-to-end autonomous driving system based on LLMs. Capable of processing multi-frame video inputs and textual queries, DriveGPT4 facilitates the interpretation of vehicle actions, offers pertinent reasoning, and effectively addresses a diverse range of questions posed by users. Furthermore, DriveGPT4 predicts low-level vehicle control signals in an end-to-end fashion.These advanced capabilities are achieved through the utilization of a bespoke visual instruction tuning dataset, specifically tailored for autonomous driving applications, in conjunction with a mix-finetuning training strategy. DriveGPT4 represents the pioneering effort to leverage LLMs for the development of an interpretable end-to-end autonomous driving solution. Evaluations conducted on the BDD-X dataset showcase the superior qualitative and quantitative performance of DriveGPT4. Additionally, the fine-tuning of domain-specific data enables DriveGPT4 to yield close or even improved results in terms of autonomous driving grounding when contrasted with GPT4-V.', 'authors': ['Kwan-Yee. K. Wong', 'Hengshuang Zhao', 'Zhenguo Li', 'Yong Guo', 'Zhen Zhao', 'Enze Xie', 'Yujia Zhang', 'Zhenhua Xu'], 'published': '2023-10-02', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'minigpt-5-interleaved-vision-and-language', 'arxiv_id': '2310.02239', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.02239v3', 'url_pdf': 'https://arxiv.org/pdf/2310.02239v3.pdf', 'title': 'MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens', 'abstract': 'The effectiveness of Multimodal Large Language Models (MLLMs) demonstrates a profound capability in multimodal understanding. However, the simultaneous generation of images with coherent texts is still underdeveloped. Addressing this, we introduce a novel interleaved vision-and-language generation method, centered around the concept of ``generative vokens\". These vokens serve as pivotal elements contributing to coherent image-text outputs. Our method is marked by a unique two-stage training strategy for description-free multimodal generation, which does not necessitate extensive descriptions of images. We integrate classifier-free guidance to enhance the alignment of generated images and texts, ensuring more seamless and contextually relevant multimodal interactions. Our model, MiniGPT-5, exhibits substantial improvement over the baseline models on multimodal generation datasets, including MMDialog and VIST. The human evaluation shows MiniGPT-5 is better than the baseline model on more than 56\\\\% cases for multimodal generation, highlighting its efficacy across diverse benchmarks.', 'authors': ['Xin Eric Wang', 'Xuehai He', 'Kaizhi Zheng'], 'published': '2023-10-03', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'towards-end-to-end-embodied-decision-making', 'arxiv_id': '2310.02071', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.02071v4', 'url_pdf': 'https://arxiv.org/pdf/2310.02071v4.pdf', 'title': 'Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond', 'abstract': 'In this study, we explore the potential of Multimodal Large Language Models (MLLMs) in improving embodied decision-making processes for agents. While Large Language Models (LLMs) have been widely used due to their advanced reasoning skills and vast world knowledge, MLLMs like GPT4-Vision offer enhanced visual understanding and reasoning capabilities. We investigate whether state-of-the-art MLLMs can handle embodied decision-making in an end-to-end manner and whether collaborations between LLMs and MLLMs can enhance decision-making. To address these questions, we introduce a new benchmark called PCA-EVAL, which evaluates embodied decision-making from the perspectives of Perception, Cognition, and Action. Additionally, we propose HOLMES, a multi-agent cooperation framework that allows LLMs to leverage MLLMs and APIs to gather multimodal information for informed decision-making. We compare end-to-end embodied decision-making and HOLMES on our benchmark and find that the GPT4-Vision model demonstrates strong end-to-end embodied decision-making abilities, outperforming GPT4-HOLMES in terms of average decision accuracy (+3%). However, this performance is exclusive to the latest GPT4-Vision model, surpassing the open-source state-of-the-art MLLM by 26%. Our results indicate that powerful MLLMs like GPT4-Vision hold promise for decision-making in embodied agents, offering new avenues for MLLM research. Code and data are open at https://github.com/pkunlp-icler/PCA-EVAL/.', 'authors': ['Peiyi Wang', 'Baobao Chang', 'Tianyu Liu', 'Yuchi Wang', 'Zefan Cai', 'Haozhe Zhao', 'Shuhuai Ren', 'Yichi Zhang', 'Liang Chen'], 'published': '2023-10-03', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'kosmos-g-generating-images-in-context-with', 'arxiv_id': '2310.02992', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.02992v3', 'url_pdf': 'https://arxiv.org/pdf/2310.02992v3.pdf', 'title': 'Kosmos-G: Generating Images in Context with Multimodal Large Language Models', 'abstract': 'Recent advancements in subject-driven image generation have made significant strides. However, current methods still fall short in diverse application scenarios, as they require test-time tuning and cannot accept interleaved multi-image and text input. These limitations keep them far from the ultimate goal of \"image as a foreign language in image generation.\" This paper presents Kosmos-G, a model that leverages the advanced multimodal perception capabilities of Multimodal Large Language Models (MLLMs) to tackle the aforementioned challenge. Our approach aligns the output space of MLLM with CLIP using the textual modality as an anchor and performs compositional instruction tuning on curated data. Kosmos-G demonstrates an impressive capability of zero-shot subject-driven generation with interleaved multi-image and text input. Notably, the score distillation instruction tuning requires no modifications to the image decoder. This allows for a seamless substitution of CLIP and effortless integration with a myriad of U-Net techniques ranging from fine-grained controls to personalized image decoder variants. We posit Kosmos-G as an initial attempt towards the goal of \"image as a foreign language in image generation.\" The code can be found at https://aka.ms/Kosmos-G', 'authors': ['Furu Wei', 'Wenhu Chen', 'Zhiliang Peng', 'Shaohan Huang', 'Li Dong', 'Xichen Pan'], 'published': '2023-10-04', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'fine-grained-audio-visual-joint', 'arxiv_id': '2310.05863', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.05863v2', 'url_pdf': 'https://arxiv.org/pdf/2310.05863v2.pdf', 'title': 'Fine-grained Audio-Visual Joint Representations for Multimodal Large Language Models', 'abstract': 'Audio-visual large language models (LLM) have drawn significant attention, yet the fine-grained combination of both input streams is rather under-explored, which is challenging but necessary for LLMs to understand general video inputs. To this end, a fine-grained audio-visual joint representation (FAVOR) learning framework for multimodal LLMs is proposed in this paper, which extends a text-based LLM to simultaneously perceive speech and audio events in the audio input stream and images or videos in the visual input stream, at the frame level. To fuse the audio and visual feature streams into joint representations and to align the joint space with the LLM input embedding space, we propose a causal Q-Former structure with a causal attention module to enhance the capture of causal relations of the audio-visual frames across time. An audio-visual evaluation benchmark (AVEB) is also proposed which comprises six representative single-modal tasks with five cross-modal tasks reflecting audio-visual co-reasoning abilities. While achieving competitive single-modal performance on audio, speech and image tasks in AVEB, FAVOR achieved over 20% accuracy improvements on the video question-answering task when fine-grained information or temporal causal reasoning is required. FAVOR, in addition, demonstrated remarkable video comprehension and reasoning abilities on tasks that are unprecedented by other multimodal LLMs. An interactive demo of FAVOR is available at https://github.com/BriansIDP/AudioVisualLLM.git, and the training code and model checkpoints will be released soon.', 'authors': ['Chao Zhang', 'Zejun Ma', 'Lu Lu', 'Wei Li', 'Tian Tan', 'Xianzhao Chen', 'Changli Tang', 'Wenyi Yu', 'Guangzhi Sun'], 'published': '2023-10-09', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'can-we-edit-multimodal-large-language-models', 'arxiv_id': '2310.08475', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.08475v5', 'url_pdf': 'https://arxiv.org/pdf/2310.08475v5.pdf', 'title': 'Can We Edit Multimodal Large Language Models?', 'abstract': 'In this paper, we focus on editing Multimodal Large Language Models (MLLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights. Code and dataset are available in https://github.com/zjunlp/EasyEdit.', 'authors': ['Ningyu Zhang', 'Huajun Chen', 'Yongheng Wang', 'Xi Chen', 'Qingbin Liu', 'Bozhong Tian', 'Siyuan Cheng'], 'published': '2023-10-12', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'autorepo-a-general-framework-for-multi-modal', 'arxiv_id': '2310.07944', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.07944v2', 'url_pdf': 'https://arxiv.org/pdf/2310.07944v2.pdf', 'title': 'AutoRepo: A general framework for multi-modal LLM-based automated construction reporting', 'abstract': 'Ensuring the safety, quality, and timely completion of construction projects is paramount, with construction inspections serving as a vital instrument towards these goals. Nevertheless, the predominantly manual approach of present-day inspections frequently results in inefficiencies and inadequate information management. Such methods often fall short of providing holistic, exhaustive assessments, consequently engendering regulatory oversights and potential safety hazards. To address this issue, this paper presents a novel framework named AutoRepo for automated generation of construction inspection reports. The unmanned vehicles efficiently perform construction inspections and collect scene information, while the multimodal large language models (LLMs) are leveraged to automatically generate the inspection reports. The framework was applied and tested on a real-world construction site, demonstrating its potential to expedite the inspection process, significantly reduce resource allocation, and produce high-quality, regulatory standard-compliant inspection reports. This research thus underscores the immense potential of multimodal large language models in revolutionizing construction inspection practices, signaling a significant leap forward towards a more efficient and safer construction management paradigm.', 'authors': ['Heng Li', 'Runhao Guo', 'Jing Li', 'Xincong Yang', 'Hongxu Pu'], 'published': '2023-10-11', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'mm-bigbench-evaluating-multimodal-models-on', 'arxiv_id': '2310.09036', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.09036v1', 'url_pdf': 'https://arxiv.org/pdf/2310.09036v1.pdf', 'title': 'MM-BigBench: Evaluating Multimodal Models on Multimodal Content Comprehension Tasks', 'abstract': \"The popularity of multimodal large language models (MLLMs) has triggered a recent surge in research efforts dedicated to evaluating these models. Nevertheless, existing evaluation studies of MLLMs primarily focus on the comprehension and reasoning of unimodal (vision) content, neglecting performance evaluations in the domain of multimodal (vision-language) content understanding. Beyond multimodal reasoning, tasks related to multimodal content comprehension necessitate a profound understanding of multimodal contexts, achieved through the multimodal interaction to obtain a final answer. In this paper, we introduce a comprehensive assessment framework called MM-BigBench, which incorporates a diverse range of metrics to offer an extensive evaluation of the performance of various models and instructions across a wide spectrum of diverse multimodal content comprehension tasks. Consequently, our work complements research on the performance of MLLMs in multimodal comprehension tasks, achieving a more comprehensive and holistic evaluation of MLLMs. To begin, we employ the Best Performance metric to ascertain each model's performance upper bound on different datasets. Subsequently, the Mean Relative Gain metric offers an assessment of the overall performance of various models and instructions, while the Stability metric measures their sensitivity. Furthermore, previous research centers on evaluating models independently or solely assessing instructions, neglecting the adaptability between models and instructions. We propose the Adaptability metric to quantify the adaptability between models and instructions. Our paper evaluates a total of 20 language models (14 MLLMs) on 14 multimodal datasets spanning 6 tasks, with 10 instructions for each task, and derives novel insights. Our code will be released at https://github.com/declare-lab/MM-BigBench.\", 'authors': ['Soujanya Poria', 'XiaoMing Fu', 'Yifei Zhang', 'Qi Sun', 'Yang Li', 'Daling Wang', 'Ming Wang', 'Shi Feng', 'Wenfang Wu', 'Xiaocui Yang'], 'published': '2023-10-13', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'robollm-robotic-vision-tasks-grounded-on', 'arxiv_id': '2310.10221', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.10221v2', 'url_pdf': 'https://arxiv.org/pdf/2310.10221v2.pdf', 'title': 'RoboLLM: Robotic Vision Tasks Grounded on Multimodal Large Language Models', 'abstract': 'Robotic vision applications often necessitate a wide range of visual perception tasks, such as object detection, segmentation, and identification. While there have been substantial advances in these individual tasks, integrating specialized models into a unified vision pipeline presents significant engineering challenges and costs. Recently, Multimodal Large Language Models (MLLMs) have emerged as novel backbones for various downstream tasks. We argue that leveraging the pre-training capabilities of MLLMs enables the creation of a simplified framework, thus mitigating the need for task-specific encoders. Specifically, the large-scale pretrained knowledge in MLLMs allows for easier fine-tuning to downstream robotic vision tasks and yields superior performance. We introduce the RoboLLM framework, equipped with a BEiT-3 backbone, to address all visual perception tasks in the ARMBench challenge-a large-scale robotic manipulation dataset about real-world warehouse scenarios. RoboLLM not only outperforms existing baselines but also substantially reduces the engineering burden associated with model selection and tuning. The source code is publicly available at https://github.com/longkukuhi/armbench.', 'authors': ['Gerardo Aragon Camarasa', 'Richard McCreadie', 'George Killick', 'Zijun Long'], 'published': '2023-10-16', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'benchmarking-sequential-visual-input', 'arxiv_id': '2310.13473', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.13473v1', 'url_pdf': 'https://arxiv.org/pdf/2310.13473v1.pdf', 'title': 'Benchmarking Sequential Visual Input Reasoning and Prediction in Multimodal Large Language Models', 'abstract': \"Multimodal large language models (MLLMs) have shown great potential in perception and interpretation tasks, but their capabilities in predictive reasoning remain under-explored. To address this gap, we introduce a novel benchmark that assesses the predictive reasoning capabilities of MLLMs across diverse scenarios. Our benchmark targets three important domains: abstract pattern reasoning, human activity prediction, and physical interaction prediction. We further develop three evaluation methods powered by large language model to robustly quantify a model's performance in predicting and reasoning the future based on multi-visual context. Empirical experiments confirm the soundness of the proposed benchmark and evaluation methods via rigorous testing and reveal pros and cons of current popular MLLMs in the task of predictive reasoning. Lastly, our proposed benchmark provides a standardized evaluation framework for MLLMs and can facilitate the development of more advanced models that can reason and predict over complex long sequence of multimodal input.\", 'authors': ['Jianwei Yin', 'Tiancheng Zhao', 'Kangjia Zhao', 'Yu Shu', 'Leigang Sha', 'Mingwei Zhu'], 'published': '2023-10-20', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'woodpecker-hallucination-correction-for', 'arxiv_id': '2310.16045', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.16045v2', 'url_pdf': 'https://arxiv.org/pdf/2310.16045v2.pdf', 'title': 'Woodpecker: Hallucination Correction for Multimodal Large Language Models', 'abstract': 'Hallucination is a big shadow hanging over the rapidly evolving Multimodal Large Language Models (MLLMs), referring to the phenomenon that the generated text is inconsistent with the image content. In order to mitigate hallucinations, existing studies mainly resort to an instruction-tuning manner that requires retraining the models with specific data. In this paper, we pave a different way, introducing a training-free method named Woodpecker. Like a woodpecker heals trees, it picks out and corrects hallucinations from the generated text. Concretely, Woodpecker consists of five stages: key concept extraction, question formulation, visual knowledge validation, visual claim generation, and hallucination correction. Implemented in a post-remedy manner, Woodpecker can easily serve different MLLMs, while being interpretable by accessing intermediate outputs of the five stages. We evaluate Woodpecker both quantitatively and qualitatively and show the huge potential of this new paradigm. On the POPE benchmark, our method obtains a 30.66%/24.33% improvement in accuracy over the baseline MiniGPT-4/mPLUG-Owl. The source code is released at https://github.com/BradyFU/Woodpecker.', 'authors': ['Enhong Chen', 'Xing Sun', 'Ke Li', 'Yunhang Shen', 'Dianbo Sui', 'Hao Wang', 'Tong Xu', 'Sirui Zhao', 'Chaoyou Fu', 'Shukang Yin'], 'published': '2023-10-24', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'visual-cropping-improves-zero-shot-question', 'arxiv_id': '2310.16033', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.16033v3', 'url_pdf': 'https://arxiv.org/pdf/2310.16033v3.pdf', 'title': 'Towards Perceiving Small Visual Details in Zero-shot Visual Question Answering with Multimodal LLMs', 'abstract': \"Multimodal Large Language Models (MLLMs) have recently achieved promising zero-shot accuracy on visual question answering (VQA) -- a fundamental task affecting various downstream applications and domains. Given the great potential for the broad use of these models, it is important to investigate their limitations in dealing with different image and question properties. In this work, we investigate whether MLLMs can perceive small details as well as large details in images. In particular, we show that their zero-shot accuracy in answering visual questions is very sensitive to the size of the visual subject of the question, declining up to 46% with size. Furthermore, we show that this effect is causal by observing that human visual cropping can significantly mitigate their sensitivity to size. Inspired by the usefulness of human cropping, we then propose five automatic visual cropping methods -- leveraging either external localization models or the decision process of the given MLLM itself -- as inference time mechanisms to improve the zero-shot performance of MLLMs. We study their effectiveness on four popular VQA datasets, and a subset of the VQAv2 dataset tailored towards fine visual details. Our findings suggest that MLLMs should be used with caution in detail-sensitive VQA applications, and that visual cropping is a promising direction to improve their zero-shot performance. To facilitate further investigation of MLLMs' behaviors, our code and data are publicly released.\", 'authors': ['Filip Ilievski', 'Prateek Chhikara', 'Mahyar Khayatkhoei', 'Jiarui Zhang'], 'published': '2023-10-24', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'chef-a-comprehensive-evaluation-framework-for', 'arxiv_id': '2311.02692', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2311.02692v1', 'url_pdf': 'https://arxiv.org/pdf/2311.02692v1.pdf', 'title': 'ChEF: A Comprehensive Evaluation Framework for Standardized Assessment of Multimodal Large Language Models', 'abstract': \"Multimodal Large Language Models (MLLMs) have shown impressive abilities in interacting with visual content with myriad potential downstream tasks. However, even though a list of benchmarks has been proposed, the capabilities and limitations of MLLMs are still not comprehensively understood, due to a lack of a standardized and holistic evaluation framework. To this end, we present the first Comprehensive Evaluation Framework (ChEF) that can holistically profile each MLLM and fairly compare different MLLMs. First, we structure ChEF as four modular components, i.e., Scenario as scalable multimodal datasets, Instruction as flexible instruction retrieving formulae, Inferencer as reliable question answering strategies, and Metric as indicative task-specific score functions. Based on them, ChEF facilitates versatile evaluations in a standardized framework, and new evaluations can be built by designing new Recipes (systematic selection of these four components). Notably, current MLLM benchmarks can be readily summarized as recipes of ChEF. Second, we introduce 6 new recipes to quantify competent MLLMs' desired capabilities (or called desiderata, i.e., calibration, in-context learning, instruction following, language performance, hallucination, and robustness) as reliable agents that can perform real-world multimodal interactions. Third, we conduct a large-scale evaluation of 9 prominent MLLMs on 9 scenarios and 6 desiderata. Our evaluation summarized over 20 valuable observations concerning the generalizability of MLLMs across various scenarios and the composite capability of MLLMs required for multimodal interactions. We will publicly release all the detailed implementations for further analysis, as well as an easy-to-use modular toolkit for the integration of new recipes and models, so that ChEF can be a growing evaluation framework for the MLLM community.\", 'authors': ['Jing Shao', 'Yu Qiao', 'Lu Sheng', 'Zhenfei Yin', 'Hongxing Fan', 'Zhipin Wang', 'Zhelun Shi'], 'published': '2023-11-05', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'octavius-mitigating-task-interference-in', 'arxiv_id': '2311.02684', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2311.02684v3', 'url_pdf': 'https://arxiv.org/pdf/2311.02684v3.pdf', 'title': 'Octavius: Mitigating Task Interference in MLLMs via LoRA-MoE', 'abstract': 'Recent studies have demonstrated Large Language Models (LLMs) can extend their zero-shot generalization capabilities to multimodal learning through instruction tuning. As more modalities and downstream tasks are introduced, negative conflicts and interference may have a worse impact on performance. While this phenomenon has been overlooked in previous work, we propose a novel and extensible framework, called Octavius, for comprehensive studies and experimentation on multimodal learning with Multimodal Large Language Models (MLLMs). Specifically, we combine the well-known Mixture-of-Experts (MoE) and one of the representative PEFT techniques, i.e., LoRA, designing a novel LLM-based decoder, called LoRA-MoE, for multimodal learning. To the best of our knowledge, we are one of the pioneering efforts to introduce MoE into MLLMs to address this problem. The experimental results (about 20% improvement) have shown the effectiveness and versatility of our design in various 2D and 3D downstream tasks. Code and datasets are available at https://openlamm.github.io/tutorial/.', 'authors': ['Jing Shao', 'Yu Qiao', 'Wanli Ouyang', 'Lu Sheng', 'Si Liu', 'Zhenfei Yin', 'Huayang Liu', 'Zhen Wang', 'Ziqin Wang', 'Zeren Chen'], 'published': '2023-11-05', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'what-large-language-models-bring-to-text-rich', 'arxiv_id': '2311.07306', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2311.07306v1', 'url_pdf': 'https://arxiv.org/pdf/2311.07306v1.pdf', 'title': 'What Large Language Models Bring to Text-rich VQA?', 'abstract': \"Text-rich VQA, namely Visual Question Answering based on text recognition in the images, is a cross-modal task that requires both image comprehension and text recognition. In this work, we focus on investigating the advantages and bottlenecks of LLM-based approaches in addressing this problem. To address the above concern, we separate the vision and language modules, where we leverage external OCR models to recognize texts in the image and Large Language Models (LLMs) to answer the question given texts. The whole framework is training-free benefiting from the in-context ability of LLMs. This pipeline achieved superior performance compared to the majority of existing Multimodal Large Language Models (MLLM) on four text-rich VQA datasets. Besides, based on the ablation study, we find that LLM brings stronger comprehension ability and may introduce helpful knowledge for the VQA problem. The bottleneck for LLM to address text-rich VQA problems may primarily lie in visual part. We also combine the OCR module with MLLMs and pleasantly find that the combination of OCR module with MLLM also works. It's worth noting that not all MLLMs can comprehend the OCR information, which provides insights into how to train an MLLM that preserves the abilities of LLM.\", 'authors': ['Fei Tan', 'Zechao Li', 'Rui Zhao', 'Jinghui Lu', 'Xinzhe Ni', 'Wei Tang', 'Xuejing Liu'], 'published': '2023-11-13', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}], 'docs': [Document(metadata={'title': 'Multimodal large language models for inclusive collaboration learning tasks'}, page_content='This PhD project leverages advancements in multimodal large language models to build an inclusive collaboration feedback loop, in order to facilitate the automated detection, modeling, and feedback for participants developing general collaboration skills. This topic is important given the role of collaboration as an essential 21st century skill, the potential to ground large language models within learning theory and real-world practice, and the expressive potential of transformer models to support equity and inclusion. We address some concerns of integrating advances in natural language processing into downstream tasks such as the learning analytics feedback loop.'), Document(metadata={'title': 'Personality-aware Human-centric Multimodal Reasoning: A New Task, Dataset and Baselines'}, page_content=\"Personality traits, emotions, and beliefs shape individuals' behavioral choices and decision-making processes. However, for one thing, the affective computing community normally focused on predicting personality traits but overlooks their application in behavior prediction. For another, the multimodal reasoning task emphasized the prediction of future states and behaviors but often neglected the incorporation of individual personality traits. In this work, we introduce a new task called Personality-aware Human-centric Multimodal Reasoning (PHMR) (T1), with the goal of forecasting the future behavior of a particular individual using multimodal information from past instances, while integrating personality factors. We accordingly construct a new dataset based on six television shows, encompassing 225 characters and 12k samples. To establish a benchmark for the task, we propose seven baseline methods: three adapted from related tasks, two pre-trained model, and two multimodal large language models. The experimental results demonstrate that incorporating personality traits enhances human-centric multimodal reasoning performance. To further solve the lack of personality annotation in real-life scenes, we introduce an extension task called Personality-predicted Human-centric Multimodal Reasoning task (T2) along with the corresponding dataset and method. We will make our dataset and code available on GitHub.\"), Document(metadata={'title': 'Visual Transformation Telling'}, page_content='Humans can naturally reason from superficial state differences (e.g. ground wetness) to transformations descriptions (e.g. raining) according to their life experience. In this paper, we propose a new visual reasoning task to test this transformation reasoning ability in real-world scenarios, called \\\\textbf{V}isual \\\\textbf{T}ransformation \\\\textbf{T}elling (VTT). Given a series of states (i.e. images), VTT requires to describe the transformation occurring between every two adjacent states. Different from existing visual reasoning tasks that focus on surface state reasoning, the advantage of VTT is that it captures the underlying causes, e.g. actions or events, behind the differences among states. We collect a novel dataset to support the study of transformation reasoning from two existing instructional video datasets, CrossTask and COIN, comprising 13,547 samples. Each sample involves the key state images along with their transformation descriptions. Our dataset covers diverse real-world activities, providing a rich resource for training and evaluation. To construct an initial benchmark for VTT, we test several models, including traditional visual storytelling methods (CST, GLACNet, Densecap) and advanced multimodal large language models (LLaVA v1.5-7B, Qwen-VL-chat, Gemini Pro Vision, GPT-4o, and GPT-4). Experimental results reveal that even state-of-the-art models still face challenges in VTT, highlighting substantial areas for improvement.'), Document(metadata={'title': 'Listen, Think, and Understand'}, page_content='The ability of artificial intelligence (AI) systems to perceive and comprehend audio signals is crucial for many applications. Although significant progress has been made in this area since the development of AudioSet, most existing models are designed to map audio inputs to pre-defined, discrete sound label sets. In contrast, humans possess the ability to not only classify sounds into general categories, but also to listen to the finer details of the sounds, explain the reason for the predictions, think about what the sound infers, and understand the scene and what action needs to be taken, if any. Such capabilities beyond perception are not yet present in existing audio models. On the other hand, modern large language models (LLMs) exhibit emerging reasoning ability but they lack audio perception capabilities. Therefore, we ask the question: can we build a model that has both audio perception and a reasoning ability? In this paper, we propose a new audio foundation model, called LTU (Listen, Think, and Understand). To train LTU, we created a new OpenAQA-5M dataset consisting of 1.9 million closed-ended and 3.7 million open-ended, diverse (audio, question, answer) tuples, and have used an autoregressive training framework with a perception-to-understanding curriculum. LTU demonstrates strong performance and generalization ability on conventional audio tasks such as classification and captioning. More importantly, it exhibits emerging audio reasoning and comprehension abilities that are absent in existing audio models. To the best of our knowledge, LTU is one of the first multimodal large language models that focus on general audio (rather than just speech) understanding.'), Document(metadata={'title': 'What Makes for Good Visual Tokenizers for Large Language Models?'}, page_content='We empirically investigate proper pre-training methods to build good visual tokenizers, making Large Language Models (LLMs) powerful Multimodal Large Language Models (MLLMs). In our benchmark, which is curated to evaluate MLLMs visual semantic understanding and fine-grained perception capabilities, we discussed different visual tokenizers pre-trained with dominant methods (i.e., DeiT, CLIP, MAE, DINO), and observe that: i) Fully/weakly supervised models capture more semantics than self-supervised models, but the gap is narrowed by scaling up the pre-training dataset. ii) Self-supervised models are better at fine-grained perception, where patch-level supervision is particularly effective. iii) Tuning the visual tokenizer leads to the loss of semantics obtained from large-scale pretraining, which is unfavorable with relatively small-scale instruction-tuning dataset. Given the findings, we reviewed methods that attempted to unify semantics and fine-grained visual understanding, e.g., patch-level feature distillation with semantically-rich targets. We obtain an intriguing insight mask-based strategies that were once all the rage may not be applicable for obtaining good visual tokenizers. Based on this critical observation, we obtain a new MLLM equipped with a tailored Good Visual Tokenizer (GVT), which exhibits strong visual comprehension capability at multiple scales. In particular, without introducing extra parameters and task-specific fine-tuning, GVT achieves superior performance on visual question answering, image captioning, and other fine-grained visual understanding tasks such as object counting and multi-class identification.'), Document(metadata={'title': 'Dynamic Clue Bottlenecks: Towards Interpretable-by-Design Visual Question Answering'}, page_content='Recent advances in multimodal large language models (LLMs) have shown extreme effectiveness in visual question answering (VQA). However, the design nature of these end-to-end models prevents them from being interpretable to humans, undermining trust and applicability in critical domains. While post-hoc rationales offer certain insight into understanding model behavior, these explanations are not guaranteed to be faithful to the model. In this paper, we address these shortcomings by introducing an interpretable by design model that factors model decisions into intermediate human-legible explanations, and allows people to easily understand why a model fails or succeeds. We propose the Dynamic Clue Bottleneck Model ( (DCLUB), a method that is designed towards an inherently interpretable VQA system. DCLUB provides an explainable intermediate space before the VQA decision and is faithful from the beginning, while maintaining comparable performance to black-box systems. Given a question, DCLUB first returns a set of visual clues: natural language statements of visually salient evidence from the image, and then generates the output based solely on the visual clues. To supervise and evaluate the generation of VQA explanations within DCLUB, we collect a dataset of 1.7k reasoning-focused questions with visual clues. Evaluations show that our inherently interpretable system can improve 4.64% over a comparable black-box system in reasoning-focused questions while preserving 99.43% of performance on VQA-v2.'), Document(metadata={'title': 'PathAsst: A Generative Foundation AI Assistant Towards Artificial General Intelligence of Pathology'}, page_content=\"As advances in large language models (LLMs) and multimodal techniques continue to mature, the development of general-purpose multimodal large language models (MLLMs) has surged, offering significant applications in interpreting natural images. However, the field of pathology has largely remained untapped, particularly in gathering high-quality data and designing comprehensive model frameworks. To bridge the gap in pathology MLLMs, we present PathAsst, a multimodal generative foundation AI assistant to revolutionize diagnostic and predictive analytics in pathology. The development of PathAsst involves three pivotal steps: data acquisition, CLIP model adaptation, and the training of PathAsst's multimodal generative capabilities. Firstly, we collect over 207K high-quality pathology image-text pairs from authoritative sources. Leveraging the advanced power of ChatGPT, we generate over 180K instruction-following samples. Furthermore, we devise additional instruction-following data specifically tailored for invoking eight pathology-specific sub-models we prepared, allowing the PathAsst to effectively collaborate with these models, enhancing its diagnostic ability. Secondly, by leveraging the collected data, we construct PathCLIP, a pathology-dedicated CLIP, to enhance PathAsst's capabilities in interpreting pathology images. Finally, we integrate PathCLIP with the Vicuna-13b and utilize pathology-specific instruction-tuning data to enhance the multimodal generation capacity of PathAsst and bolster its synergistic interactions with sub-models. The experimental results of PathAsst show the potential of harnessing AI-powered generative foundation model to improve pathology diagnosis and treatment processes.\"), Document(metadata={'title': 'Contextual Object Detection with Multimodal Large Language Models'}, page_content='Recent Multimodal Large Language Models (MLLMs) are remarkable in vision-language tasks, such as image captioning and question answering, but lack the essential perception ability, i.e., object detection. In this work, we address this limitation by introducing a novel research problem of contextual object detection -- understanding visible objects within different human-AI interactive contexts. Three representative scenarios are investigated, including the language cloze test, visual captioning, and question answering. Moreover, we present ContextDET, a unified multimodal model that is capable of end-to-end differentiable modeling of visual-language contexts, so as to locate, identify, and associate visual objects with language inputs for human-AI interaction. Our ContextDET involves three key submodels: (i) a visual encoder for extracting visual representations, (ii) a pre-trained LLM for multimodal context decoding, and (iii) a visual decoder for predicting bounding boxes given contextual object words. The new generate-then-detect framework enables us to detect object words within human vocabulary. Extensive experiments show the advantages of ContextDET on our proposed CODE benchmark, open-vocabulary detection, and referring image segmentation. Github: https://github.com/yuhangzang/ContextDET.'), Document(metadata={'title': 'A Survey on Multimodal Large Language Models'}, page_content='Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.'), Document(metadata={'title': 'MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models'}, page_content='Multimodal Large Language Model (MLLM) relies on the powerful LLM to perform multimodal tasks, showing amazing emergent abilities in recent studies, such as writing poems based on an image. However, it is difficult for these case studies to fully reflect the performance of MLLM, lacking a comprehensive evaluation. In this paper, we fill in this blank, presenting the first comprehensive MLLM Evaluation benchmark MME. It measures both perception and cognition abilities on a total of 14 subtasks. In order to avoid data leakage that may arise from direct use of public datasets for evaluation, the annotations of instruction-answer pairs are all manually designed. The concise instruction design allows us to fairly compare MLLMs, instead of struggling in prompt engineering. Besides, with such an instruction, we can also easily carry out quantitative statistics. A total of 30 advanced MLLMs are comprehensively evaluated on our MME, which not only suggests that existing MLLMs still have a large room for improvement, but also reveals the potential directions for the subsequent model optimization. The data application manner and online leaderboards are released at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation.'), Document(metadata={'title': 'Kosmos-2: Grounding Multimodal Large Language Models to the World'}, page_content=\"We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. Specifically, we represent refer expressions as links in Markdown, i.e., ``[text span](bounding boxes)'', where object descriptions are sequences of location tokens. Together with multimodal corpora, we construct large-scale data of grounded image-text pairs (called GrIT) to train the model. In addition to the existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), Kosmos-2 integrates the grounding capability into downstream applications. We evaluate Kosmos-2 on a wide range of tasks, including (i) multimodal grounding, such as referring expression comprehension, and phrase grounding, (ii) multimodal referring, such as referring expression generation, (iii) perception-language tasks, and (iv) language understanding and generation. This work lays out the foundation for the development of Embodiment AI and sheds light on the big convergence of language, multimodal perception, action, and world modeling, which is a key step toward artificial general intelligence. Code and pretrained models are available at https://aka.ms/kosmos-2.\"), Document(metadata={'title': \"Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic\"}, page_content=\"In human conversations, individuals can indicate relevant regions within a scene while addressing others. In turn, the other person can then respond by referring to specific regions if necessary. This natural referential ability in dialogue remains absent in current Multimodal Large Language Models (MLLMs). To fill this gap, this paper proposes an MLLM called Shikra, which can handle spatial coordinate inputs and outputs in natural language. Its architecture consists of a vision encoder, an alignment layer, and a LLM. It is designed to be straightforward and simple, without the need for extra vocabularies, position encoder, pre-/post-detection modules, or external plug-in models. All inputs and outputs are in natural language form. Referential dialogue is a superset of various vision-language (VL) tasks. Shikra can naturally handle location-related tasks like REC and PointQA, as well as conventional VL tasks such as Image Captioning and VQA. Experimental results showcase Shikra's promising performance. Furthermore, it enables numerous exciting applications, like providing mentioned objects' coordinates in chains of thoughts and comparing user-pointed regions similarities. Our code, model and dataset are accessed at https://github.com/shikras/shikra.\"), Document(metadata={'title': 'SVIT: Scaling up Visual Instruction Tuning'}, page_content=\"Thanks to the emerging of foundation models, the large language and vision models are integrated to acquire the multimodal ability of visual captioning, question answering, etc. Although existing multimodal models present impressive performance of visual understanding and reasoning, their limits are still largely under-explored due to the scarcity of high-quality instruction tuning data. To push the limits of multimodal capability, we Scale up Visual Instruction Tuning (SVIT) by constructing a dataset of 4.2 million visual instruction tuning data including 1.6M conversation question-answer (QA) pairs, 1.6M complex reasoning QA pairs, 1.0M referring QA pairs and 106K detailed image descriptions. Besides the volume, the proposed dataset is also featured by the high quality and rich diversity, which is generated by prompting GPT-4 with the abundant manual annotations of images. We also propose a new data recipe to select subset with better diversity and balance, which evokes model's superior capabilities. Extensive experiments verify that SVIT-v1.5, trained on the proposed dataset, outperforms state-of-the-art Multimodal Large Language Models on popular benchmarks. The data and code are publicly available at https://github.com/BAAI-DCAI/Visual-Instruction-Tuning.\"), Document(metadata={'title': 'ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning'}, page_content=\"Human-AI interactivity is a critical aspect that reflects the usability of multimodal large language models (MLLMs). However, existing end-to-end MLLMs only allow users to interact with them through language instructions, leading to the limitation of the interactive accuracy and efficiency. In this study, we present precise referring instructions that utilize diverse reference representations such as points and boxes as referring prompts to refer to the special region. This enables MLLMs to focus on the region of interest and achieve finer-grained interaction. Based on precise referring instruction, we propose ChatSpot, a unified end-to-end multimodal large language model that supports diverse forms of interactivity including mouse clicks, drag-and-drop, and drawing boxes, which provides a more flexible and seamless interactive experience. We also construct a multi-grained vision-language instruction-following dataset based on existing datasets and GPT-4 generating. Furthermore, we design a series of evaluation tasks to assess the effectiveness of region recognition and interaction. Experimental results showcase ChatSpot's promising performance.\"), Document(metadata={'title': 'SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension'}, page_content='Based on powerful Large Language Models (LLMs), recent generative Multimodal Large Language Models (MLLMs) have gained prominence as a pivotal research area, exhibiting remarkable capability for both comprehension and generation. In this work, we address the evaluation of generative comprehension in MLLMs as a preliminary step towards a comprehensive assessment of generative models, by introducing a benchmark named SEED-Bench. SEED-Bench consists of 19K multiple choice questions with accurate human annotations (x 6 larger than existing benchmarks), which spans 12 evaluation dimensions including the comprehension of both the image and video modality. We develop an advanced pipeline for generating multiple-choice questions that target specific evaluation dimensions, integrating both automatic filtering and manual verification processes. Multiple-choice questions with groundtruth options derived from human annotation enables an objective and efficient assessment of model performance, eliminating the need for human or GPT intervention during evaluation. We further evaluate the performance of 18 models across all 12 dimensions, covering both the spatial and temporal understanding. By revealing the limitations of existing MLLMs through evaluation results, we aim for SEED-Bench to provide insights for motivating future research. We will launch and consistently maintain a leaderboard to provide a platform for the community to assess and investigate model capability.'), Document(metadata={'title': 'LISA: Reasoning Segmentation via Large Language Model'}, page_content='Although perception systems have made remarkable advancements in recent years, they still rely on explicit human instruction or pre-defined categories to identify the target objects before executing visual recognition tasks. Such systems cannot actively reason and comprehend implicit user intention. In this work, we propose a new segmentation task -- reasoning segmentation. The task is designed to output a segmentation mask given a complex and implicit query text. Furthermore, we establish a benchmark comprising over one thousand image-instruction-mask data samples, incorporating intricate reasoning and world knowledge for evaluation purposes. Finally, we present LISA: large Language Instructed Segmentation Assistant, which inherits the language generation capabilities of multimodal Large Language Models (LLMs) while also possessing the ability to produce segmentation masks. We expand the original vocabulary with a <SEG> token and propose the embedding-as-mask paradigm to unlock the segmentation capability. Remarkably, LISA can handle cases involving complex reasoning and world knowledge. Also, it demonstrates robust zero-shot capability when trained exclusively on reasoning-free datasets. In addition, fine-tuning the model with merely 239 reasoning segmentation data samples results in further performance enhancement. Both quantitative and qualitative experiments show our method effectively unlocks new reasoning segmentation capabilities for multimodal LLMs. Code, models, and data are available at https://github.com/dvlab-research/LISA.'), Document(metadata={'title': 'Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions'}, page_content=\"Recent advancements in Multimodal Large Language Models (MLLMs) have been utilizing Visual Prompt Generators (VPGs) to convert visual features into tokens that LLMs can recognize. This is achieved by training the VPGs on millions of image-caption pairs, where the VPG-generated tokens of images are fed into a frozen LLM to generate the corresponding captions. However, this image-captioning based training objective inherently biases the VPG to concentrate solely on the primary visual contents sufficient for caption generation, often neglecting other visual details. This shortcoming results in MLLMs' underperformance in comprehending demonstrative instructions consisting of multiple, interleaved, and multimodal instructions that demonstrate the required context to complete a task. To address this issue, we introduce a generic and lightweight Visual Prompt Generator Complete module (VPG-C), which can infer and complete the missing details essential for comprehending demonstrative instructions. Further, we propose a synthetic discriminative training strategy to fine-tune VPG-C, eliminating the need for supervised demonstrative instructions. As for evaluation, we build DEMON, a comprehensive benchmark for demonstrative instruction understanding. Synthetically trained with the proposed strategy, VPG-C achieves significantly stronger zero-shot performance across all tasks of DEMON. Further evaluation on the MME and OwlEval benchmarks also demonstrate the superiority of VPG-C. Our benchmark, code, and pre-trained models are available at https://github.com/DCDmllm/Cheetah.\"), Document(metadata={'title': 'Link-Context Learning for Multimodal LLMs'}, page_content='The ability to learn from context with novel concepts, and deliver appropriate responses are essential in human conversations. Despite current Multimodal Large Language Models (MLLMs) and Large Language Models (LLMs) being trained on mega-scale datasets, recognizing unseen images or understanding novel concepts in a training-free manner remains a challenge. In-Context Learning (ICL) explores training-free few-shot learning, where models are encouraged to ``learn to learn\" from limited tasks and generalize to unseen tasks. In this work, we propose link-context learning (LCL), which emphasizes \"reasoning from cause and effect\" to augment the learning capabilities of MLLMs. LCL goes beyond traditional ICL by explicitly strengthening the causal relationship between the support set and the query set. By providing demonstrations with causal links, LCL guides the model to discern not only the analogy but also the underlying causal associations between data points, which empowers MLLMs to recognize unseen images and understand novel concepts more effectively. To facilitate the evaluation of this novel approach, we introduce the ISEKAI dataset, comprising exclusively of unseen generated image-label pairs designed for link-context learning. Extensive experiments show that our LCL-MLLM exhibits strong link-context learning capabilities to novel concepts over vanilla MLLMs. Code and data will be released at https://github.com/isekai-portal/Link-Context-Learning.'), Document(metadata={'title': 'FashionLOGO: Prompting Multimodal Large Language Models for Fashion Logo Embeddings'}, page_content=\"Logo embedding models convert the product logos in images into vectors, enabling their utilization for logo recognition and detection within e-commerce platforms. This facilitates the enforcement of intellectual property rights and enhances product search capabilities. However, current methods treat logo embedding as a purely visual problem. A noteworthy issue is that visual models capture features more than logos. Instead, we view this as a multimodal task, using text as auxiliary information to facilitate the visual model's understanding of the logo. The emerging Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in both visual and textual understanding. Inspired by this, we propose an approach, \\\\textbf{FashionLOGO}, to explore how to prompt MLLMs to generate appropriate text for product images, which can help visual models achieve better logo embeddings. We adopt a cross-attention transformer block that enables visual embedding to automatically learn supplementary knowledge from textual embedding. Our extensive experiments on real-world datasets prove that FashionLOGO is capable of generating generic and robust logo embeddings, achieving state-of-the-art performance in all benchmarks.\"), Document(metadata={'title': 'WanJuan: A Comprehensive Multimodal Dataset for Advancing English and Chinese Large Models'}, page_content='The rise in popularity of ChatGPT and GPT-4 has significantly accelerated the development of large models, leading to the creation of numerous impressive large language models(LLMs) and multimodal large language models (MLLMs). These cutting-edge models owe their remarkable performance to high-quality data. However, the details of the training data used in leading paradigms are often kept confidential. This lack of transparency, coupled with the scarcity of open-source data, impedes further developments within the community. As a response, this paper presents \"Wan Juan\", a large-scale multimodal dataset composed of both Chinese and English data, collected from a wide range of web sources. The dataset incorporates text, image-text, and video modalities, with a total volume exceeding 2TB. It was utilized in the training of InternLM, a model that demonstrated significant advantages in multi-dimensional evaluations when compared to models of a similar scale. All data can be accessed at https://opendatalab.org.cn/WanJuan1.0.'), Document(metadata={'title': 'StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data'}, page_content=\"The remarkable multimodal capabilities demonstrated by OpenAI's GPT-4 have sparked significant interest in the development of multimodal Large Language Models (LLMs). A primary research objective of such models is to align visual and textual modalities effectively while comprehending human instructions. Current methodologies often rely on annotations derived from benchmark datasets to construct image-dialogue datasets for training purposes, akin to instruction tuning in LLMs. However, these datasets often exhibit domain bias, potentially constraining the generative capabilities of the models. In an effort to mitigate these limitations, we propose a novel data collection methodology that synchronously synthesizes images and dialogues for visual instruction tuning. This approach harnesses the power of generative models, marrying the abilities of ChatGPT and text-to-image generative models to yield a diverse and controllable dataset with varied image content. Additionally, datasets can be arbitrarily scaled. This not only provides greater flexibility compared to existing methodologies but also significantly enhances several model capabilities. Our research includes comprehensive experiments conducted on various datasets. The results emphasize substantial enhancements in more than ten commonly assessed capabilities. Additionally, our model achieves state-of-the-art results across multiple widely recognized multimodal benchmarks.\"), Document(metadata={'title': 'InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4'}, page_content='Multimodal large language models are typically trained in two stages: first pre-training on image-text pairs, and then fine-tuning using supervised vision-language instruction data. Recent studies have shown that large language models can achieve satisfactory results even with a limited amount of high-quality instruction-following data. In this paper, we introduce InstructionGPT-4, which is fine-tuned on a small dataset comprising only 200 examples, amounting to approximately 6\\\\% of the instruction-following data used in the alignment dataset for MiniGPT-4. To achieve this, we first propose several metrics to access the quality of multimodal instruction data. Based on these metrics, we present an effective and trainable data selector to automatically identify and filter low-quality vision-language data. By employing this method, InstructionGPT-4 outperforms the original MiniGPT-4 on various evaluations. Overall, our findings demonstrate that less but high-quality instruction tuning data is efficient in enabling multimodal large language models to generate better output. Our code is available at https://github.com/waltonfuture/InstructionGPT-4.'), Document(metadata={'title': 'VIGC: Visual Instruction Generation and Correction'}, page_content=\"The integration of visual encoders and large language models (LLMs) has driven recent progress in multimodal large language models (MLLMs). However, the scarcity of high-quality instruction-tuning data for vision-language tasks remains a challenge. The current leading paradigm, such as LLaVA, relies on language-only GPT-4 to generate data, which requires pre-annotated image captions and detection bounding boxes, suffering from understanding image details. A practical solution to this problem would be to utilize the available multimodal large language models (MLLMs) to generate instruction data for vision-language tasks. However, it's worth noting that the currently accessible MLLMs are not as powerful as their LLM counterparts, as they tend to produce inadequate responses and generate false information. As a solution for addressing the current issue, this paper proposes the Visual Instruction Generation and Correction (VIGC) framework that enables multimodal large language models to generate instruction-tuning data and progressively enhance its quality on-the-fly. Specifically, Visual Instruction Generation (VIG) guides the vision-language model to generate diverse instruction-tuning data. To ensure generation quality, Visual Instruction Correction (VIC) adopts an iterative update mechanism to correct any inaccuracies in data produced by VIG, effectively reducing the risk of hallucination. Leveraging the diverse, high-quality data generated by VIGC, we finetune mainstream models and validate data quality based on various evaluations. Experimental results demonstrate that VIGC not only compensates for the shortcomings of language-only data generation methods, but also effectively enhances the benchmark performance. The models, datasets, and code are available at https://opendatalab.github.io/VIGC.\"), Document(metadata={'title': 'Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models'}, page_content='Recently, Multimodal Large Language Models (MLLMs) that enable Large Language Models (LLMs) to interpret images through visual instruction tuning have achieved significant success. However, existing visual instruction tuning methods only utilize image-language instruction data to align the language and image modalities, lacking a more fine-grained cross-modal alignment. In this paper, we propose Position-enhanced Visual Instruction Tuning (PVIT), which extends the functionality of MLLMs by integrating an additional region-level vision encoder. This integration promotes a more detailed comprehension of images for the MLLM. In addition, to efficiently achieve a fine-grained alignment between the vision modules and the LLM, we design multiple data generation strategies to construct an image-region-language instruction dataset. Finally, we present both quantitative experiments and qualitative analysis that demonstrate the superiority of the proposed model. Code and data will be released at https://github.com/PVIT-official/PVIT.'), Document(metadata={'title': 'MLLM-DataEngine: An Iterative Refinement Approach for MLLM'}, page_content='Despite the great advance of Multimodal Large Language Models (MLLMs) in both instruction dataset building and benchmarking, the independence of training and evaluation makes current MLLMs hard to further improve their capability under the guidance of evaluation results with a relatively low human cost. In this paper, we propose MLLM-DataEngine, a novel closed-loop system that bridges data generation, model training, and evaluation. Within each loop iteration, the MLLM-DataEngine first analyze the weakness of the model based on the evaluation results, then generate a proper incremental dataset for the next training iteration and enhance the model capability iteratively. Compared with previous data collection methods which are separate from the benchmarking, the data generated by MLLM-DataEngine shows better targeting, quality, and correctness. For targeting, we propose an Adaptive Bad-case Sampling module, which adjusts the ratio of different types of data within each incremental dataset based on the benchmarking results. For quality, we resort to GPT-4 to generate high-quality data with each given data type. For correctness, prompt design is critical for the data generation results. Rather than previous hand-crafted prompt, we propose an Interactive Prompt Optimization strategy, which optimizes the prompt with the multi-round interaction between human and GPT, and improve the correctness of generated data greatly. Through extensive experiments, we find our MLLM-DataEngine could boost the MLLM capability in a targeted and automatic manner, with only a few human participation. We hope it could be a general solution for the following MLLMs building. The MLLM-DataEngine has been open-sourced and is now available at https://github.com/opendatalab/MLLM-DataEngine.'), Document(metadata={'title': 'Socratis: Are large multimodal models emotionally aware?'}, page_content='Existing emotion prediction benchmarks contain coarse emotion labels which do not consider the diversity of emotions that an image and text can elicit in humans due to various reasons. Learning diverse reactions to multimodal content is important as intelligent machines take a central role in generating and delivering content to society. To address this gap, we propose Socratis, a societal reactions benchmark, where each image-caption (IC) pair is annotated with multiple emotions and the reasons for feeling them. Socratis contains 18K free-form reactions for 980 emotions on 2075 image-caption pairs from 5 widely-read news and image-caption (IC) datasets. We benchmark the capability of state-of-the-art multimodal large language models to generate the reasons for feeling an emotion given an IC pair. Based on a preliminary human study, we observe that humans prefer human-written reasons over 2 times more often than machine-generated ones. This shows our task is harder than standard generation tasks because it starkly contrasts recent findings where humans cannot tell apart machine vs human-written news articles, for instance. We further see that current captioning metrics based on large vision-language models also fail to correlate with human preferences. We hope that these findings and our benchmark will inspire further research on training emotionally aware models.'), Document(metadata={'title': 'MultiWay-Adapater: Adapting large-scale multi-modal models for scalable image-text retrieval'}, page_content=\"As Multimodal Large Language Models (MLLMs) grow in size, adapting them to specialized tasks becomes increasingly challenging due to high computational and memory demands. Indeed, traditional fine-tuning methods are costly, due to the need for extensive, task-specific training. While efficient adaptation methods exist that aim to reduce these costs, in practice they suffer from shallow inter-modal alignment, which severely hurts model effectiveness. To tackle these computational challenges and improve inter-modal alignment, we introduce the MultiWay-Adapter (MWA), a novel framework featuring an 'Alignment Enhancer'. This enhancer deepens inter-modal alignment, enabling high transferability with minimal tuning effort. Our experiments show that unlike prior efficient tuning approaches, MWA maintains model effectiveness, while reducing training time by up-to 57%. MWA is also lightweight, increasing model size by only 2-3% (in terms of parameters) for state-of-the-art foundation models like BEiT-3 Large. These results demonstrate that MWA provides an efficient and effective adaptation method for MLLMs, significantly broadening their applicability.\"), Document(metadata={'title': 'NExT-GPT: Any-to-Any Multimodal LLM'}, page_content='While recently Multimodal Large Language Models (MM-LLMs) have made exciting strides, they mostly fall prey to the limitation of only input-side multimodal understanding, without the ability to produce content in multiple modalities. As we humans always perceive the world and communicate with people through various modalities, developing any-to-any MM-LLMs capable of accepting and delivering content in any modality becomes essential to human-level AI. To fill the gap, we present an end-to-end general-purpose any-to-any MM-LLM system, NExT-GPT. We connect an LLM with multimodal adaptors and different diffusion decoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio. By leveraging the existing well-trained highly-performing encoders and decoders, NExT-GPT is tuned with only a small amount of parameter (1%) of certain projection layers, which not only benefits low-cost training and also facilitates convenient expansion to more potential modalities. Moreover, we introduce a modality-switching instruction tuning (MosIT) and manually curate a high-quality dataset for MosIT, based on which NExT-GPT is empowered with complex cross-modal semantic understanding and content generation. Overall, our research showcases the promising possibility of building an AI agent capable of modeling universal modalities, paving the way for more human-like AI research in the community. Project page: https://next-gpt.github.io/'), Document(metadata={'title': 'HiLM-D: Towards High-Resolution Understanding in Multimodal Large Language Models for Autonomous Driving'}, page_content=\"Autonomous driving systems generally employ separate models for different tasks resulting in intricate designs. For the first time, we leverage singular multimodal large language models (MLLMs) to consolidate multiple autonomous driving tasks from videos, i.e., the Risk Object Localization and Intention and Suggestion Prediction (ROLISP) task. ROLISP uses natural language to simultaneously identify and interpret risk objects, understand ego-vehicle intentions, and provide motion suggestions, eliminating the necessity for task-specific architectures. However, lacking high-resolution (HR) information, existing MLLMs often miss small objects (e.g., traffic cones) and overly focus on salient ones (e.g., large trucks) when applied to ROLISP. We propose HiLM-D (Towards High-Resolution Understanding in MLLMs for Autonomous Driving), an efficient method to incorporate HR information into MLLMs for the ROLISP task. Especially, HiLM-D integrates two branches: (i) the low-resolution reasoning branch, can be any MLLMs, processes low-resolution videos to caption risk objects and discern ego-vehicle intentions/suggestions; (ii) the high-resolution perception branch (HR-PB), prominent to HiLM-D,, ingests HR images to enhance detection by capturing vision-specific HR feature maps and prioritizing all potential risks over merely salient objects. Our HR-PB serves as a plug-and-play module, seamlessly fitting into current MLLMs. Experiments on the ROLISP benchmark reveal HiLM-D's notable advantage over leading MLLMs, with improvements of 4.8% in BLEU-4 for captioning and 17.2% in mIoU for detection.\"), Document(metadata={'title': 'Investigating the Catastrophic Forgetting in Multimodal Large Language Models'}, page_content='Following the success of GPT4, there has been a surge in interest in multimodal large language model (MLLM) research. This line of research focuses on developing general-purpose LLMs through fine-tuning pre-trained LLMs and vision models. However, catastrophic forgetting, a notorious phenomenon where the fine-tuned model fails to retain similar performance compared to the pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM). In this paper, we introduce EMT: Evaluating MulTimodality for evaluating the catastrophic forgetting in MLLMs, by treating each MLLM as an image classifier. We first apply EMT to evaluate several open-source fine-tuned MLLMs and we discover that almost all evaluated MLLMs fail to retain the same performance levels as their vision encoders on standard image classification tasks. Moreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess performance throughout the fine-tuning. Interestingly, our results suggest that early-stage fine-tuning on an image dataset improves performance across other image datasets, by enhancing the alignment of text and visual features. However, as fine-tuning proceeds, the MLLMs begin to hallucinate, resulting in a significant loss of generalizability, even when the image encoder remains frozen. Our results suggest that MLLMs have yet to demonstrate performance on par with their vision models on standard image classification tasks and the current MLLM fine-tuning procedure still has room for improvement.'), Document(metadata={'title': 'DreamLLM: Synergistic Multimodal Comprehension and Creation'}, page_content=\"This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy. Project page: https://dreamllm.github.io.\"), Document(metadata={'title': \"How Robust is Google's Bard to Adversarial Image Attacks?\"}, page_content=\"Multimodal Large Language Models (MLLMs) that integrate text and other modalities (especially vision) have achieved unprecedented performance in various multimodal tasks. However, due to the unsolved adversarial robustness problem of vision models, MLLMs can have more severe safety and security risks by introducing the vision inputs. In this work, we study the adversarial robustness of Google's Bard, a competitive chatbot to ChatGPT that released its multimodal capability recently, to better understand the vulnerabilities of commercial MLLMs. By attacking white-box surrogate vision encoders or MLLMs, the generated adversarial examples can mislead Bard to output wrong image descriptions with a 22% success rate based solely on the transferability. We show that the adversarial examples can also attack other MLLMs, e.g., a 26% attack success rate against Bing Chat and a 86% attack success rate against ERNIE bot. Moreover, we identify two defense mechanisms of Bard, including face detection and toxicity detection of images. We design corresponding attacks to evade these defenses, demonstrating that the current defenses of Bard are also vulnerable. We hope this work can deepen our understanding on the robustness of MLLMs and facilitate future research on defenses. Our code is available at https://github.com/thu-ml/Attack-Bard. Update: GPT-4V is available at October 2023. We further evaluate its robustness under the same set of adversarial examples, achieving a 45% attack success rate.\"), Document(metadata={'title': 'VDC: Versatile Data Cleanser based on Visual-Linguistic Inconsistency by Multimodal Large Language Models'}, page_content='The role of data in building AI systems has recently been emphasized by the emerging concept of data-centric AI. Unfortunately, in the real-world, datasets may contain dirty samples, such as poisoned samples from backdoor attack, noisy labels in crowdsourcing, and even hybrids of them. The presence of such dirty samples makes the DNNs vunerable and unreliable.Hence, it is critical to detect dirty samples to improve the quality and realiability of dataset. Existing detectors only focus on detecting poisoned samples or noisy labels, that are often prone to weak generalization when dealing with dirty samples from other domains.In this paper, we find a commonality of various dirty samples is visual-linguistic inconsistency between images and associated labels. To capture the semantic inconsistency between modalities, we propose versatile data cleanser (VDC) leveraging the surpassing capabilities of multimodal large language models (MLLM) in cross-modal alignment and reasoning.It consists of three consecutive modules: the visual question generation module to generate insightful questions about the image; the visual question answering module to acquire the semantics of the visual content by answering the questions with MLLM; followed by the visual answer evaluation module to evaluate the inconsistency.Extensive experiments demonstrate its superior performance and generalization to various categories and types of dirty samples. The code is available at \\\\url{https://github.com/zihao-ai/vdc}.'), Document(metadata={'title': 'Guiding Instruction-based Image Editing via Multimodal Large Language Models'}, page_content='Instruction-based image editing improves the controllability and flexibility of image manipulation via natural commands without elaborate descriptions or regional masks. However, human instructions are sometimes too brief for current methods to capture and follow. Multimodal large language models (MLLMs) show promising capabilities in cross-modal understanding and visual-aware response generation via LMs. We investigate how MLLMs facilitate edit instructions and present MLLM-Guided Image Editing (MGIE). MGIE learns to derive expressive instructions and provides explicit guidance. The editing model jointly captures this visual imagination and performs manipulation through end-to-end training. We evaluate various aspects of Photoshop-style modification, global photo optimization, and local editing. Extensive experimental results demonstrate that expressive instructions are crucial to instruction-based image editing, and our MGIE can lead to a notable improvement in automatic metrics and human evaluation while maintaining competitive inference efficiency.'), Document(metadata={'title': 'Reformulating Vision-Language Foundation Models and Datasets Towards Universal Multimodal Assistants'}, page_content=\"Recent Multimodal Large Language Models (MLLMs) exhibit impressive abilities to perceive images and follow open-ended instructions. The capabilities of MLLMs depend on two crucial factors: the model architecture to facilitate the feature alignment of visual modules and large language models; the multimodal instruction tuning datasets for human instruction following. (i) For the model architecture, most existing models introduce an external bridge module to connect vision encoders with language models, which needs an additional feature-alignment pre-training. In this work, we discover that compact pre-trained vision language models can inherently serve as ``out-of-the-box'' bridges between vision and language. Based on this, we propose Muffin framework, which directly employs pre-trained vision-language models to act as providers of visual signals. (ii) For the multimodal instruction tuning datasets, existing methods omit the complementary relationship between different datasets and simply mix datasets from different tasks. Instead, we propose UniMM-Chat dataset which explores the complementarities of datasets to generate 1.1M high-quality and diverse multimodal instructions. We merge information describing the same image from diverse datasets and transforms it into more knowledge-intensive conversation data. Experimental results demonstrate the effectiveness of the Muffin framework and UniMM-Chat dataset. Muffin achieves state-of-the-art performance on a wide range of vision-language tasks, significantly surpassing state-of-the-art models like LLaVA and InstructBLIP. Our model and dataset are all accessible at https://github.com/thunlp/muffin.\"), Document(metadata={'title': 'DriveGPT4: Interpretable End-to-end Autonomous Driving via Large Language Model'}, page_content='Multimodal large language models (MLLMs) have emerged as a prominent area of interest within the research community, given their proficiency in handling and reasoning with non-textual data, including images and videos. This study seeks to extend the application of MLLMs to the realm of autonomous driving by introducing DriveGPT4, a novel interpretable end-to-end autonomous driving system based on LLMs. Capable of processing multi-frame video inputs and textual queries, DriveGPT4 facilitates the interpretation of vehicle actions, offers pertinent reasoning, and effectively addresses a diverse range of questions posed by users. Furthermore, DriveGPT4 predicts low-level vehicle control signals in an end-to-end fashion.These advanced capabilities are achieved through the utilization of a bespoke visual instruction tuning dataset, specifically tailored for autonomous driving applications, in conjunction with a mix-finetuning training strategy. DriveGPT4 represents the pioneering effort to leverage LLMs for the development of an interpretable end-to-end autonomous driving solution. Evaluations conducted on the BDD-X dataset showcase the superior qualitative and quantitative performance of DriveGPT4. Additionally, the fine-tuning of domain-specific data enables DriveGPT4 to yield close or even improved results in terms of autonomous driving grounding when contrasted with GPT4-V.'), Document(metadata={'title': 'MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens'}, page_content='The effectiveness of Multimodal Large Language Models (MLLMs) demonstrates a profound capability in multimodal understanding. However, the simultaneous generation of images with coherent texts is still underdeveloped. Addressing this, we introduce a novel interleaved vision-and-language generation method, centered around the concept of ``generative vokens\". These vokens serve as pivotal elements contributing to coherent image-text outputs. Our method is marked by a unique two-stage training strategy for description-free multimodal generation, which does not necessitate extensive descriptions of images. We integrate classifier-free guidance to enhance the alignment of generated images and texts, ensuring more seamless and contextually relevant multimodal interactions. Our model, MiniGPT-5, exhibits substantial improvement over the baseline models on multimodal generation datasets, including MMDialog and VIST. The human evaluation shows MiniGPT-5 is better than the baseline model on more than 56\\\\% cases for multimodal generation, highlighting its efficacy across diverse benchmarks.'), Document(metadata={'title': 'Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond'}, page_content='In this study, we explore the potential of Multimodal Large Language Models (MLLMs) in improving embodied decision-making processes for agents. While Large Language Models (LLMs) have been widely used due to their advanced reasoning skills and vast world knowledge, MLLMs like GPT4-Vision offer enhanced visual understanding and reasoning capabilities. We investigate whether state-of-the-art MLLMs can handle embodied decision-making in an end-to-end manner and whether collaborations between LLMs and MLLMs can enhance decision-making. To address these questions, we introduce a new benchmark called PCA-EVAL, which evaluates embodied decision-making from the perspectives of Perception, Cognition, and Action. Additionally, we propose HOLMES, a multi-agent cooperation framework that allows LLMs to leverage MLLMs and APIs to gather multimodal information for informed decision-making. We compare end-to-end embodied decision-making and HOLMES on our benchmark and find that the GPT4-Vision model demonstrates strong end-to-end embodied decision-making abilities, outperforming GPT4-HOLMES in terms of average decision accuracy (+3%). However, this performance is exclusive to the latest GPT4-Vision model, surpassing the open-source state-of-the-art MLLM by 26%. Our results indicate that powerful MLLMs like GPT4-Vision hold promise for decision-making in embodied agents, offering new avenues for MLLM research. Code and data are open at https://github.com/pkunlp-icler/PCA-EVAL/.'), Document(metadata={'title': 'Kosmos-G: Generating Images in Context with Multimodal Large Language Models'}, page_content='Recent advancements in subject-driven image generation have made significant strides. However, current methods still fall short in diverse application scenarios, as they require test-time tuning and cannot accept interleaved multi-image and text input. These limitations keep them far from the ultimate goal of \"image as a foreign language in image generation.\" This paper presents Kosmos-G, a model that leverages the advanced multimodal perception capabilities of Multimodal Large Language Models (MLLMs) to tackle the aforementioned challenge. Our approach aligns the output space of MLLM with CLIP using the textual modality as an anchor and performs compositional instruction tuning on curated data. Kosmos-G demonstrates an impressive capability of zero-shot subject-driven generation with interleaved multi-image and text input. Notably, the score distillation instruction tuning requires no modifications to the image decoder. This allows for a seamless substitution of CLIP and effortless integration with a myriad of U-Net techniques ranging from fine-grained controls to personalized image decoder variants. We posit Kosmos-G as an initial attempt towards the goal of \"image as a foreign language in image generation.\" The code can be found at https://aka.ms/Kosmos-G'), Document(metadata={'title': 'Fine-grained Audio-Visual Joint Representations for Multimodal Large Language Models'}, page_content='Audio-visual large language models (LLM) have drawn significant attention, yet the fine-grained combination of both input streams is rather under-explored, which is challenging but necessary for LLMs to understand general video inputs. To this end, a fine-grained audio-visual joint representation (FAVOR) learning framework for multimodal LLMs is proposed in this paper, which extends a text-based LLM to simultaneously perceive speech and audio events in the audio input stream and images or videos in the visual input stream, at the frame level. To fuse the audio and visual feature streams into joint representations and to align the joint space with the LLM input embedding space, we propose a causal Q-Former structure with a causal attention module to enhance the capture of causal relations of the audio-visual frames across time. An audio-visual evaluation benchmark (AVEB) is also proposed which comprises six representative single-modal tasks with five cross-modal tasks reflecting audio-visual co-reasoning abilities. While achieving competitive single-modal performance on audio, speech and image tasks in AVEB, FAVOR achieved over 20% accuracy improvements on the video question-answering task when fine-grained information or temporal causal reasoning is required. FAVOR, in addition, demonstrated remarkable video comprehension and reasoning abilities on tasks that are unprecedented by other multimodal LLMs. An interactive demo of FAVOR is available at https://github.com/BriansIDP/AudioVisualLLM.git, and the training code and model checkpoints will be released soon.'), Document(metadata={'title': 'Can We Edit Multimodal Large Language Models?'}, page_content='In this paper, we focus on editing Multimodal Large Language Models (MLLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights. Code and dataset are available in https://github.com/zjunlp/EasyEdit.'), Document(metadata={'title': 'AutoRepo: A general framework for multi-modal LLM-based automated construction reporting'}, page_content='Ensuring the safety, quality, and timely completion of construction projects is paramount, with construction inspections serving as a vital instrument towards these goals. Nevertheless, the predominantly manual approach of present-day inspections frequently results in inefficiencies and inadequate information management. Such methods often fall short of providing holistic, exhaustive assessments, consequently engendering regulatory oversights and potential safety hazards. To address this issue, this paper presents a novel framework named AutoRepo for automated generation of construction inspection reports. The unmanned vehicles efficiently perform construction inspections and collect scene information, while the multimodal large language models (LLMs) are leveraged to automatically generate the inspection reports. The framework was applied and tested on a real-world construction site, demonstrating its potential to expedite the inspection process, significantly reduce resource allocation, and produce high-quality, regulatory standard-compliant inspection reports. This research thus underscores the immense potential of multimodal large language models in revolutionizing construction inspection practices, signaling a significant leap forward towards a more efficient and safer construction management paradigm.'), Document(metadata={'title': 'MM-BigBench: Evaluating Multimodal Models on Multimodal Content Comprehension Tasks'}, page_content=\"The popularity of multimodal large language models (MLLMs) has triggered a recent surge in research efforts dedicated to evaluating these models. Nevertheless, existing evaluation studies of MLLMs primarily focus on the comprehension and reasoning of unimodal (vision) content, neglecting performance evaluations in the domain of multimodal (vision-language) content understanding. Beyond multimodal reasoning, tasks related to multimodal content comprehension necessitate a profound understanding of multimodal contexts, achieved through the multimodal interaction to obtain a final answer. In this paper, we introduce a comprehensive assessment framework called MM-BigBench, which incorporates a diverse range of metrics to offer an extensive evaluation of the performance of various models and instructions across a wide spectrum of diverse multimodal content comprehension tasks. Consequently, our work complements research on the performance of MLLMs in multimodal comprehension tasks, achieving a more comprehensive and holistic evaluation of MLLMs. To begin, we employ the Best Performance metric to ascertain each model's performance upper bound on different datasets. Subsequently, the Mean Relative Gain metric offers an assessment of the overall performance of various models and instructions, while the Stability metric measures their sensitivity. Furthermore, previous research centers on evaluating models independently or solely assessing instructions, neglecting the adaptability between models and instructions. We propose the Adaptability metric to quantify the adaptability between models and instructions. Our paper evaluates a total of 20 language models (14 MLLMs) on 14 multimodal datasets spanning 6 tasks, with 10 instructions for each task, and derives novel insights. Our code will be released at https://github.com/declare-lab/MM-BigBench.\"), Document(metadata={'title': 'RoboLLM: Robotic Vision Tasks Grounded on Multimodal Large Language Models'}, page_content='Robotic vision applications often necessitate a wide range of visual perception tasks, such as object detection, segmentation, and identification. While there have been substantial advances in these individual tasks, integrating specialized models into a unified vision pipeline presents significant engineering challenges and costs. Recently, Multimodal Large Language Models (MLLMs) have emerged as novel backbones for various downstream tasks. We argue that leveraging the pre-training capabilities of MLLMs enables the creation of a simplified framework, thus mitigating the need for task-specific encoders. Specifically, the large-scale pretrained knowledge in MLLMs allows for easier fine-tuning to downstream robotic vision tasks and yields superior performance. We introduce the RoboLLM framework, equipped with a BEiT-3 backbone, to address all visual perception tasks in the ARMBench challenge-a large-scale robotic manipulation dataset about real-world warehouse scenarios. RoboLLM not only outperforms existing baselines but also substantially reduces the engineering burden associated with model selection and tuning. The source code is publicly available at https://github.com/longkukuhi/armbench.'), Document(metadata={'title': 'Benchmarking Sequential Visual Input Reasoning and Prediction in Multimodal Large Language Models'}, page_content=\"Multimodal large language models (MLLMs) have shown great potential in perception and interpretation tasks, but their capabilities in predictive reasoning remain under-explored. To address this gap, we introduce a novel benchmark that assesses the predictive reasoning capabilities of MLLMs across diverse scenarios. Our benchmark targets three important domains: abstract pattern reasoning, human activity prediction, and physical interaction prediction. We further develop three evaluation methods powered by large language model to robustly quantify a model's performance in predicting and reasoning the future based on multi-visual context. Empirical experiments confirm the soundness of the proposed benchmark and evaluation methods via rigorous testing and reveal pros and cons of current popular MLLMs in the task of predictive reasoning. Lastly, our proposed benchmark provides a standardized evaluation framework for MLLMs and can facilitate the development of more advanced models that can reason and predict over complex long sequence of multimodal input.\"), Document(metadata={'title': 'Woodpecker: Hallucination Correction for Multimodal Large Language Models'}, page_content='Hallucination is a big shadow hanging over the rapidly evolving Multimodal Large Language Models (MLLMs), referring to the phenomenon that the generated text is inconsistent with the image content. In order to mitigate hallucinations, existing studies mainly resort to an instruction-tuning manner that requires retraining the models with specific data. In this paper, we pave a different way, introducing a training-free method named Woodpecker. Like a woodpecker heals trees, it picks out and corrects hallucinations from the generated text. Concretely, Woodpecker consists of five stages: key concept extraction, question formulation, visual knowledge validation, visual claim generation, and hallucination correction. Implemented in a post-remedy manner, Woodpecker can easily serve different MLLMs, while being interpretable by accessing intermediate outputs of the five stages. We evaluate Woodpecker both quantitatively and qualitatively and show the huge potential of this new paradigm. On the POPE benchmark, our method obtains a 30.66%/24.33% improvement in accuracy over the baseline MiniGPT-4/mPLUG-Owl. The source code is released at https://github.com/BradyFU/Woodpecker.'), Document(metadata={'title': 'Towards Perceiving Small Visual Details in Zero-shot Visual Question Answering with Multimodal LLMs'}, page_content=\"Multimodal Large Language Models (MLLMs) have recently achieved promising zero-shot accuracy on visual question answering (VQA) -- a fundamental task affecting various downstream applications and domains. Given the great potential for the broad use of these models, it is important to investigate their limitations in dealing with different image and question properties. In this work, we investigate whether MLLMs can perceive small details as well as large details in images. In particular, we show that their zero-shot accuracy in answering visual questions is very sensitive to the size of the visual subject of the question, declining up to 46% with size. Furthermore, we show that this effect is causal by observing that human visual cropping can significantly mitigate their sensitivity to size. Inspired by the usefulness of human cropping, we then propose five automatic visual cropping methods -- leveraging either external localization models or the decision process of the given MLLM itself -- as inference time mechanisms to improve the zero-shot performance of MLLMs. We study their effectiveness on four popular VQA datasets, and a subset of the VQAv2 dataset tailored towards fine visual details. Our findings suggest that MLLMs should be used with caution in detail-sensitive VQA applications, and that visual cropping is a promising direction to improve their zero-shot performance. To facilitate further investigation of MLLMs' behaviors, our code and data are publicly released.\"), Document(metadata={'title': 'ChEF: A Comprehensive Evaluation Framework for Standardized Assessment of Multimodal Large Language Models'}, page_content=\"Multimodal Large Language Models (MLLMs) have shown impressive abilities in interacting with visual content with myriad potential downstream tasks. However, even though a list of benchmarks has been proposed, the capabilities and limitations of MLLMs are still not comprehensively understood, due to a lack of a standardized and holistic evaluation framework. To this end, we present the first Comprehensive Evaluation Framework (ChEF) that can holistically profile each MLLM and fairly compare different MLLMs. First, we structure ChEF as four modular components, i.e., Scenario as scalable multimodal datasets, Instruction as flexible instruction retrieving formulae, Inferencer as reliable question answering strategies, and Metric as indicative task-specific score functions. Based on them, ChEF facilitates versatile evaluations in a standardized framework, and new evaluations can be built by designing new Recipes (systematic selection of these four components). Notably, current MLLM benchmarks can be readily summarized as recipes of ChEF. Second, we introduce 6 new recipes to quantify competent MLLMs' desired capabilities (or called desiderata, i.e., calibration, in-context learning, instruction following, language performance, hallucination, and robustness) as reliable agents that can perform real-world multimodal interactions. Third, we conduct a large-scale evaluation of 9 prominent MLLMs on 9 scenarios and 6 desiderata. Our evaluation summarized over 20 valuable observations concerning the generalizability of MLLMs across various scenarios and the composite capability of MLLMs required for multimodal interactions. We will publicly release all the detailed implementations for further analysis, as well as an easy-to-use modular toolkit for the integration of new recipes and models, so that ChEF can be a growing evaluation framework for the MLLM community.\"), Document(metadata={'title': 'Octavius: Mitigating Task Interference in MLLMs via LoRA-MoE'}, page_content='Recent studies have demonstrated Large Language Models (LLMs) can extend their zero-shot generalization capabilities to multimodal learning through instruction tuning. As more modalities and downstream tasks are introduced, negative conflicts and interference may have a worse impact on performance. While this phenomenon has been overlooked in previous work, we propose a novel and extensible framework, called Octavius, for comprehensive studies and experimentation on multimodal learning with Multimodal Large Language Models (MLLMs). Specifically, we combine the well-known Mixture-of-Experts (MoE) and one of the representative PEFT techniques, i.e., LoRA, designing a novel LLM-based decoder, called LoRA-MoE, for multimodal learning. To the best of our knowledge, we are one of the pioneering efforts to introduce MoE into MLLMs to address this problem. The experimental results (about 20% improvement) have shown the effectiveness and versatility of our design in various 2D and 3D downstream tasks. Code and datasets are available at https://openlamm.github.io/tutorial/.'), Document(metadata={'title': 'What Large Language Models Bring to Text-rich VQA?'}, page_content=\"Text-rich VQA, namely Visual Question Answering based on text recognition in the images, is a cross-modal task that requires both image comprehension and text recognition. In this work, we focus on investigating the advantages and bottlenecks of LLM-based approaches in addressing this problem. To address the above concern, we separate the vision and language modules, where we leverage external OCR models to recognize texts in the image and Large Language Models (LLMs) to answer the question given texts. The whole framework is training-free benefiting from the in-context ability of LLMs. This pipeline achieved superior performance compared to the majority of existing Multimodal Large Language Models (MLLM) on four text-rich VQA datasets. Besides, based on the ablation study, we find that LLM brings stronger comprehension ability and may introduce helpful knowledge for the VQA problem. The bottleneck for LLM to address text-rich VQA problems may primarily lie in visual part. We also combine the OCR module with MLLMs and pleasantly find that the combination of OCR module with MLLM also works. It's worth noting that not all MLLMs can comprehend the OCR information, which provides insights into how to train an MLLM that preserves the abilities of LLM.\")], 'messages': [], 'retry_count': 2}\n{'query': \"latest advances in multimodal learning Consider User's response for the search query: I dont want to know about latest advancements, I want to know about multimodal llms\", 'search_query': 'multimodal large language models', 'papers': [{'id': 'multimodal-large-language-models-for', 'arxiv_id': None, 'nips_id': None, 'url_abs': 'https://aclanthology.org/2022.naacl-srw.26', 'url_pdf': 'https://aclanthology.org/2022.naacl-srw.26.pdf', 'title': 'Multimodal large language models for inclusive collaboration learning tasks', 'abstract': 'This PhD project leverages advancements in multimodal large language models to build an inclusive collaboration feedback loop, in order to facilitate the automated detection, modeling, and feedback for participants developing general collaboration skills. This topic is important given the role of collaboration as an essential 21st century skill, the potential to ground large language models within learning theory and real-world practice, and the expressive potential of transformer models to support equity and inclusion. We address some concerns of integrating advances in natural language processing into downstream tasks such as the learning analytics feedback loop.', 'authors': ['Armanda Lewis'], 'published': None, 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': 'naacl-acl-2022-7'}, {'id': 'personality-aware-human-centric-multimodal', 'arxiv_id': '2304.02313', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2304.02313v2', 'url_pdf': 'https://arxiv.org/pdf/2304.02313v2.pdf', 'title': 'Personality-aware Human-centric Multimodal Reasoning: A New Task, Dataset and Baselines', 'abstract': \"Personality traits, emotions, and beliefs shape individuals' behavioral choices and decision-making processes. However, for one thing, the affective computing community normally focused on predicting personality traits but overlooks their application in behavior prediction. For another, the multimodal reasoning task emphasized the prediction of future states and behaviors but often neglected the incorporation of individual personality traits. In this work, we introduce a new task called Personality-aware Human-centric Multimodal Reasoning (PHMR) (T1), with the goal of forecasting the future behavior of a particular individual using multimodal information from past instances, while integrating personality factors. We accordingly construct a new dataset based on six television shows, encompassing 225 characters and 12k samples. To establish a benchmark for the task, we propose seven baseline methods: three adapted from related tasks, two pre-trained model, and two multimodal large language models. The experimental results demonstrate that incorporating personality traits enhances human-centric multimodal reasoning performance. To further solve the lack of personality annotation in real-life scenes, we introduce an extension task called Personality-predicted Human-centric Multimodal Reasoning task (T2) along with the corresponding dataset and method. We will make our dataset and code available on GitHub.\", 'authors': ['Rui Xia', 'Xiangqing Shen', 'Yaochen Zhu'], 'published': '2023-04-05', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'visual-transformation-telling', 'arxiv_id': '2305.01928', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2305.01928v2', 'url_pdf': 'https://arxiv.org/pdf/2305.01928v2.pdf', 'title': 'Visual Transformation Telling', 'abstract': 'Humans can naturally reason from superficial state differences (e.g. ground wetness) to transformations descriptions (e.g. raining) according to their life experience. In this paper, we propose a new visual reasoning task to test this transformation reasoning ability in real-world scenarios, called \\\\textbf{V}isual \\\\textbf{T}ransformation \\\\textbf{T}elling (VTT). Given a series of states (i.e. images), VTT requires to describe the transformation occurring between every two adjacent states. Different from existing visual reasoning tasks that focus on surface state reasoning, the advantage of VTT is that it captures the underlying causes, e.g. actions or events, behind the differences among states. We collect a novel dataset to support the study of transformation reasoning from two existing instructional video datasets, CrossTask and COIN, comprising 13,547 samples. Each sample involves the key state images along with their transformation descriptions. Our dataset covers diverse real-world activities, providing a rich resource for training and evaluation. To construct an initial benchmark for VTT, we test several models, including traditional visual storytelling methods (CST, GLACNet, Densecap) and advanced multimodal large language models (LLaVA v1.5-7B, Qwen-VL-chat, Gemini Pro Vision, GPT-4o, and GPT-4). Experimental results reveal that even state-of-the-art models still face challenges in VTT, highlighting substantial areas for improvement.', 'authors': ['Liang Pang', 'Xin Hong', 'Wanqing Cui', 'Xueqi Cheng', 'Jiafeng Guo', 'Yanyan Lan'], 'published': '2023-05-03', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'listen-think-and-understand', 'arxiv_id': '2305.10790', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2305.10790v3', 'url_pdf': 'https://arxiv.org/pdf/2305.10790v3.pdf', 'title': 'Listen, Think, and Understand', 'abstract': 'The ability of artificial intelligence (AI) systems to perceive and comprehend audio signals is crucial for many applications. Although significant progress has been made in this area since the development of AudioSet, most existing models are designed to map audio inputs to pre-defined, discrete sound label sets. In contrast, humans possess the ability to not only classify sounds into general categories, but also to listen to the finer details of the sounds, explain the reason for the predictions, think about what the sound infers, and understand the scene and what action needs to be taken, if any. Such capabilities beyond perception are not yet present in existing audio models. On the other hand, modern large language models (LLMs) exhibit emerging reasoning ability but they lack audio perception capabilities. Therefore, we ask the question: can we build a model that has both audio perception and a reasoning ability? In this paper, we propose a new audio foundation model, called LTU (Listen, Think, and Understand). To train LTU, we created a new OpenAQA-5M dataset consisting of 1.9 million closed-ended and 3.7 million open-ended, diverse (audio, question, answer) tuples, and have used an autoregressive training framework with a perception-to-understanding curriculum. LTU demonstrates strong performance and generalization ability on conventional audio tasks such as classification and captioning. More importantly, it exhibits emerging audio reasoning and comprehension abilities that are absent in existing audio models. To the best of our knowledge, LTU is one of the first multimodal large language models that focus on general audio (rather than just speech) understanding.', 'authors': ['James Glass', 'Leonid Karlinsky', 'Alexander H. Liu', 'Hongyin Luo', 'Yuan Gong'], 'published': '2023-05-18', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'what-makes-for-good-visual-tokenizers-for', 'arxiv_id': '2305.12223', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2305.12223v2', 'url_pdf': 'https://arxiv.org/pdf/2305.12223v2.pdf', 'title': 'What Makes for Good Visual Tokenizers for Large Language Models?', 'abstract': 'We empirically investigate proper pre-training methods to build good visual tokenizers, making Large Language Models (LLMs) powerful Multimodal Large Language Models (MLLMs). In our benchmark, which is curated to evaluate MLLMs visual semantic understanding and fine-grained perception capabilities, we discussed different visual tokenizers pre-trained with dominant methods (i.e., DeiT, CLIP, MAE, DINO), and observe that: i) Fully/weakly supervised models capture more semantics than self-supervised models, but the gap is narrowed by scaling up the pre-training dataset. ii) Self-supervised models are better at fine-grained perception, where patch-level supervision is particularly effective. iii) Tuning the visual tokenizer leads to the loss of semantics obtained from large-scale pretraining, which is unfavorable with relatively small-scale instruction-tuning dataset. Given the findings, we reviewed methods that attempted to unify semantics and fine-grained visual understanding, e.g., patch-level feature distillation with semantically-rich targets. We obtain an intriguing insight mask-based strategies that were once all the rage may not be applicable for obtaining good visual tokenizers. Based on this critical observation, we obtain a new MLLM equipped with a tailored Good Visual Tokenizer (GVT), which exhibits strong visual comprehension capability at multiple scales. In particular, without introducing extra parameters and task-specific fine-tuning, GVT achieves superior performance on visual question answering, image captioning, and other fine-grained visual understanding tasks such as object counting and multi-class identification.', 'authors': ['Ying Shan', 'Mohan Kankanhalli', 'Xiaohan Ding', 'Yixiao Ge', 'Guangzhi Wang'], 'published': '2023-05-20', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'interpretable-by-design-visual-question', 'arxiv_id': '2305.14882', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2305.14882v2', 'url_pdf': 'https://arxiv.org/pdf/2305.14882v2.pdf', 'title': 'Dynamic Clue Bottlenecks: Towards Interpretable-by-Design Visual Question Answering', 'abstract': 'Recent advances in multimodal large language models (LLMs) have shown extreme effectiveness in visual question answering (VQA). However, the design nature of these end-to-end models prevents them from being interpretable to humans, undermining trust and applicability in critical domains. While post-hoc rationales offer certain insight into understanding model behavior, these explanations are not guaranteed to be faithful to the model. In this paper, we address these shortcomings by introducing an interpretable by design model that factors model decisions into intermediate human-legible explanations, and allows people to easily understand why a model fails or succeeds. We propose the Dynamic Clue Bottleneck Model ( (DCLUB), a method that is designed towards an inherently interpretable VQA system. DCLUB provides an explainable intermediate space before the VQA decision and is faithful from the beginning, while maintaining comparable performance to black-box systems. Given a question, DCLUB first returns a set of visual clues: natural language statements of visually salient evidence from the image, and then generates the output based solely on the visual clues. To supervise and evaluate the generation of VQA explanations within DCLUB, we collect a dataset of 1.7k reasoning-focused questions with visual clues. Evaluations show that our inherently interpretable system can improve 4.64% over a comparable black-box system in reasoning-focused questions while preserving 99.43% of performance on VQA-v2.', 'authors': ['Dan Roth', 'Mark Yatskar', 'Sihao Chen', 'Ben Zhou', 'Xingyu Fu'], 'published': '2023-05-24', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'pathasst-redefining-pathology-through', 'arxiv_id': '2305.15072', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2305.15072v2', 'url_pdf': 'https://arxiv.org/pdf/2305.15072v2.pdf', 'title': 'PathAsst: A Generative Foundation AI Assistant Towards Artificial General Intelligence of Pathology', 'abstract': \"As advances in large language models (LLMs) and multimodal techniques continue to mature, the development of general-purpose multimodal large language models (MLLMs) has surged, offering significant applications in interpreting natural images. However, the field of pathology has largely remained untapped, particularly in gathering high-quality data and designing comprehensive model frameworks. To bridge the gap in pathology MLLMs, we present PathAsst, a multimodal generative foundation AI assistant to revolutionize diagnostic and predictive analytics in pathology. The development of PathAsst involves three pivotal steps: data acquisition, CLIP model adaptation, and the training of PathAsst's multimodal generative capabilities. Firstly, we collect over 207K high-quality pathology image-text pairs from authoritative sources. Leveraging the advanced power of ChatGPT, we generate over 180K instruction-following samples. Furthermore, we devise additional instruction-following data specifically tailored for invoking eight pathology-specific sub-models we prepared, allowing the PathAsst to effectively collaborate with these models, enhancing its diagnostic ability. Secondly, by leveraging the collected data, we construct PathCLIP, a pathology-dedicated CLIP, to enhance PathAsst's capabilities in interpreting pathology images. Finally, we integrate PathCLIP with the Vicuna-13b and utilize pathology-specific instruction-tuning data to enhance the multimodal generation capacity of PathAsst and bolster its synergistic interactions with sub-models. The experimental results of PathAsst show the potential of harnessing AI-powered generative foundation model to improve pathology diagnosis and treatment processes.\", 'authors': ['Yunlong Zhang', 'Zhongyi Shui', 'Lin Sun', 'Lin Yang', 'Honglin Li', 'Kai Zhang', 'Sunyi Zheng', 'Chenglu Zhu', 'Yuxuan Sun'], 'published': '2023-05-24', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'contextual-object-detection-with-multimodal', 'arxiv_id': '2305.18279', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2305.18279v2', 'url_pdf': 'https://arxiv.org/pdf/2305.18279v2.pdf', 'title': 'Contextual Object Detection with Multimodal Large Language Models', 'abstract': 'Recent Multimodal Large Language Models (MLLMs) are remarkable in vision-language tasks, such as image captioning and question answering, but lack the essential perception ability, i.e., object detection. In this work, we address this limitation by introducing a novel research problem of contextual object detection -- understanding visible objects within different human-AI interactive contexts. Three representative scenarios are investigated, including the language cloze test, visual captioning, and question answering. Moreover, we present ContextDET, a unified multimodal model that is capable of end-to-end differentiable modeling of visual-language contexts, so as to locate, identify, and associate visual objects with language inputs for human-AI interaction. Our ContextDET involves three key submodels: (i) a visual encoder for extracting visual representations, (ii) a pre-trained LLM for multimodal context decoding, and (iii) a visual decoder for predicting bounding boxes given contextual object words. The new generate-then-detect framework enables us to detect object words within human vocabulary. Extensive experiments show the advantages of ContextDET on our proposed CODE benchmark, open-vocabulary detection, and referring image segmentation. Github: https://github.com/yuhangzang/ContextDET.', 'authors': ['Chen Change Loy', 'Kaiyang Zhou', 'Jun Han', 'Wei Li', 'Yuhang Zang'], 'published': '2023-05-29', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'a-survey-on-multimodal-large-language-models', 'arxiv_id': '2306.13549', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2306.13549v4', 'url_pdf': 'https://arxiv.org/pdf/2306.13549v4.pdf', 'title': 'A Survey on Multimodal Large Language Models', 'abstract': 'Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.', 'authors': ['Enhong Chen', 'Tong Xu', 'Xing Sun', 'Ke Li', 'Sirui Zhao', 'Chaoyou Fu', 'Shukang Yin'], 'published': '2023-06-23', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'mme-a-comprehensive-evaluation-benchmark-for', 'arxiv_id': '2306.13394', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2306.13394v4', 'url_pdf': 'https://arxiv.org/pdf/2306.13394v4.pdf', 'title': 'MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models', 'abstract': 'Multimodal Large Language Model (MLLM) relies on the powerful LLM to perform multimodal tasks, showing amazing emergent abilities in recent studies, such as writing poems based on an image. However, it is difficult for these case studies to fully reflect the performance of MLLM, lacking a comprehensive evaluation. In this paper, we fill in this blank, presenting the first comprehensive MLLM Evaluation benchmark MME. It measures both perception and cognition abilities on a total of 14 subtasks. In order to avoid data leakage that may arise from direct use of public datasets for evaluation, the annotations of instruction-answer pairs are all manually designed. The concise instruction design allows us to fairly compare MLLMs, instead of struggling in prompt engineering. Besides, with such an instruction, we can also easily carry out quantitative statistics. A total of 30 advanced MLLMs are comprehensively evaluated on our MME, which not only suggests that existing MLLMs still have a large room for improvement, but also reveals the potential directions for the subsequent model optimization. The data application manner and online leaderboards are released at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation.', 'authors': ['Rongrong Ji', 'Yunsheng Wu', 'Jinrui Yang', 'Xing Sun', 'Ke Li', 'Xiawu Zheng', 'Xu Lin', 'Mengdan Zhang', 'Yulei Qin', 'Yunhang Shen', 'Peixian Chen', 'Chaoyou Fu'], 'published': '2023-06-23', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'kosmos-2-grounding-multimodal-large-language', 'arxiv_id': '2306.14824', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2306.14824v3', 'url_pdf': 'https://arxiv.org/pdf/2306.14824v3.pdf', 'title': 'Kosmos-2: Grounding Multimodal Large Language Models to the World', 'abstract': \"We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. Specifically, we represent refer expressions as links in Markdown, i.e., ``[text span](bounding boxes)'', where object descriptions are sequences of location tokens. Together with multimodal corpora, we construct large-scale data of grounded image-text pairs (called GrIT) to train the model. In addition to the existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), Kosmos-2 integrates the grounding capability into downstream applications. We evaluate Kosmos-2 on a wide range of tasks, including (i) multimodal grounding, such as referring expression comprehension, and phrase grounding, (ii) multimodal referring, such as referring expression generation, (iii) perception-language tasks, and (iv) language understanding and generation. This work lays out the foundation for the development of Embodiment AI and sheds light on the big convergence of language, multimodal perception, action, and world modeling, which is a key step toward artificial general intelligence. Code and pretrained models are available at https://aka.ms/kosmos-2.\", 'authors': ['Furu Wei', 'Shuming Ma', 'Shaohan Huang', 'Yaru Hao', 'Li Dong', 'Wenhui Wang', 'Zhiliang Peng'], 'published': '2023-06-26', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'shikra-unleashing-multimodal-llm-s', 'arxiv_id': '2306.15195', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2306.15195v2', 'url_pdf': 'https://arxiv.org/pdf/2306.15195v2.pdf', 'title': \"Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic\", 'abstract': \"In human conversations, individuals can indicate relevant regions within a scene while addressing others. In turn, the other person can then respond by referring to specific regions if necessary. This natural referential ability in dialogue remains absent in current Multimodal Large Language Models (MLLMs). To fill this gap, this paper proposes an MLLM called Shikra, which can handle spatial coordinate inputs and outputs in natural language. Its architecture consists of a vision encoder, an alignment layer, and a LLM. It is designed to be straightforward and simple, without the need for extra vocabularies, position encoder, pre-/post-detection modules, or external plug-in models. All inputs and outputs are in natural language form. Referential dialogue is a superset of various vision-language (VL) tasks. Shikra can naturally handle location-related tasks like REC and PointQA, as well as conventional VL tasks such as Image Captioning and VQA. Experimental results showcase Shikra's promising performance. Furthermore, it enables numerous exciting applications, like providing mentioned objects' coordinates in chains of thoughts and comparing user-pointed regions similarities. Our code, model and dataset are accessed at https://github.com/shikras/shikra.\", 'authors': ['Rui Zhao', 'Feng Zhu', 'Richong Zhang', 'Weili Zeng', 'Zhao Zhang', 'Keqin Chen'], 'published': '2023-06-27', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'svit-scaling-up-visual-instruction-tuning', 'arxiv_id': '2307.04087', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2307.04087v3', 'url_pdf': 'https://arxiv.org/pdf/2307.04087v3.pdf', 'title': 'SVIT: Scaling up Visual Instruction Tuning', 'abstract': \"Thanks to the emerging of foundation models, the large language and vision models are integrated to acquire the multimodal ability of visual captioning, question answering, etc. Although existing multimodal models present impressive performance of visual understanding and reasoning, their limits are still largely under-explored due to the scarcity of high-quality instruction tuning data. To push the limits of multimodal capability, we Scale up Visual Instruction Tuning (SVIT) by constructing a dataset of 4.2 million visual instruction tuning data including 1.6M conversation question-answer (QA) pairs, 1.6M complex reasoning QA pairs, 1.0M referring QA pairs and 106K detailed image descriptions. Besides the volume, the proposed dataset is also featured by the high quality and rich diversity, which is generated by prompting GPT-4 with the abundant manual annotations of images. We also propose a new data recipe to select subset with better diversity and balance, which evokes model's superior capabilities. Extensive experiments verify that SVIT-v1.5, trained on the proposed dataset, outperforms state-of-the-art Multimodal Large Language Models on popular benchmarks. The data and code are publicly available at https://github.com/BAAI-DCAI/Visual-Instruction-Tuning.\", 'authors': ['Tiejun Huang', 'Muyang He', 'Boya Wu', 'Bo Zhao'], 'published': '2023-07-09', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'chatspot-bootstrapping-multimodal-llms-via', 'arxiv_id': '2307.09474', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2307.09474v1', 'url_pdf': 'https://arxiv.org/pdf/2307.09474v1.pdf', 'title': 'ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning', 'abstract': \"Human-AI interactivity is a critical aspect that reflects the usability of multimodal large language models (MLLMs). However, existing end-to-end MLLMs only allow users to interact with them through language instructions, leading to the limitation of the interactive accuracy and efficiency. In this study, we present precise referring instructions that utilize diverse reference representations such as points and boxes as referring prompts to refer to the special region. This enables MLLMs to focus on the region of interest and achieve finer-grained interaction. Based on precise referring instruction, we propose ChatSpot, a unified end-to-end multimodal large language model that supports diverse forms of interactivity including mouse clicks, drag-and-drop, and drawing boxes, which provides a more flexible and seamless interactive experience. We also construct a multi-grained vision-language instruction-following dataset based on existing datasets and GPT-4 generating. Furthermore, we design a series of evaluation tasks to assess the effectiveness of region recognition and interaction. Experimental results showcase ChatSpot's promising performance.\", 'authors': ['Xiangyu Zhang', 'Chunrui Han', 'Runpei Dong', 'Yuang Peng', 'Jianjian Sun', 'HongYu Zhou', 'Haoran Wei', 'Jinrong Yang', 'Zheng Ge', 'En Yu', 'Liang Zhao'], 'published': '2023-07-18', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'seed-bench-benchmarking-multimodal-llms-with', 'arxiv_id': '2307.16125', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2307.16125v2', 'url_pdf': 'https://arxiv.org/pdf/2307.16125v2.pdf', 'title': 'SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension', 'abstract': 'Based on powerful Large Language Models (LLMs), recent generative Multimodal Large Language Models (MLLMs) have gained prominence as a pivotal research area, exhibiting remarkable capability for both comprehension and generation. In this work, we address the evaluation of generative comprehension in MLLMs as a preliminary step towards a comprehensive assessment of generative models, by introducing a benchmark named SEED-Bench. SEED-Bench consists of 19K multiple choice questions with accurate human annotations (x 6 larger than existing benchmarks), which spans 12 evaluation dimensions including the comprehension of both the image and video modality. We develop an advanced pipeline for generating multiple-choice questions that target specific evaluation dimensions, integrating both automatic filtering and manual verification processes. Multiple-choice questions with groundtruth options derived from human annotation enables an objective and efficient assessment of model performance, eliminating the need for human or GPT intervention during evaluation. We further evaluate the performance of 18 models across all 12 dimensions, covering both the spatial and temporal understanding. By revealing the limitations of existing MLLMs through evaluation results, we aim for SEED-Bench to provide insights for motivating future research. We will launch and consistently maintain a leaderboard to provide a platform for the community to assess and investigate model capability.', 'authors': ['Ying Shan', 'Yixiao Ge', 'Yuying Ge', 'Guangzhi Wang', 'Rui Wang', 'Bohao Li'], 'published': '2023-07-30', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'lisa-reasoning-segmentation-via-large', 'arxiv_id': '2308.00692', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.00692v3', 'url_pdf': 'https://arxiv.org/pdf/2308.00692v3.pdf', 'title': 'LISA: Reasoning Segmentation via Large Language Model', 'abstract': 'Although perception systems have made remarkable advancements in recent years, they still rely on explicit human instruction or pre-defined categories to identify the target objects before executing visual recognition tasks. Such systems cannot actively reason and comprehend implicit user intention. In this work, we propose a new segmentation task -- reasoning segmentation. The task is designed to output a segmentation mask given a complex and implicit query text. Furthermore, we establish a benchmark comprising over one thousand image-instruction-mask data samples, incorporating intricate reasoning and world knowledge for evaluation purposes. Finally, we present LISA: large Language Instructed Segmentation Assistant, which inherits the language generation capabilities of multimodal Large Language Models (LLMs) while also possessing the ability to produce segmentation masks. We expand the original vocabulary with a <SEG> token and propose the embedding-as-mask paradigm to unlock the segmentation capability. Remarkably, LISA can handle cases involving complex reasoning and world knowledge. Also, it demonstrates robust zero-shot capability when trained exclusively on reasoning-free datasets. In addition, fine-tuning the model with merely 239 reasoning segmentation data samples results in further performance enhancement. Both quantitative and qualitative experiments show our method effectively unlocks new reasoning segmentation capabilities for multimodal LLMs. Code, models, and data are available at https://github.com/dvlab-research/LISA.', 'authors': ['Jiaya Jia', 'Shu Liu', 'Yuhui Yuan', 'Yanwei Li', 'Yukang Chen', 'Zhuotao Tian', 'Xin Lai'], 'published': '2023-08-01', 'conference': None, 'conference_url_abs': 'http://openaccess.thecvf.com//content/CVPR2024/html/Lai_LISA_Reasoning_Segmentation_via_Large_Language_Model_CVPR_2024_paper.html', 'conference_url_pdf': 'http://openaccess.thecvf.com//content/CVPR2024/papers/Lai_LISA_Reasoning_Segmentation_via_Large_Language_Model_CVPR_2024_paper.pdf', 'proceeding': 'cvpr-2024-1'}, {'id': 'empowering-vision-language-models-to-follow', 'arxiv_id': '2308.04152', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.04152v4', 'url_pdf': 'https://arxiv.org/pdf/2308.04152v4.pdf', 'title': 'Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions', 'abstract': \"Recent advancements in Multimodal Large Language Models (MLLMs) have been utilizing Visual Prompt Generators (VPGs) to convert visual features into tokens that LLMs can recognize. This is achieved by training the VPGs on millions of image-caption pairs, where the VPG-generated tokens of images are fed into a frozen LLM to generate the corresponding captions. However, this image-captioning based training objective inherently biases the VPG to concentrate solely on the primary visual contents sufficient for caption generation, often neglecting other visual details. This shortcoming results in MLLMs' underperformance in comprehending demonstrative instructions consisting of multiple, interleaved, and multimodal instructions that demonstrate the required context to complete a task. To address this issue, we introduce a generic and lightweight Visual Prompt Generator Complete module (VPG-C), which can infer and complete the missing details essential for comprehending demonstrative instructions. Further, we propose a synthetic discriminative training strategy to fine-tune VPG-C, eliminating the need for supervised demonstrative instructions. As for evaluation, we build DEMON, a comprehensive benchmark for demonstrative instruction understanding. Synthetically trained with the proposed strategy, VPG-C achieves significantly stronger zero-shot performance across all tasks of DEMON. Further evaluation on the MME and OwlEval benchmarks also demonstrate the superiority of VPG-C. Our benchmark, code, and pre-trained models are available at https://github.com/DCDmllm/Cheetah.\", 'authors': ['Wenqiao Zhang', 'Wei Ji', 'Yueting Zhuang', 'Siliang Tang', 'Tat-Seng Chua', 'Hanwang Zhang', 'Minghe Gao', 'Zhiqi Ge', 'Kaihang Pan', 'Juncheng Li'], 'published': '2023-08-08', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'link-context-learning-for-multimodal-llms', 'arxiv_id': '2308.07891', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.07891v1', 'url_pdf': 'https://arxiv.org/pdf/2308.07891v1.pdf', 'title': 'Link-Context Learning for Multimodal LLMs', 'abstract': 'The ability to learn from context with novel concepts, and deliver appropriate responses are essential in human conversations. Despite current Multimodal Large Language Models (MLLMs) and Large Language Models (LLMs) being trained on mega-scale datasets, recognizing unseen images or understanding novel concepts in a training-free manner remains a challenge. In-Context Learning (ICL) explores training-free few-shot learning, where models are encouraged to ``learn to learn\" from limited tasks and generalize to unseen tasks. In this work, we propose link-context learning (LCL), which emphasizes \"reasoning from cause and effect\" to augment the learning capabilities of MLLMs. LCL goes beyond traditional ICL by explicitly strengthening the causal relationship between the support set and the query set. By providing demonstrations with causal links, LCL guides the model to discern not only the analogy but also the underlying causal associations between data points, which empowers MLLMs to recognize unseen images and understand novel concepts more effectively. To facilitate the evaluation of this novel approach, we introduce the ISEKAI dataset, comprising exclusively of unseen generated image-label pairs designed for link-context learning. Extensive experiments show that our LCL-MLLM exhibits strong link-context learning capabilities to novel concepts over vanilla MLLMs. Code and data will be released at https://github.com/isekai-portal/Link-Context-Learning.', 'authors': ['Ziwei Liu', 'Rui Zhao', 'Feng Zhu', 'Zhao Zhang', 'Weichen Fan', 'Yan Tai'], 'published': '2023-08-15', 'conference': None, 'conference_url_abs': 'http://openaccess.thecvf.com//content/CVPR2024/html/Tai_Link-Context_Learning_for_Multimodal_LLMs_CVPR_2024_paper.html', 'conference_url_pdf': 'http://openaccess.thecvf.com//content/CVPR2024/papers/Tai_Link-Context_Learning_for_Multimodal_LLMs_CVPR_2024_paper.pdf', 'proceeding': 'cvpr-2024-1'}, {'id': 'fashionlogo-prompting-multimodal-large', 'arxiv_id': '2308.09012', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.09012v2', 'url_pdf': 'https://arxiv.org/pdf/2308.09012v2.pdf', 'title': 'FashionLOGO: Prompting Multimodal Large Language Models for Fashion Logo Embeddings', 'abstract': \"Logo embedding models convert the product logos in images into vectors, enabling their utilization for logo recognition and detection within e-commerce platforms. This facilitates the enforcement of intellectual property rights and enhances product search capabilities. However, current methods treat logo embedding as a purely visual problem. A noteworthy issue is that visual models capture features more than logos. Instead, we view this as a multimodal task, using text as auxiliary information to facilitate the visual model's understanding of the logo. The emerging Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in both visual and textual understanding. Inspired by this, we propose an approach, \\\\textbf{FashionLOGO}, to explore how to prompt MLLMs to generate appropriate text for product images, which can help visual models achieve better logo embeddings. We adopt a cross-attention transformer block that enables visual embedding to automatically learn supplementary knowledge from textual embedding. Our extensive experiments on real-world datasets prove that FashionLOGO is capable of generating generic and robust logo embeddings, achieving state-of-the-art performance in all benchmarks.\", 'authors': ['Walton Wang', 'Minghui Qiu', 'Min Yang', 'Yulin Su', 'Da Li', 'Zhen Wang'], 'published': '2023-08-17', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'wanjuan-a-comprehensive-multimodal-dataset', 'arxiv_id': '2308.10755', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.10755v3', 'url_pdf': 'https://arxiv.org/pdf/2308.10755v3.pdf', 'title': 'WanJuan: A Comprehensive Multimodal Dataset for Advancing English and Chinese Large Models', 'abstract': 'The rise in popularity of ChatGPT and GPT-4 has significantly accelerated the development of large models, leading to the creation of numerous impressive large language models(LLMs) and multimodal large language models (MLLMs). These cutting-edge models owe their remarkable performance to high-quality data. However, the details of the training data used in leading paradigms are often kept confidential. This lack of transparency, coupled with the scarcity of open-source data, impedes further developments within the community. As a response, this paper presents \"Wan Juan\", a large-scale multimodal dataset composed of both Chinese and English data, collected from a wide range of web sources. The dataset incorporates text, image-text, and video modalities, with a total volume exceeding 2TB. It was utilized in the training of InternLM, a model that demonstrated significant advantages in multi-dimensional evaluations when compared to models of a similar scale. All data can be accessed at https://opendatalab.org.cn/WanJuan1.0.', 'authors': ['Dahua Lin', 'Jiaqi Wang', 'Hang Yan', 'Wei Li', 'Bin Wang', 'Jiantao Qiu', 'Chao Xu', 'Zhenjiang Jin', 'Conghui He'], 'published': '2023-08-21', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'stablellava-enhanced-visual-instruction', 'arxiv_id': '2308.10253', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.10253v2', 'url_pdf': 'https://arxiv.org/pdf/2308.10253v2.pdf', 'title': 'StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data', 'abstract': \"The remarkable multimodal capabilities demonstrated by OpenAI's GPT-4 have sparked significant interest in the development of multimodal Large Language Models (LLMs). A primary research objective of such models is to align visual and textual modalities effectively while comprehending human instructions. Current methodologies often rely on annotations derived from benchmark datasets to construct image-dialogue datasets for training purposes, akin to instruction tuning in LLMs. However, these datasets often exhibit domain bias, potentially constraining the generative capabilities of the models. In an effort to mitigate these limitations, we propose a novel data collection methodology that synchronously synthesizes images and dialogues for visual instruction tuning. This approach harnesses the power of generative models, marrying the abilities of ChatGPT and text-to-image generative models to yield a diverse and controllable dataset with varied image content. Additionally, datasets can be arbitrarily scaled. This not only provides greater flexibility compared to existing methodologies but also significantly enhances several model capabilities. Our research includes comprehensive experiments conducted on various datasets. The results emphasize substantial enhancements in more than ten commonly assessed capabilities. Additionally, our model achieves state-of-the-art results across multiple widely recognized multimodal benchmarks.\", 'authors': ['Yunchao Wei', 'Ling Chen', 'Chunhua Shen', 'Guosheng Lin', 'Bin Fu', 'Zhibin Wang', 'Gang Yu', 'Chi Zhang', 'Yanda Li'], 'published': '2023-08-20', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'instructiongpt-4-a-200-instruction-paradigm', 'arxiv_id': '2308.12067', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.12067v2', 'url_pdf': 'https://arxiv.org/pdf/2308.12067v2.pdf', 'title': 'InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4', 'abstract': 'Multimodal large language models are typically trained in two stages: first pre-training on image-text pairs, and then fine-tuning using supervised vision-language instruction data. Recent studies have shown that large language models can achieve satisfactory results even with a limited amount of high-quality instruction-following data. In this paper, we introduce InstructionGPT-4, which is fine-tuned on a small dataset comprising only 200 examples, amounting to approximately 6\\\\% of the instruction-following data used in the alignment dataset for MiniGPT-4. To achieve this, we first propose several metrics to access the quality of multimodal instruction data. Based on these metrics, we present an effective and trainable data selector to automatically identify and filter low-quality vision-language data. By employing this method, InstructionGPT-4 outperforms the original MiniGPT-4 on various evaluations. Overall, our findings demonstrate that less but high-quality instruction tuning data is efficient in enabling multimodal large language models to generate better output. Our code is available at https://github.com/waltonfuture/InstructionGPT-4.', 'authors': ['Lichao Sun', 'Weiran Huang', 'Zihao Jiang', 'Lai Wei'], 'published': '2023-08-23', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'vigc-visual-instruction-generation-and', 'arxiv_id': '2308.12714', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.12714v3', 'url_pdf': 'https://arxiv.org/pdf/2308.12714v3.pdf', 'title': 'VIGC: Visual Instruction Generation and Correction', 'abstract': \"The integration of visual encoders and large language models (LLMs) has driven recent progress in multimodal large language models (MLLMs). However, the scarcity of high-quality instruction-tuning data for vision-language tasks remains a challenge. The current leading paradigm, such as LLaVA, relies on language-only GPT-4 to generate data, which requires pre-annotated image captions and detection bounding boxes, suffering from understanding image details. A practical solution to this problem would be to utilize the available multimodal large language models (MLLMs) to generate instruction data for vision-language tasks. However, it's worth noting that the currently accessible MLLMs are not as powerful as their LLM counterparts, as they tend to produce inadequate responses and generate false information. As a solution for addressing the current issue, this paper proposes the Visual Instruction Generation and Correction (VIGC) framework that enables multimodal large language models to generate instruction-tuning data and progressively enhance its quality on-the-fly. Specifically, Visual Instruction Generation (VIG) guides the vision-language model to generate diverse instruction-tuning data. To ensure generation quality, Visual Instruction Correction (VIC) adopts an iterative update mechanism to correct any inaccuracies in data produced by VIG, effectively reducing the risk of hallucination. Leveraging the diverse, high-quality data generated by VIGC, we finetune mainstream models and validate data quality based on various evaluations. Experimental results demonstrate that VIGC not only compensates for the shortcomings of language-only data generation methods, but also effectively enhances the benchmark performance. The models, datasets, and code are available at https://opendatalab.github.io/VIGC.\", 'authors': ['Conghui He', 'Jiaqi Wang', 'Wei Li', 'Weijia Li', 'Xiaoyi Dong', 'Pan Zhang', 'Huaping Zhong', 'Jiahui Peng', 'Xiao Han', 'Fan Wu', 'Bin Wang'], 'published': '2023-08-24', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'position-enhanced-visual-instruction-tuning', 'arxiv_id': '2308.13437', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.13437v2', 'url_pdf': 'https://arxiv.org/pdf/2308.13437v2.pdf', 'title': 'Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models', 'abstract': 'Recently, Multimodal Large Language Models (MLLMs) that enable Large Language Models (LLMs) to interpret images through visual instruction tuning have achieved significant success. However, existing visual instruction tuning methods only utilize image-language instruction data to align the language and image modalities, lacking a more fine-grained cross-modal alignment. In this paper, we propose Position-enhanced Visual Instruction Tuning (PVIT), which extends the functionality of MLLMs by integrating an additional region-level vision encoder. This integration promotes a more detailed comprehension of images for the MLLM. In addition, to efficiently achieve a fine-grained alignment between the vision modules and the LLM, we design multiple data generation strategies to construct an image-region-language instruction dataset. Finally, we present both quantitative experiments and qualitative analysis that demonstrate the superiority of the proposed model. Code and data will be released at https://github.com/PVIT-official/PVIT.', 'authors': ['Yang Liu', 'Maosong Sun', 'Peng Li', 'Xiaoyue Mi', 'Fuwen Luo', 'Ruoyu Qin', 'Chi Chen'], 'published': '2023-08-25', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'mllm-dataengine-an-iterative-refinement', 'arxiv_id': '2308.13566', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.13566v2', 'url_pdf': 'https://arxiv.org/pdf/2308.13566v2.pdf', 'title': 'MLLM-DataEngine: An Iterative Refinement Approach for MLLM', 'abstract': 'Despite the great advance of Multimodal Large Language Models (MLLMs) in both instruction dataset building and benchmarking, the independence of training and evaluation makes current MLLMs hard to further improve their capability under the guidance of evaluation results with a relatively low human cost. In this paper, we propose MLLM-DataEngine, a novel closed-loop system that bridges data generation, model training, and evaluation. Within each loop iteration, the MLLM-DataEngine first analyze the weakness of the model based on the evaluation results, then generate a proper incremental dataset for the next training iteration and enhance the model capability iteratively. Compared with previous data collection methods which are separate from the benchmarking, the data generated by MLLM-DataEngine shows better targeting, quality, and correctness. For targeting, we propose an Adaptive Bad-case Sampling module, which adjusts the ratio of different types of data within each incremental dataset based on the benchmarking results. For quality, we resort to GPT-4 to generate high-quality data with each given data type. For correctness, prompt design is critical for the data generation results. Rather than previous hand-crafted prompt, we propose an Interactive Prompt Optimization strategy, which optimizes the prompt with the multi-round interaction between human and GPT, and improve the correctness of generated data greatly. Through extensive experiments, we find our MLLM-DataEngine could boost the MLLM capability in a targeted and automatic manner, with only a few human participation. We hope it could be a general solution for the following MLLMs building. The MLLM-DataEngine has been open-sourced and is now available at https://github.com/opendatalab/MLLM-DataEngine.', 'authors': ['Conghui He', 'Jiaqi Wang', 'Xiaoyi Dong', 'Pan Zhang', 'Siyuan Huang', 'Bin Wang', 'Linke Ouyang', 'Zhiyuan Zhao'], 'published': '2023-08-25', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'socratis-are-large-multimodal-models', 'arxiv_id': '2308.16741', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.16741v3', 'url_pdf': 'https://arxiv.org/pdf/2308.16741v3.pdf', 'title': 'Socratis: Are large multimodal models emotionally aware?', 'abstract': 'Existing emotion prediction benchmarks contain coarse emotion labels which do not consider the diversity of emotions that an image and text can elicit in humans due to various reasons. Learning diverse reactions to multimodal content is important as intelligent machines take a central role in generating and delivering content to society. To address this gap, we propose Socratis, a societal reactions benchmark, where each image-caption (IC) pair is annotated with multiple emotions and the reasons for feeling them. Socratis contains 18K free-form reactions for 980 emotions on 2075 image-caption pairs from 5 widely-read news and image-caption (IC) datasets. We benchmark the capability of state-of-the-art multimodal large language models to generate the reasons for feeling an emotion given an IC pair. Based on a preliminary human study, we observe that humans prefer human-written reasons over 2 times more often than machine-generated ones. This shows our task is harder than standard generation tasks because it starkly contrasts recent findings where humans cannot tell apart machine vs human-written news articles, for instance. We further see that current captioning metrics based on large vision-language models also fail to correlate with human preferences. We hope that these findings and our benchmark will inspire further research on training emotionally aware models.', 'authors': ['Kate Saenko', 'Bryan A. Plummer', 'Saadia Gabriel', 'Reuben Tan', 'Arijit Ray', 'Katherine Deng'], 'published': '2023-08-31', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'multiway-adapater-adapting-large-scale-multi', 'arxiv_id': '2309.01516', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.01516v3', 'url_pdf': 'https://arxiv.org/pdf/2309.01516v3.pdf', 'title': 'MultiWay-Adapater: Adapting large-scale multi-modal models for scalable image-text retrieval', 'abstract': \"As Multimodal Large Language Models (MLLMs) grow in size, adapting them to specialized tasks becomes increasingly challenging due to high computational and memory demands. Indeed, traditional fine-tuning methods are costly, due to the need for extensive, task-specific training. While efficient adaptation methods exist that aim to reduce these costs, in practice they suffer from shallow inter-modal alignment, which severely hurts model effectiveness. To tackle these computational challenges and improve inter-modal alignment, we introduce the MultiWay-Adapter (MWA), a novel framework featuring an 'Alignment Enhancer'. This enhancer deepens inter-modal alignment, enabling high transferability with minimal tuning effort. Our experiments show that unlike prior efficient tuning approaches, MWA maintains model effectiveness, while reducing training time by up-to 57%. MWA is also lightweight, increasing model size by only 2-3% (in terms of parameters) for state-of-the-art foundation models like BEiT-3 Large. These results demonstrate that MWA provides an efficient and effective adaptation method for MLLMs, significantly broadening their applicability.\", 'authors': ['Gerardo Aragon Camarasa', 'Richard McCreadie', 'George Killick', 'Zijun Long'], 'published': '2023-09-04', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'next-gpt-any-to-any-multimodal-llm', 'arxiv_id': '2309.05519', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.05519v3', 'url_pdf': 'https://arxiv.org/pdf/2309.05519v3.pdf', 'title': 'NExT-GPT: Any-to-Any Multimodal LLM', 'abstract': 'While recently Multimodal Large Language Models (MM-LLMs) have made exciting strides, they mostly fall prey to the limitation of only input-side multimodal understanding, without the ability to produce content in multiple modalities. As we humans always perceive the world and communicate with people through various modalities, developing any-to-any MM-LLMs capable of accepting and delivering content in any modality becomes essential to human-level AI. To fill the gap, we present an end-to-end general-purpose any-to-any MM-LLM system, NExT-GPT. We connect an LLM with multimodal adaptors and different diffusion decoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio. By leveraging the existing well-trained highly-performing encoders and decoders, NExT-GPT is tuned with only a small amount of parameter (1%) of certain projection layers, which not only benefits low-cost training and also facilitates convenient expansion to more potential modalities. Moreover, we introduce a modality-switching instruction tuning (MosIT) and manually curate a high-quality dataset for MosIT, based on which NExT-GPT is empowered with complex cross-modal semantic understanding and content generation. Overall, our research showcases the promising possibility of building an AI agent capable of modeling universal modalities, paving the way for more human-like AI research in the community. Project page: https://next-gpt.github.io/', 'authors': ['Tat-Seng Chua', 'Wei Ji', 'Leigang Qu', 'Hao Fei', 'Shengqiong Wu'], 'published': '2023-09-11', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'hilm-d-towards-high-resolution-understanding', 'arxiv_id': '2309.05186', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.05186v1', 'url_pdf': 'https://arxiv.org/pdf/2309.05186v1.pdf', 'title': 'HiLM-D: Towards High-Resolution Understanding in Multimodal Large Language Models for Autonomous Driving', 'abstract': \"Autonomous driving systems generally employ separate models for different tasks resulting in intricate designs. For the first time, we leverage singular multimodal large language models (MLLMs) to consolidate multiple autonomous driving tasks from videos, i.e., the Risk Object Localization and Intention and Suggestion Prediction (ROLISP) task. ROLISP uses natural language to simultaneously identify and interpret risk objects, understand ego-vehicle intentions, and provide motion suggestions, eliminating the necessity for task-specific architectures. However, lacking high-resolution (HR) information, existing MLLMs often miss small objects (e.g., traffic cones) and overly focus on salient ones (e.g., large trucks) when applied to ROLISP. We propose HiLM-D (Towards High-Resolution Understanding in MLLMs for Autonomous Driving), an efficient method to incorporate HR information into MLLMs for the ROLISP task. Especially, HiLM-D integrates two branches: (i) the low-resolution reasoning branch, can be any MLLMs, processes low-resolution videos to caption risk objects and discern ego-vehicle intentions/suggestions; (ii) the high-resolution perception branch (HR-PB), prominent to HiLM-D,, ingests HR images to enhance detection by capturing vision-specific HR feature maps and prioritizing all potential risks over merely salient objects. Our HR-PB serves as a plug-and-play module, seamlessly fitting into current MLLMs. Experiments on the ROLISP benchmark reveal HiLM-D's notable advantage over leading MLLMs, with improvements of 4.8% in BLEU-4 for captioning and 17.2% in mIoU for detection.\", 'authors': ['Xiaomeng Li', 'Wei zhang', 'Hang Xu', 'Jianhua Han', 'Xinpeng Ding'], 'published': '2023-09-11', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'investigating-the-catastrophic-forgetting-in', 'arxiv_id': '2309.10313', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.10313v4', 'url_pdf': 'https://arxiv.org/pdf/2309.10313v4.pdf', 'title': 'Investigating the Catastrophic Forgetting in Multimodal Large Language Models', 'abstract': 'Following the success of GPT4, there has been a surge in interest in multimodal large language model (MLLM) research. This line of research focuses on developing general-purpose LLMs through fine-tuning pre-trained LLMs and vision models. However, catastrophic forgetting, a notorious phenomenon where the fine-tuned model fails to retain similar performance compared to the pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM). In this paper, we introduce EMT: Evaluating MulTimodality for evaluating the catastrophic forgetting in MLLMs, by treating each MLLM as an image classifier. We first apply EMT to evaluate several open-source fine-tuned MLLMs and we discover that almost all evaluated MLLMs fail to retain the same performance levels as their vision encoders on standard image classification tasks. Moreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess performance throughout the fine-tuning. Interestingly, our results suggest that early-stage fine-tuning on an image dataset improves performance across other image datasets, by enhancing the alignment of text and visual features. However, as fine-tuning proceeds, the MLLMs begin to hallucinate, resulting in a significant loss of generalizability, even when the image encoder remains frozen. Our results suggest that MLLMs have yet to demonstrate performance on par with their vision models on standard image classification tasks and the current MLLM fine-tuning procedure still has room for improvement.', 'authors': ['Yi Ma', 'Yong Jae Lee', 'Qing Qu', 'Mu Cai', 'Xiao Li', 'Shengbang Tong', 'Yuexiang Zhai'], 'published': '2023-09-19', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'dreamllm-synergistic-multimodal-comprehension', 'arxiv_id': '2309.11499', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.11499v2', 'url_pdf': 'https://arxiv.org/pdf/2309.11499v2.pdf', 'title': 'DreamLLM: Synergistic Multimodal Comprehension and Creation', 'abstract': \"This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy. Project page: https://dreamllm.github.io.\", 'authors': ['Li Yi', 'Kaisheng Ma', 'Xiangyu Zhang', 'Xiangwen Kong', 'Haoran Wei', 'HongYu Zhou', 'Jianjian Sun', 'Liang Zhao', 'Jinrong Yang', 'Zheng Ge', 'Zekun Qi', 'Yuang Peng', 'Chunrui Han', 'Runpei Dong'], 'published': '2023-09-20', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'how-robust-is-google-s-bard-to-adversarial', 'arxiv_id': '2309.11751', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.11751v2', 'url_pdf': 'https://arxiv.org/pdf/2309.11751v2.pdf', 'title': \"How Robust is Google's Bard to Adversarial Image Attacks?\", 'abstract': \"Multimodal Large Language Models (MLLMs) that integrate text and other modalities (especially vision) have achieved unprecedented performance in various multimodal tasks. However, due to the unsolved adversarial robustness problem of vision models, MLLMs can have more severe safety and security risks by introducing the vision inputs. In this work, we study the adversarial robustness of Google's Bard, a competitive chatbot to ChatGPT that released its multimodal capability recently, to better understand the vulnerabilities of commercial MLLMs. By attacking white-box surrogate vision encoders or MLLMs, the generated adversarial examples can mislead Bard to output wrong image descriptions with a 22% success rate based solely on the transferability. We show that the adversarial examples can also attack other MLLMs, e.g., a 26% attack success rate against Bing Chat and a 86% attack success rate against ERNIE bot. Moreover, we identify two defense mechanisms of Bard, including face detection and toxicity detection of images. We design corresponding attacks to evade these defenses, demonstrating that the current defenses of Bard are also vulnerable. We hope this work can deepen our understanding on the robustness of MLLMs and facilitate future research on defenses. Our code is available at https://github.com/thu-ml/Attack-Bard. Update: GPT-4V is available at October 2023. We further evaluate its robustness under the same set of adversarial examples, achieving a 45% attack success rate.\", 'authors': ['Jun Zhu', 'Hang Su', 'Yu Tian', 'Yichi Zhang', 'Xiao Yang', 'Zhengwei Fang', 'Jiawei Chen', 'Huanran Chen', 'Yinpeng Dong'], 'published': '2023-09-21', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'vdc-versatile-data-cleanser-for-detecting', 'arxiv_id': '2309.16211', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.16211v2', 'url_pdf': 'https://arxiv.org/pdf/2309.16211v2.pdf', 'title': 'VDC: Versatile Data Cleanser based on Visual-Linguistic Inconsistency by Multimodal Large Language Models', 'abstract': 'The role of data in building AI systems has recently been emphasized by the emerging concept of data-centric AI. Unfortunately, in the real-world, datasets may contain dirty samples, such as poisoned samples from backdoor attack, noisy labels in crowdsourcing, and even hybrids of them. The presence of such dirty samples makes the DNNs vunerable and unreliable.Hence, it is critical to detect dirty samples to improve the quality and realiability of dataset. Existing detectors only focus on detecting poisoned samples or noisy labels, that are often prone to weak generalization when dealing with dirty samples from other domains.In this paper, we find a commonality of various dirty samples is visual-linguistic inconsistency between images and associated labels. To capture the semantic inconsistency between modalities, we propose versatile data cleanser (VDC) leveraging the surpassing capabilities of multimodal large language models (MLLM) in cross-modal alignment and reasoning.It consists of three consecutive modules: the visual question generation module to generate insightful questions about the image; the visual question answering module to acquire the semantics of the visual content by answering the questions with MLLM; followed by the visual answer evaluation module to evaluate the inconsistency.Extensive experiments demonstrate its superior performance and generalization to various categories and types of dirty samples. The code is available at \\\\url{https://github.com/zihao-ai/vdc}.', 'authors': ['Baoyuan Wu', 'Bingzhe Wu', 'Shaokui Wei', 'Mingda Zhang', 'Zihao Zhu'], 'published': '2023-09-28', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'guiding-instruction-based-image-editing-via', 'arxiv_id': '2309.17102', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.17102v2', 'url_pdf': 'https://arxiv.org/pdf/2309.17102v2.pdf', 'title': 'Guiding Instruction-based Image Editing via Multimodal Large Language Models', 'abstract': 'Instruction-based image editing improves the controllability and flexibility of image manipulation via natural commands without elaborate descriptions or regional masks. However, human instructions are sometimes too brief for current methods to capture and follow. Multimodal large language models (MLLMs) show promising capabilities in cross-modal understanding and visual-aware response generation via LMs. We investigate how MLLMs facilitate edit instructions and present MLLM-Guided Image Editing (MGIE). MGIE learns to derive expressive instructions and provides explicit guidance. The editing model jointly captures this visual imagination and performs manipulation through end-to-end training. We evaluate various aspects of Photoshop-style modification, global photo optimization, and local editing. Extensive experimental results demonstrate that expressive instructions are crucial to instruction-based image editing, and our MGIE can lead to a notable improvement in automatic metrics and human evaluation while maintaining competitive inference efficiency.', 'authors': ['Zhe Gan', 'Yinfei Yang', 'William Yang Wang', 'Xianzhi Du', 'Wenze Hu', 'Tsu-Jui Fu'], 'published': '2023-09-29', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'reformulating-vision-language-foundation', 'arxiv_id': '2310.00653', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.00653v1', 'url_pdf': 'https://arxiv.org/pdf/2310.00653v1.pdf', 'title': 'Reformulating Vision-Language Foundation Models and Datasets Towards Universal Multimodal Assistants', 'abstract': \"Recent Multimodal Large Language Models (MLLMs) exhibit impressive abilities to perceive images and follow open-ended instructions. The capabilities of MLLMs depend on two crucial factors: the model architecture to facilitate the feature alignment of visual modules and large language models; the multimodal instruction tuning datasets for human instruction following. (i) For the model architecture, most existing models introduce an external bridge module to connect vision encoders with language models, which needs an additional feature-alignment pre-training. In this work, we discover that compact pre-trained vision language models can inherently serve as ``out-of-the-box'' bridges between vision and language. Based on this, we propose Muffin framework, which directly employs pre-trained vision-language models to act as providers of visual signals. (ii) For the multimodal instruction tuning datasets, existing methods omit the complementary relationship between different datasets and simply mix datasets from different tasks. Instead, we propose UniMM-Chat dataset which explores the complementarities of datasets to generate 1.1M high-quality and diverse multimodal instructions. We merge information describing the same image from diverse datasets and transforms it into more knowledge-intensive conversation data. Experimental results demonstrate the effectiveness of the Muffin framework and UniMM-Chat dataset. Muffin achieves state-of-the-art performance on a wide range of vision-language tasks, significantly surpassing state-of-the-art models like LLaVA and InstructBLIP. Our model and dataset are all accessible at https://github.com/thunlp/muffin.\", 'authors': ['Maosong Sun', 'Hai-Tao Zheng', 'Zhiyuan Liu', 'Dahai Li', 'Jiao Xue', 'Yinxv Pan', 'Shan Wang', 'Chongyi Wang', 'Yue Zhao', 'Haoye Zhang', 'Yuan YAO', 'Jinyi Hu', 'Tianyu Yu'], 'published': '2023-10-01', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'drivegpt4-interpretable-end-to-end-autonomous', 'arxiv_id': '2310.01412', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.01412v5', 'url_pdf': 'https://arxiv.org/pdf/2310.01412v5.pdf', 'title': 'DriveGPT4: Interpretable End-to-end Autonomous Driving via Large Language Model', 'abstract': 'Multimodal large language models (MLLMs) have emerged as a prominent area of interest within the research community, given their proficiency in handling and reasoning with non-textual data, including images and videos. This study seeks to extend the application of MLLMs to the realm of autonomous driving by introducing DriveGPT4, a novel interpretable end-to-end autonomous driving system based on LLMs. Capable of processing multi-frame video inputs and textual queries, DriveGPT4 facilitates the interpretation of vehicle actions, offers pertinent reasoning, and effectively addresses a diverse range of questions posed by users. Furthermore, DriveGPT4 predicts low-level vehicle control signals in an end-to-end fashion.These advanced capabilities are achieved through the utilization of a bespoke visual instruction tuning dataset, specifically tailored for autonomous driving applications, in conjunction with a mix-finetuning training strategy. DriveGPT4 represents the pioneering effort to leverage LLMs for the development of an interpretable end-to-end autonomous driving solution. Evaluations conducted on the BDD-X dataset showcase the superior qualitative and quantitative performance of DriveGPT4. Additionally, the fine-tuning of domain-specific data enables DriveGPT4 to yield close or even improved results in terms of autonomous driving grounding when contrasted with GPT4-V.', 'authors': ['Kwan-Yee. K. Wong', 'Hengshuang Zhao', 'Zhenguo Li', 'Yong Guo', 'Zhen Zhao', 'Enze Xie', 'Yujia Zhang', 'Zhenhua Xu'], 'published': '2023-10-02', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'minigpt-5-interleaved-vision-and-language', 'arxiv_id': '2310.02239', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.02239v3', 'url_pdf': 'https://arxiv.org/pdf/2310.02239v3.pdf', 'title': 'MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens', 'abstract': 'The effectiveness of Multimodal Large Language Models (MLLMs) demonstrates a profound capability in multimodal understanding. However, the simultaneous generation of images with coherent texts is still underdeveloped. Addressing this, we introduce a novel interleaved vision-and-language generation method, centered around the concept of ``generative vokens\". These vokens serve as pivotal elements contributing to coherent image-text outputs. Our method is marked by a unique two-stage training strategy for description-free multimodal generation, which does not necessitate extensive descriptions of images. We integrate classifier-free guidance to enhance the alignment of generated images and texts, ensuring more seamless and contextually relevant multimodal interactions. Our model, MiniGPT-5, exhibits substantial improvement over the baseline models on multimodal generation datasets, including MMDialog and VIST. The human evaluation shows MiniGPT-5 is better than the baseline model on more than 56\\\\% cases for multimodal generation, highlighting its efficacy across diverse benchmarks.', 'authors': ['Xin Eric Wang', 'Xuehai He', 'Kaizhi Zheng'], 'published': '2023-10-03', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'towards-end-to-end-embodied-decision-making', 'arxiv_id': '2310.02071', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.02071v4', 'url_pdf': 'https://arxiv.org/pdf/2310.02071v4.pdf', 'title': 'Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond', 'abstract': 'In this study, we explore the potential of Multimodal Large Language Models (MLLMs) in improving embodied decision-making processes for agents. While Large Language Models (LLMs) have been widely used due to their advanced reasoning skills and vast world knowledge, MLLMs like GPT4-Vision offer enhanced visual understanding and reasoning capabilities. We investigate whether state-of-the-art MLLMs can handle embodied decision-making in an end-to-end manner and whether collaborations between LLMs and MLLMs can enhance decision-making. To address these questions, we introduce a new benchmark called PCA-EVAL, which evaluates embodied decision-making from the perspectives of Perception, Cognition, and Action. Additionally, we propose HOLMES, a multi-agent cooperation framework that allows LLMs to leverage MLLMs and APIs to gather multimodal information for informed decision-making. We compare end-to-end embodied decision-making and HOLMES on our benchmark and find that the GPT4-Vision model demonstrates strong end-to-end embodied decision-making abilities, outperforming GPT4-HOLMES in terms of average decision accuracy (+3%). However, this performance is exclusive to the latest GPT4-Vision model, surpassing the open-source state-of-the-art MLLM by 26%. Our results indicate that powerful MLLMs like GPT4-Vision hold promise for decision-making in embodied agents, offering new avenues for MLLM research. Code and data are open at https://github.com/pkunlp-icler/PCA-EVAL/.', 'authors': ['Peiyi Wang', 'Baobao Chang', 'Tianyu Liu', 'Yuchi Wang', 'Zefan Cai', 'Haozhe Zhao', 'Shuhuai Ren', 'Yichi Zhang', 'Liang Chen'], 'published': '2023-10-03', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'kosmos-g-generating-images-in-context-with', 'arxiv_id': '2310.02992', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.02992v3', 'url_pdf': 'https://arxiv.org/pdf/2310.02992v3.pdf', 'title': 'Kosmos-G: Generating Images in Context with Multimodal Large Language Models', 'abstract': 'Recent advancements in subject-driven image generation have made significant strides. However, current methods still fall short in diverse application scenarios, as they require test-time tuning and cannot accept interleaved multi-image and text input. These limitations keep them far from the ultimate goal of \"image as a foreign language in image generation.\" This paper presents Kosmos-G, a model that leverages the advanced multimodal perception capabilities of Multimodal Large Language Models (MLLMs) to tackle the aforementioned challenge. Our approach aligns the output space of MLLM with CLIP using the textual modality as an anchor and performs compositional instruction tuning on curated data. Kosmos-G demonstrates an impressive capability of zero-shot subject-driven generation with interleaved multi-image and text input. Notably, the score distillation instruction tuning requires no modifications to the image decoder. This allows for a seamless substitution of CLIP and effortless integration with a myriad of U-Net techniques ranging from fine-grained controls to personalized image decoder variants. We posit Kosmos-G as an initial attempt towards the goal of \"image as a foreign language in image generation.\" The code can be found at https://aka.ms/Kosmos-G', 'authors': ['Furu Wei', 'Wenhu Chen', 'Zhiliang Peng', 'Shaohan Huang', 'Li Dong', 'Xichen Pan'], 'published': '2023-10-04', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'fine-grained-audio-visual-joint', 'arxiv_id': '2310.05863', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.05863v2', 'url_pdf': 'https://arxiv.org/pdf/2310.05863v2.pdf', 'title': 'Fine-grained Audio-Visual Joint Representations for Multimodal Large Language Models', 'abstract': 'Audio-visual large language models (LLM) have drawn significant attention, yet the fine-grained combination of both input streams is rather under-explored, which is challenging but necessary for LLMs to understand general video inputs. To this end, a fine-grained audio-visual joint representation (FAVOR) learning framework for multimodal LLMs is proposed in this paper, which extends a text-based LLM to simultaneously perceive speech and audio events in the audio input stream and images or videos in the visual input stream, at the frame level. To fuse the audio and visual feature streams into joint representations and to align the joint space with the LLM input embedding space, we propose a causal Q-Former structure with a causal attention module to enhance the capture of causal relations of the audio-visual frames across time. An audio-visual evaluation benchmark (AVEB) is also proposed which comprises six representative single-modal tasks with five cross-modal tasks reflecting audio-visual co-reasoning abilities. While achieving competitive single-modal performance on audio, speech and image tasks in AVEB, FAVOR achieved over 20% accuracy improvements on the video question-answering task when fine-grained information or temporal causal reasoning is required. FAVOR, in addition, demonstrated remarkable video comprehension and reasoning abilities on tasks that are unprecedented by other multimodal LLMs. An interactive demo of FAVOR is available at https://github.com/BriansIDP/AudioVisualLLM.git, and the training code and model checkpoints will be released soon.', 'authors': ['Chao Zhang', 'Zejun Ma', 'Lu Lu', 'Wei Li', 'Tian Tan', 'Xianzhao Chen', 'Changli Tang', 'Wenyi Yu', 'Guangzhi Sun'], 'published': '2023-10-09', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'can-we-edit-multimodal-large-language-models', 'arxiv_id': '2310.08475', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.08475v5', 'url_pdf': 'https://arxiv.org/pdf/2310.08475v5.pdf', 'title': 'Can We Edit Multimodal Large Language Models?', 'abstract': 'In this paper, we focus on editing Multimodal Large Language Models (MLLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights. Code and dataset are available in https://github.com/zjunlp/EasyEdit.', 'authors': ['Ningyu Zhang', 'Huajun Chen', 'Yongheng Wang', 'Xi Chen', 'Qingbin Liu', 'Bozhong Tian', 'Siyuan Cheng'], 'published': '2023-10-12', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'autorepo-a-general-framework-for-multi-modal', 'arxiv_id': '2310.07944', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.07944v2', 'url_pdf': 'https://arxiv.org/pdf/2310.07944v2.pdf', 'title': 'AutoRepo: A general framework for multi-modal LLM-based automated construction reporting', 'abstract': 'Ensuring the safety, quality, and timely completion of construction projects is paramount, with construction inspections serving as a vital instrument towards these goals. Nevertheless, the predominantly manual approach of present-day inspections frequently results in inefficiencies and inadequate information management. Such methods often fall short of providing holistic, exhaustive assessments, consequently engendering regulatory oversights and potential safety hazards. To address this issue, this paper presents a novel framework named AutoRepo for automated generation of construction inspection reports. The unmanned vehicles efficiently perform construction inspections and collect scene information, while the multimodal large language models (LLMs) are leveraged to automatically generate the inspection reports. The framework was applied and tested on a real-world construction site, demonstrating its potential to expedite the inspection process, significantly reduce resource allocation, and produce high-quality, regulatory standard-compliant inspection reports. This research thus underscores the immense potential of multimodal large language models in revolutionizing construction inspection practices, signaling a significant leap forward towards a more efficient and safer construction management paradigm.', 'authors': ['Heng Li', 'Runhao Guo', 'Jing Li', 'Xincong Yang', 'Hongxu Pu'], 'published': '2023-10-11', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'mm-bigbench-evaluating-multimodal-models-on', 'arxiv_id': '2310.09036', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.09036v1', 'url_pdf': 'https://arxiv.org/pdf/2310.09036v1.pdf', 'title': 'MM-BigBench: Evaluating Multimodal Models on Multimodal Content Comprehension Tasks', 'abstract': \"The popularity of multimodal large language models (MLLMs) has triggered a recent surge in research efforts dedicated to evaluating these models. Nevertheless, existing evaluation studies of MLLMs primarily focus on the comprehension and reasoning of unimodal (vision) content, neglecting performance evaluations in the domain of multimodal (vision-language) content understanding. Beyond multimodal reasoning, tasks related to multimodal content comprehension necessitate a profound understanding of multimodal contexts, achieved through the multimodal interaction to obtain a final answer. In this paper, we introduce a comprehensive assessment framework called MM-BigBench, which incorporates a diverse range of metrics to offer an extensive evaluation of the performance of various models and instructions across a wide spectrum of diverse multimodal content comprehension tasks. Consequently, our work complements research on the performance of MLLMs in multimodal comprehension tasks, achieving a more comprehensive and holistic evaluation of MLLMs. To begin, we employ the Best Performance metric to ascertain each model's performance upper bound on different datasets. Subsequently, the Mean Relative Gain metric offers an assessment of the overall performance of various models and instructions, while the Stability metric measures their sensitivity. Furthermore, previous research centers on evaluating models independently or solely assessing instructions, neglecting the adaptability between models and instructions. We propose the Adaptability metric to quantify the adaptability between models and instructions. Our paper evaluates a total of 20 language models (14 MLLMs) on 14 multimodal datasets spanning 6 tasks, with 10 instructions for each task, and derives novel insights. Our code will be released at https://github.com/declare-lab/MM-BigBench.\", 'authors': ['Soujanya Poria', 'XiaoMing Fu', 'Yifei Zhang', 'Qi Sun', 'Yang Li', 'Daling Wang', 'Ming Wang', 'Shi Feng', 'Wenfang Wu', 'Xiaocui Yang'], 'published': '2023-10-13', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'robollm-robotic-vision-tasks-grounded-on', 'arxiv_id': '2310.10221', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.10221v2', 'url_pdf': 'https://arxiv.org/pdf/2310.10221v2.pdf', 'title': 'RoboLLM: Robotic Vision Tasks Grounded on Multimodal Large Language Models', 'abstract': 'Robotic vision applications often necessitate a wide range of visual perception tasks, such as object detection, segmentation, and identification. While there have been substantial advances in these individual tasks, integrating specialized models into a unified vision pipeline presents significant engineering challenges and costs. Recently, Multimodal Large Language Models (MLLMs) have emerged as novel backbones for various downstream tasks. We argue that leveraging the pre-training capabilities of MLLMs enables the creation of a simplified framework, thus mitigating the need for task-specific encoders. Specifically, the large-scale pretrained knowledge in MLLMs allows for easier fine-tuning to downstream robotic vision tasks and yields superior performance. We introduce the RoboLLM framework, equipped with a BEiT-3 backbone, to address all visual perception tasks in the ARMBench challenge-a large-scale robotic manipulation dataset about real-world warehouse scenarios. RoboLLM not only outperforms existing baselines but also substantially reduces the engineering burden associated with model selection and tuning. The source code is publicly available at https://github.com/longkukuhi/armbench.', 'authors': ['Gerardo Aragon Camarasa', 'Richard McCreadie', 'George Killick', 'Zijun Long'], 'published': '2023-10-16', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'benchmarking-sequential-visual-input', 'arxiv_id': '2310.13473', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.13473v1', 'url_pdf': 'https://arxiv.org/pdf/2310.13473v1.pdf', 'title': 'Benchmarking Sequential Visual Input Reasoning and Prediction in Multimodal Large Language Models', 'abstract': \"Multimodal large language models (MLLMs) have shown great potential in perception and interpretation tasks, but their capabilities in predictive reasoning remain under-explored. To address this gap, we introduce a novel benchmark that assesses the predictive reasoning capabilities of MLLMs across diverse scenarios. Our benchmark targets three important domains: abstract pattern reasoning, human activity prediction, and physical interaction prediction. We further develop three evaluation methods powered by large language model to robustly quantify a model's performance in predicting and reasoning the future based on multi-visual context. Empirical experiments confirm the soundness of the proposed benchmark and evaluation methods via rigorous testing and reveal pros and cons of current popular MLLMs in the task of predictive reasoning. Lastly, our proposed benchmark provides a standardized evaluation framework for MLLMs and can facilitate the development of more advanced models that can reason and predict over complex long sequence of multimodal input.\", 'authors': ['Jianwei Yin', 'Tiancheng Zhao', 'Kangjia Zhao', 'Yu Shu', 'Leigang Sha', 'Mingwei Zhu'], 'published': '2023-10-20', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'woodpecker-hallucination-correction-for', 'arxiv_id': '2310.16045', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.16045v2', 'url_pdf': 'https://arxiv.org/pdf/2310.16045v2.pdf', 'title': 'Woodpecker: Hallucination Correction for Multimodal Large Language Models', 'abstract': 'Hallucination is a big shadow hanging over the rapidly evolving Multimodal Large Language Models (MLLMs), referring to the phenomenon that the generated text is inconsistent with the image content. In order to mitigate hallucinations, existing studies mainly resort to an instruction-tuning manner that requires retraining the models with specific data. In this paper, we pave a different way, introducing a training-free method named Woodpecker. Like a woodpecker heals trees, it picks out and corrects hallucinations from the generated text. Concretely, Woodpecker consists of five stages: key concept extraction, question formulation, visual knowledge validation, visual claim generation, and hallucination correction. Implemented in a post-remedy manner, Woodpecker can easily serve different MLLMs, while being interpretable by accessing intermediate outputs of the five stages. We evaluate Woodpecker both quantitatively and qualitatively and show the huge potential of this new paradigm. On the POPE benchmark, our method obtains a 30.66%/24.33% improvement in accuracy over the baseline MiniGPT-4/mPLUG-Owl. The source code is released at https://github.com/BradyFU/Woodpecker.', 'authors': ['Enhong Chen', 'Xing Sun', 'Ke Li', 'Yunhang Shen', 'Dianbo Sui', 'Hao Wang', 'Tong Xu', 'Sirui Zhao', 'Chaoyou Fu', 'Shukang Yin'], 'published': '2023-10-24', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'visual-cropping-improves-zero-shot-question', 'arxiv_id': '2310.16033', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.16033v3', 'url_pdf': 'https://arxiv.org/pdf/2310.16033v3.pdf', 'title': 'Towards Perceiving Small Visual Details in Zero-shot Visual Question Answering with Multimodal LLMs', 'abstract': \"Multimodal Large Language Models (MLLMs) have recently achieved promising zero-shot accuracy on visual question answering (VQA) -- a fundamental task affecting various downstream applications and domains. Given the great potential for the broad use of these models, it is important to investigate their limitations in dealing with different image and question properties. In this work, we investigate whether MLLMs can perceive small details as well as large details in images. In particular, we show that their zero-shot accuracy in answering visual questions is very sensitive to the size of the visual subject of the question, declining up to 46% with size. Furthermore, we show that this effect is causal by observing that human visual cropping can significantly mitigate their sensitivity to size. Inspired by the usefulness of human cropping, we then propose five automatic visual cropping methods -- leveraging either external localization models or the decision process of the given MLLM itself -- as inference time mechanisms to improve the zero-shot performance of MLLMs. We study their effectiveness on four popular VQA datasets, and a subset of the VQAv2 dataset tailored towards fine visual details. Our findings suggest that MLLMs should be used with caution in detail-sensitive VQA applications, and that visual cropping is a promising direction to improve their zero-shot performance. To facilitate further investigation of MLLMs' behaviors, our code and data are publicly released.\", 'authors': ['Filip Ilievski', 'Prateek Chhikara', 'Mahyar Khayatkhoei', 'Jiarui Zhang'], 'published': '2023-10-24', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'chef-a-comprehensive-evaluation-framework-for', 'arxiv_id': '2311.02692', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2311.02692v1', 'url_pdf': 'https://arxiv.org/pdf/2311.02692v1.pdf', 'title': 'ChEF: A Comprehensive Evaluation Framework for Standardized Assessment of Multimodal Large Language Models', 'abstract': \"Multimodal Large Language Models (MLLMs) have shown impressive abilities in interacting with visual content with myriad potential downstream tasks. However, even though a list of benchmarks has been proposed, the capabilities and limitations of MLLMs are still not comprehensively understood, due to a lack of a standardized and holistic evaluation framework. To this end, we present the first Comprehensive Evaluation Framework (ChEF) that can holistically profile each MLLM and fairly compare different MLLMs. First, we structure ChEF as four modular components, i.e., Scenario as scalable multimodal datasets, Instruction as flexible instruction retrieving formulae, Inferencer as reliable question answering strategies, and Metric as indicative task-specific score functions. Based on them, ChEF facilitates versatile evaluations in a standardized framework, and new evaluations can be built by designing new Recipes (systematic selection of these four components). Notably, current MLLM benchmarks can be readily summarized as recipes of ChEF. Second, we introduce 6 new recipes to quantify competent MLLMs' desired capabilities (or called desiderata, i.e., calibration, in-context learning, instruction following, language performance, hallucination, and robustness) as reliable agents that can perform real-world multimodal interactions. Third, we conduct a large-scale evaluation of 9 prominent MLLMs on 9 scenarios and 6 desiderata. Our evaluation summarized over 20 valuable observations concerning the generalizability of MLLMs across various scenarios and the composite capability of MLLMs required for multimodal interactions. We will publicly release all the detailed implementations for further analysis, as well as an easy-to-use modular toolkit for the integration of new recipes and models, so that ChEF can be a growing evaluation framework for the MLLM community.\", 'authors': ['Jing Shao', 'Yu Qiao', 'Lu Sheng', 'Zhenfei Yin', 'Hongxing Fan', 'Zhipin Wang', 'Zhelun Shi'], 'published': '2023-11-05', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'octavius-mitigating-task-interference-in', 'arxiv_id': '2311.02684', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2311.02684v3', 'url_pdf': 'https://arxiv.org/pdf/2311.02684v3.pdf', 'title': 'Octavius: Mitigating Task Interference in MLLMs via LoRA-MoE', 'abstract': 'Recent studies have demonstrated Large Language Models (LLMs) can extend their zero-shot generalization capabilities to multimodal learning through instruction tuning. As more modalities and downstream tasks are introduced, negative conflicts and interference may have a worse impact on performance. While this phenomenon has been overlooked in previous work, we propose a novel and extensible framework, called Octavius, for comprehensive studies and experimentation on multimodal learning with Multimodal Large Language Models (MLLMs). Specifically, we combine the well-known Mixture-of-Experts (MoE) and one of the representative PEFT techniques, i.e., LoRA, designing a novel LLM-based decoder, called LoRA-MoE, for multimodal learning. To the best of our knowledge, we are one of the pioneering efforts to introduce MoE into MLLMs to address this problem. The experimental results (about 20% improvement) have shown the effectiveness and versatility of our design in various 2D and 3D downstream tasks. Code and datasets are available at https://openlamm.github.io/tutorial/.', 'authors': ['Jing Shao', 'Yu Qiao', 'Wanli Ouyang', 'Lu Sheng', 'Si Liu', 'Zhenfei Yin', 'Huayang Liu', 'Zhen Wang', 'Ziqin Wang', 'Zeren Chen'], 'published': '2023-11-05', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'what-large-language-models-bring-to-text-rich', 'arxiv_id': '2311.07306', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2311.07306v1', 'url_pdf': 'https://arxiv.org/pdf/2311.07306v1.pdf', 'title': 'What Large Language Models Bring to Text-rich VQA?', 'abstract': \"Text-rich VQA, namely Visual Question Answering based on text recognition in the images, is a cross-modal task that requires both image comprehension and text recognition. In this work, we focus on investigating the advantages and bottlenecks of LLM-based approaches in addressing this problem. To address the above concern, we separate the vision and language modules, where we leverage external OCR models to recognize texts in the image and Large Language Models (LLMs) to answer the question given texts. The whole framework is training-free benefiting from the in-context ability of LLMs. This pipeline achieved superior performance compared to the majority of existing Multimodal Large Language Models (MLLM) on four text-rich VQA datasets. Besides, based on the ablation study, we find that LLM brings stronger comprehension ability and may introduce helpful knowledge for the VQA problem. The bottleneck for LLM to address text-rich VQA problems may primarily lie in visual part. We also combine the OCR module with MLLMs and pleasantly find that the combination of OCR module with MLLM also works. It's worth noting that not all MLLMs can comprehend the OCR information, which provides insights into how to train an MLLM that preserves the abilities of LLM.\", 'authors': ['Fei Tan', 'Zechao Li', 'Rui Zhao', 'Jinghui Lu', 'Xinzhe Ni', 'Wei Tang', 'Xuejing Liu'], 'published': '2023-11-13', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}], 'docs': [Document(metadata={'title': 'Multimodal large language models for inclusive collaboration learning tasks'}, page_content='This PhD project leverages advancements in multimodal large language models to build an inclusive collaboration feedback loop, in order to facilitate the automated detection, modeling, and feedback for participants developing general collaboration skills. This topic is important given the role of collaboration as an essential 21st century skill, the potential to ground large language models within learning theory and real-world practice, and the expressive potential of transformer models to support equity and inclusion. We address some concerns of integrating advances in natural language processing into downstream tasks such as the learning analytics feedback loop.'), Document(metadata={'title': 'Personality-aware Human-centric Multimodal Reasoning: A New Task, Dataset and Baselines'}, page_content=\"Personality traits, emotions, and beliefs shape individuals' behavioral choices and decision-making processes. However, for one thing, the affective computing community normally focused on predicting personality traits but overlooks their application in behavior prediction. For another, the multimodal reasoning task emphasized the prediction of future states and behaviors but often neglected the incorporation of individual personality traits. In this work, we introduce a new task called Personality-aware Human-centric Multimodal Reasoning (PHMR) (T1), with the goal of forecasting the future behavior of a particular individual using multimodal information from past instances, while integrating personality factors. We accordingly construct a new dataset based on six television shows, encompassing 225 characters and 12k samples. To establish a benchmark for the task, we propose seven baseline methods: three adapted from related tasks, two pre-trained model, and two multimodal large language models. The experimental results demonstrate that incorporating personality traits enhances human-centric multimodal reasoning performance. To further solve the lack of personality annotation in real-life scenes, we introduce an extension task called Personality-predicted Human-centric Multimodal Reasoning task (T2) along with the corresponding dataset and method. We will make our dataset and code available on GitHub.\"), Document(metadata={'title': 'Visual Transformation Telling'}, page_content='Humans can naturally reason from superficial state differences (e.g. ground wetness) to transformations descriptions (e.g. raining) according to their life experience. In this paper, we propose a new visual reasoning task to test this transformation reasoning ability in real-world scenarios, called \\\\textbf{V}isual \\\\textbf{T}ransformation \\\\textbf{T}elling (VTT). Given a series of states (i.e. images), VTT requires to describe the transformation occurring between every two adjacent states. Different from existing visual reasoning tasks that focus on surface state reasoning, the advantage of VTT is that it captures the underlying causes, e.g. actions or events, behind the differences among states. We collect a novel dataset to support the study of transformation reasoning from two existing instructional video datasets, CrossTask and COIN, comprising 13,547 samples. Each sample involves the key state images along with their transformation descriptions. Our dataset covers diverse real-world activities, providing a rich resource for training and evaluation. To construct an initial benchmark for VTT, we test several models, including traditional visual storytelling methods (CST, GLACNet, Densecap) and advanced multimodal large language models (LLaVA v1.5-7B, Qwen-VL-chat, Gemini Pro Vision, GPT-4o, and GPT-4). Experimental results reveal that even state-of-the-art models still face challenges in VTT, highlighting substantial areas for improvement.'), Document(metadata={'title': 'Listen, Think, and Understand'}, page_content='The ability of artificial intelligence (AI) systems to perceive and comprehend audio signals is crucial for many applications. Although significant progress has been made in this area since the development of AudioSet, most existing models are designed to map audio inputs to pre-defined, discrete sound label sets. In contrast, humans possess the ability to not only classify sounds into general categories, but also to listen to the finer details of the sounds, explain the reason for the predictions, think about what the sound infers, and understand the scene and what action needs to be taken, if any. Such capabilities beyond perception are not yet present in existing audio models. On the other hand, modern large language models (LLMs) exhibit emerging reasoning ability but they lack audio perception capabilities. Therefore, we ask the question: can we build a model that has both audio perception and a reasoning ability? In this paper, we propose a new audio foundation model, called LTU (Listen, Think, and Understand). To train LTU, we created a new OpenAQA-5M dataset consisting of 1.9 million closed-ended and 3.7 million open-ended, diverse (audio, question, answer) tuples, and have used an autoregressive training framework with a perception-to-understanding curriculum. LTU demonstrates strong performance and generalization ability on conventional audio tasks such as classification and captioning. More importantly, it exhibits emerging audio reasoning and comprehension abilities that are absent in existing audio models. To the best of our knowledge, LTU is one of the first multimodal large language models that focus on general audio (rather than just speech) understanding.'), Document(metadata={'title': 'What Makes for Good Visual Tokenizers for Large Language Models?'}, page_content='We empirically investigate proper pre-training methods to build good visual tokenizers, making Large Language Models (LLMs) powerful Multimodal Large Language Models (MLLMs). In our benchmark, which is curated to evaluate MLLMs visual semantic understanding and fine-grained perception capabilities, we discussed different visual tokenizers pre-trained with dominant methods (i.e., DeiT, CLIP, MAE, DINO), and observe that: i) Fully/weakly supervised models capture more semantics than self-supervised models, but the gap is narrowed by scaling up the pre-training dataset. ii) Self-supervised models are better at fine-grained perception, where patch-level supervision is particularly effective. iii) Tuning the visual tokenizer leads to the loss of semantics obtained from large-scale pretraining, which is unfavorable with relatively small-scale instruction-tuning dataset. Given the findings, we reviewed methods that attempted to unify semantics and fine-grained visual understanding, e.g., patch-level feature distillation with semantically-rich targets. We obtain an intriguing insight mask-based strategies that were once all the rage may not be applicable for obtaining good visual tokenizers. Based on this critical observation, we obtain a new MLLM equipped with a tailored Good Visual Tokenizer (GVT), which exhibits strong visual comprehension capability at multiple scales. In particular, without introducing extra parameters and task-specific fine-tuning, GVT achieves superior performance on visual question answering, image captioning, and other fine-grained visual understanding tasks such as object counting and multi-class identification.'), Document(metadata={'title': 'Dynamic Clue Bottlenecks: Towards Interpretable-by-Design Visual Question Answering'}, page_content='Recent advances in multimodal large language models (LLMs) have shown extreme effectiveness in visual question answering (VQA). However, the design nature of these end-to-end models prevents them from being interpretable to humans, undermining trust and applicability in critical domains. While post-hoc rationales offer certain insight into understanding model behavior, these explanations are not guaranteed to be faithful to the model. In this paper, we address these shortcomings by introducing an interpretable by design model that factors model decisions into intermediate human-legible explanations, and allows people to easily understand why a model fails or succeeds. We propose the Dynamic Clue Bottleneck Model ( (DCLUB), a method that is designed towards an inherently interpretable VQA system. DCLUB provides an explainable intermediate space before the VQA decision and is faithful from the beginning, while maintaining comparable performance to black-box systems. Given a question, DCLUB first returns a set of visual clues: natural language statements of visually salient evidence from the image, and then generates the output based solely on the visual clues. To supervise and evaluate the generation of VQA explanations within DCLUB, we collect a dataset of 1.7k reasoning-focused questions with visual clues. Evaluations show that our inherently interpretable system can improve 4.64% over a comparable black-box system in reasoning-focused questions while preserving 99.43% of performance on VQA-v2.'), Document(metadata={'title': 'PathAsst: A Generative Foundation AI Assistant Towards Artificial General Intelligence of Pathology'}, page_content=\"As advances in large language models (LLMs) and multimodal techniques continue to mature, the development of general-purpose multimodal large language models (MLLMs) has surged, offering significant applications in interpreting natural images. However, the field of pathology has largely remained untapped, particularly in gathering high-quality data and designing comprehensive model frameworks. To bridge the gap in pathology MLLMs, we present PathAsst, a multimodal generative foundation AI assistant to revolutionize diagnostic and predictive analytics in pathology. The development of PathAsst involves three pivotal steps: data acquisition, CLIP model adaptation, and the training of PathAsst's multimodal generative capabilities. Firstly, we collect over 207K high-quality pathology image-text pairs from authoritative sources. Leveraging the advanced power of ChatGPT, we generate over 180K instruction-following samples. Furthermore, we devise additional instruction-following data specifically tailored for invoking eight pathology-specific sub-models we prepared, allowing the PathAsst to effectively collaborate with these models, enhancing its diagnostic ability. Secondly, by leveraging the collected data, we construct PathCLIP, a pathology-dedicated CLIP, to enhance PathAsst's capabilities in interpreting pathology images. Finally, we integrate PathCLIP with the Vicuna-13b and utilize pathology-specific instruction-tuning data to enhance the multimodal generation capacity of PathAsst and bolster its synergistic interactions with sub-models. The experimental results of PathAsst show the potential of harnessing AI-powered generative foundation model to improve pathology diagnosis and treatment processes.\"), Document(metadata={'title': 'Contextual Object Detection with Multimodal Large Language Models'}, page_content='Recent Multimodal Large Language Models (MLLMs) are remarkable in vision-language tasks, such as image captioning and question answering, but lack the essential perception ability, i.e., object detection. In this work, we address this limitation by introducing a novel research problem of contextual object detection -- understanding visible objects within different human-AI interactive contexts. Three representative scenarios are investigated, including the language cloze test, visual captioning, and question answering. Moreover, we present ContextDET, a unified multimodal model that is capable of end-to-end differentiable modeling of visual-language contexts, so as to locate, identify, and associate visual objects with language inputs for human-AI interaction. Our ContextDET involves three key submodels: (i) a visual encoder for extracting visual representations, (ii) a pre-trained LLM for multimodal context decoding, and (iii) a visual decoder for predicting bounding boxes given contextual object words. The new generate-then-detect framework enables us to detect object words within human vocabulary. Extensive experiments show the advantages of ContextDET on our proposed CODE benchmark, open-vocabulary detection, and referring image segmentation. Github: https://github.com/yuhangzang/ContextDET.'), Document(metadata={'title': 'A Survey on Multimodal Large Language Models'}, page_content='Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.'), Document(metadata={'title': 'MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models'}, page_content='Multimodal Large Language Model (MLLM) relies on the powerful LLM to perform multimodal tasks, showing amazing emergent abilities in recent studies, such as writing poems based on an image. However, it is difficult for these case studies to fully reflect the performance of MLLM, lacking a comprehensive evaluation. In this paper, we fill in this blank, presenting the first comprehensive MLLM Evaluation benchmark MME. It measures both perception and cognition abilities on a total of 14 subtasks. In order to avoid data leakage that may arise from direct use of public datasets for evaluation, the annotations of instruction-answer pairs are all manually designed. The concise instruction design allows us to fairly compare MLLMs, instead of struggling in prompt engineering. Besides, with such an instruction, we can also easily carry out quantitative statistics. A total of 30 advanced MLLMs are comprehensively evaluated on our MME, which not only suggests that existing MLLMs still have a large room for improvement, but also reveals the potential directions for the subsequent model optimization. The data application manner and online leaderboards are released at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation.'), Document(metadata={'title': 'Kosmos-2: Grounding Multimodal Large Language Models to the World'}, page_content=\"We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. Specifically, we represent refer expressions as links in Markdown, i.e., ``[text span](bounding boxes)'', where object descriptions are sequences of location tokens. Together with multimodal corpora, we construct large-scale data of grounded image-text pairs (called GrIT) to train the model. In addition to the existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), Kosmos-2 integrates the grounding capability into downstream applications. We evaluate Kosmos-2 on a wide range of tasks, including (i) multimodal grounding, such as referring expression comprehension, and phrase grounding, (ii) multimodal referring, such as referring expression generation, (iii) perception-language tasks, and (iv) language understanding and generation. This work lays out the foundation for the development of Embodiment AI and sheds light on the big convergence of language, multimodal perception, action, and world modeling, which is a key step toward artificial general intelligence. Code and pretrained models are available at https://aka.ms/kosmos-2.\"), Document(metadata={'title': \"Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic\"}, page_content=\"In human conversations, individuals can indicate relevant regions within a scene while addressing others. In turn, the other person can then respond by referring to specific regions if necessary. This natural referential ability in dialogue remains absent in current Multimodal Large Language Models (MLLMs). To fill this gap, this paper proposes an MLLM called Shikra, which can handle spatial coordinate inputs and outputs in natural language. Its architecture consists of a vision encoder, an alignment layer, and a LLM. It is designed to be straightforward and simple, without the need for extra vocabularies, position encoder, pre-/post-detection modules, or external plug-in models. All inputs and outputs are in natural language form. Referential dialogue is a superset of various vision-language (VL) tasks. Shikra can naturally handle location-related tasks like REC and PointQA, as well as conventional VL tasks such as Image Captioning and VQA. Experimental results showcase Shikra's promising performance. Furthermore, it enables numerous exciting applications, like providing mentioned objects' coordinates in chains of thoughts and comparing user-pointed regions similarities. Our code, model and dataset are accessed at https://github.com/shikras/shikra.\"), Document(metadata={'title': 'SVIT: Scaling up Visual Instruction Tuning'}, page_content=\"Thanks to the emerging of foundation models, the large language and vision models are integrated to acquire the multimodal ability of visual captioning, question answering, etc. Although existing multimodal models present impressive performance of visual understanding and reasoning, their limits are still largely under-explored due to the scarcity of high-quality instruction tuning data. To push the limits of multimodal capability, we Scale up Visual Instruction Tuning (SVIT) by constructing a dataset of 4.2 million visual instruction tuning data including 1.6M conversation question-answer (QA) pairs, 1.6M complex reasoning QA pairs, 1.0M referring QA pairs and 106K detailed image descriptions. Besides the volume, the proposed dataset is also featured by the high quality and rich diversity, which is generated by prompting GPT-4 with the abundant manual annotations of images. We also propose a new data recipe to select subset with better diversity and balance, which evokes model's superior capabilities. Extensive experiments verify that SVIT-v1.5, trained on the proposed dataset, outperforms state-of-the-art Multimodal Large Language Models on popular benchmarks. The data and code are publicly available at https://github.com/BAAI-DCAI/Visual-Instruction-Tuning.\"), Document(metadata={'title': 'ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning'}, page_content=\"Human-AI interactivity is a critical aspect that reflects the usability of multimodal large language models (MLLMs). However, existing end-to-end MLLMs only allow users to interact with them through language instructions, leading to the limitation of the interactive accuracy and efficiency. In this study, we present precise referring instructions that utilize diverse reference representations such as points and boxes as referring prompts to refer to the special region. This enables MLLMs to focus on the region of interest and achieve finer-grained interaction. Based on precise referring instruction, we propose ChatSpot, a unified end-to-end multimodal large language model that supports diverse forms of interactivity including mouse clicks, drag-and-drop, and drawing boxes, which provides a more flexible and seamless interactive experience. We also construct a multi-grained vision-language instruction-following dataset based on existing datasets and GPT-4 generating. Furthermore, we design a series of evaluation tasks to assess the effectiveness of region recognition and interaction. Experimental results showcase ChatSpot's promising performance.\"), Document(metadata={'title': 'SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension'}, page_content='Based on powerful Large Language Models (LLMs), recent generative Multimodal Large Language Models (MLLMs) have gained prominence as a pivotal research area, exhibiting remarkable capability for both comprehension and generation. In this work, we address the evaluation of generative comprehension in MLLMs as a preliminary step towards a comprehensive assessment of generative models, by introducing a benchmark named SEED-Bench. SEED-Bench consists of 19K multiple choice questions with accurate human annotations (x 6 larger than existing benchmarks), which spans 12 evaluation dimensions including the comprehension of both the image and video modality. We develop an advanced pipeline for generating multiple-choice questions that target specific evaluation dimensions, integrating both automatic filtering and manual verification processes. Multiple-choice questions with groundtruth options derived from human annotation enables an objective and efficient assessment of model performance, eliminating the need for human or GPT intervention during evaluation. We further evaluate the performance of 18 models across all 12 dimensions, covering both the spatial and temporal understanding. By revealing the limitations of existing MLLMs through evaluation results, we aim for SEED-Bench to provide insights for motivating future research. We will launch and consistently maintain a leaderboard to provide a platform for the community to assess and investigate model capability.'), Document(metadata={'title': 'LISA: Reasoning Segmentation via Large Language Model'}, page_content='Although perception systems have made remarkable advancements in recent years, they still rely on explicit human instruction or pre-defined categories to identify the target objects before executing visual recognition tasks. Such systems cannot actively reason and comprehend implicit user intention. In this work, we propose a new segmentation task -- reasoning segmentation. The task is designed to output a segmentation mask given a complex and implicit query text. Furthermore, we establish a benchmark comprising over one thousand image-instruction-mask data samples, incorporating intricate reasoning and world knowledge for evaluation purposes. Finally, we present LISA: large Language Instructed Segmentation Assistant, which inherits the language generation capabilities of multimodal Large Language Models (LLMs) while also possessing the ability to produce segmentation masks. We expand the original vocabulary with a <SEG> token and propose the embedding-as-mask paradigm to unlock the segmentation capability. Remarkably, LISA can handle cases involving complex reasoning and world knowledge. Also, it demonstrates robust zero-shot capability when trained exclusively on reasoning-free datasets. In addition, fine-tuning the model with merely 239 reasoning segmentation data samples results in further performance enhancement. Both quantitative and qualitative experiments show our method effectively unlocks new reasoning segmentation capabilities for multimodal LLMs. Code, models, and data are available at https://github.com/dvlab-research/LISA.'), Document(metadata={'title': 'Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions'}, page_content=\"Recent advancements in Multimodal Large Language Models (MLLMs) have been utilizing Visual Prompt Generators (VPGs) to convert visual features into tokens that LLMs can recognize. This is achieved by training the VPGs on millions of image-caption pairs, where the VPG-generated tokens of images are fed into a frozen LLM to generate the corresponding captions. However, this image-captioning based training objective inherently biases the VPG to concentrate solely on the primary visual contents sufficient for caption generation, often neglecting other visual details. This shortcoming results in MLLMs' underperformance in comprehending demonstrative instructions consisting of multiple, interleaved, and multimodal instructions that demonstrate the required context to complete a task. To address this issue, we introduce a generic and lightweight Visual Prompt Generator Complete module (VPG-C), which can infer and complete the missing details essential for comprehending demonstrative instructions. Further, we propose a synthetic discriminative training strategy to fine-tune VPG-C, eliminating the need for supervised demonstrative instructions. As for evaluation, we build DEMON, a comprehensive benchmark for demonstrative instruction understanding. Synthetically trained with the proposed strategy, VPG-C achieves significantly stronger zero-shot performance across all tasks of DEMON. Further evaluation on the MME and OwlEval benchmarks also demonstrate the superiority of VPG-C. Our benchmark, code, and pre-trained models are available at https://github.com/DCDmllm/Cheetah.\"), Document(metadata={'title': 'Link-Context Learning for Multimodal LLMs'}, page_content='The ability to learn from context with novel concepts, and deliver appropriate responses are essential in human conversations. Despite current Multimodal Large Language Models (MLLMs) and Large Language Models (LLMs) being trained on mega-scale datasets, recognizing unseen images or understanding novel concepts in a training-free manner remains a challenge. In-Context Learning (ICL) explores training-free few-shot learning, where models are encouraged to ``learn to learn\" from limited tasks and generalize to unseen tasks. In this work, we propose link-context learning (LCL), which emphasizes \"reasoning from cause and effect\" to augment the learning capabilities of MLLMs. LCL goes beyond traditional ICL by explicitly strengthening the causal relationship between the support set and the query set. By providing demonstrations with causal links, LCL guides the model to discern not only the analogy but also the underlying causal associations between data points, which empowers MLLMs to recognize unseen images and understand novel concepts more effectively. To facilitate the evaluation of this novel approach, we introduce the ISEKAI dataset, comprising exclusively of unseen generated image-label pairs designed for link-context learning. Extensive experiments show that our LCL-MLLM exhibits strong link-context learning capabilities to novel concepts over vanilla MLLMs. Code and data will be released at https://github.com/isekai-portal/Link-Context-Learning.'), Document(metadata={'title': 'FashionLOGO: Prompting Multimodal Large Language Models for Fashion Logo Embeddings'}, page_content=\"Logo embedding models convert the product logos in images into vectors, enabling their utilization for logo recognition and detection within e-commerce platforms. This facilitates the enforcement of intellectual property rights and enhances product search capabilities. However, current methods treat logo embedding as a purely visual problem. A noteworthy issue is that visual models capture features more than logos. Instead, we view this as a multimodal task, using text as auxiliary information to facilitate the visual model's understanding of the logo. The emerging Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in both visual and textual understanding. Inspired by this, we propose an approach, \\\\textbf{FashionLOGO}, to explore how to prompt MLLMs to generate appropriate text for product images, which can help visual models achieve better logo embeddings. We adopt a cross-attention transformer block that enables visual embedding to automatically learn supplementary knowledge from textual embedding. Our extensive experiments on real-world datasets prove that FashionLOGO is capable of generating generic and robust logo embeddings, achieving state-of-the-art performance in all benchmarks.\"), Document(metadata={'title': 'WanJuan: A Comprehensive Multimodal Dataset for Advancing English and Chinese Large Models'}, page_content='The rise in popularity of ChatGPT and GPT-4 has significantly accelerated the development of large models, leading to the creation of numerous impressive large language models(LLMs) and multimodal large language models (MLLMs). These cutting-edge models owe their remarkable performance to high-quality data. However, the details of the training data used in leading paradigms are often kept confidential. This lack of transparency, coupled with the scarcity of open-source data, impedes further developments within the community. As a response, this paper presents \"Wan Juan\", a large-scale multimodal dataset composed of both Chinese and English data, collected from a wide range of web sources. The dataset incorporates text, image-text, and video modalities, with a total volume exceeding 2TB. It was utilized in the training of InternLM, a model that demonstrated significant advantages in multi-dimensional evaluations when compared to models of a similar scale. All data can be accessed at https://opendatalab.org.cn/WanJuan1.0.'), Document(metadata={'title': 'StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data'}, page_content=\"The remarkable multimodal capabilities demonstrated by OpenAI's GPT-4 have sparked significant interest in the development of multimodal Large Language Models (LLMs). A primary research objective of such models is to align visual and textual modalities effectively while comprehending human instructions. Current methodologies often rely on annotations derived from benchmark datasets to construct image-dialogue datasets for training purposes, akin to instruction tuning in LLMs. However, these datasets often exhibit domain bias, potentially constraining the generative capabilities of the models. In an effort to mitigate these limitations, we propose a novel data collection methodology that synchronously synthesizes images and dialogues for visual instruction tuning. This approach harnesses the power of generative models, marrying the abilities of ChatGPT and text-to-image generative models to yield a diverse and controllable dataset with varied image content. Additionally, datasets can be arbitrarily scaled. This not only provides greater flexibility compared to existing methodologies but also significantly enhances several model capabilities. Our research includes comprehensive experiments conducted on various datasets. The results emphasize substantial enhancements in more than ten commonly assessed capabilities. Additionally, our model achieves state-of-the-art results across multiple widely recognized multimodal benchmarks.\"), Document(metadata={'title': 'InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4'}, page_content='Multimodal large language models are typically trained in two stages: first pre-training on image-text pairs, and then fine-tuning using supervised vision-language instruction data. Recent studies have shown that large language models can achieve satisfactory results even with a limited amount of high-quality instruction-following data. In this paper, we introduce InstructionGPT-4, which is fine-tuned on a small dataset comprising only 200 examples, amounting to approximately 6\\\\% of the instruction-following data used in the alignment dataset for MiniGPT-4. To achieve this, we first propose several metrics to access the quality of multimodal instruction data. Based on these metrics, we present an effective and trainable data selector to automatically identify and filter low-quality vision-language data. By employing this method, InstructionGPT-4 outperforms the original MiniGPT-4 on various evaluations. Overall, our findings demonstrate that less but high-quality instruction tuning data is efficient in enabling multimodal large language models to generate better output. Our code is available at https://github.com/waltonfuture/InstructionGPT-4.'), Document(metadata={'title': 'VIGC: Visual Instruction Generation and Correction'}, page_content=\"The integration of visual encoders and large language models (LLMs) has driven recent progress in multimodal large language models (MLLMs). However, the scarcity of high-quality instruction-tuning data for vision-language tasks remains a challenge. The current leading paradigm, such as LLaVA, relies on language-only GPT-4 to generate data, which requires pre-annotated image captions and detection bounding boxes, suffering from understanding image details. A practical solution to this problem would be to utilize the available multimodal large language models (MLLMs) to generate instruction data for vision-language tasks. However, it's worth noting that the currently accessible MLLMs are not as powerful as their LLM counterparts, as they tend to produce inadequate responses and generate false information. As a solution for addressing the current issue, this paper proposes the Visual Instruction Generation and Correction (VIGC) framework that enables multimodal large language models to generate instruction-tuning data and progressively enhance its quality on-the-fly. Specifically, Visual Instruction Generation (VIG) guides the vision-language model to generate diverse instruction-tuning data. To ensure generation quality, Visual Instruction Correction (VIC) adopts an iterative update mechanism to correct any inaccuracies in data produced by VIG, effectively reducing the risk of hallucination. Leveraging the diverse, high-quality data generated by VIGC, we finetune mainstream models and validate data quality based on various evaluations. Experimental results demonstrate that VIGC not only compensates for the shortcomings of language-only data generation methods, but also effectively enhances the benchmark performance. The models, datasets, and code are available at https://opendatalab.github.io/VIGC.\"), Document(metadata={'title': 'Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models'}, page_content='Recently, Multimodal Large Language Models (MLLMs) that enable Large Language Models (LLMs) to interpret images through visual instruction tuning have achieved significant success. However, existing visual instruction tuning methods only utilize image-language instruction data to align the language and image modalities, lacking a more fine-grained cross-modal alignment. In this paper, we propose Position-enhanced Visual Instruction Tuning (PVIT), which extends the functionality of MLLMs by integrating an additional region-level vision encoder. This integration promotes a more detailed comprehension of images for the MLLM. In addition, to efficiently achieve a fine-grained alignment between the vision modules and the LLM, we design multiple data generation strategies to construct an image-region-language instruction dataset. Finally, we present both quantitative experiments and qualitative analysis that demonstrate the superiority of the proposed model. Code and data will be released at https://github.com/PVIT-official/PVIT.'), Document(metadata={'title': 'MLLM-DataEngine: An Iterative Refinement Approach for MLLM'}, page_content='Despite the great advance of Multimodal Large Language Models (MLLMs) in both instruction dataset building and benchmarking, the independence of training and evaluation makes current MLLMs hard to further improve their capability under the guidance of evaluation results with a relatively low human cost. In this paper, we propose MLLM-DataEngine, a novel closed-loop system that bridges data generation, model training, and evaluation. Within each loop iteration, the MLLM-DataEngine first analyze the weakness of the model based on the evaluation results, then generate a proper incremental dataset for the next training iteration and enhance the model capability iteratively. Compared with previous data collection methods which are separate from the benchmarking, the data generated by MLLM-DataEngine shows better targeting, quality, and correctness. For targeting, we propose an Adaptive Bad-case Sampling module, which adjusts the ratio of different types of data within each incremental dataset based on the benchmarking results. For quality, we resort to GPT-4 to generate high-quality data with each given data type. For correctness, prompt design is critical for the data generation results. Rather than previous hand-crafted prompt, we propose an Interactive Prompt Optimization strategy, which optimizes the prompt with the multi-round interaction between human and GPT, and improve the correctness of generated data greatly. Through extensive experiments, we find our MLLM-DataEngine could boost the MLLM capability in a targeted and automatic manner, with only a few human participation. We hope it could be a general solution for the following MLLMs building. The MLLM-DataEngine has been open-sourced and is now available at https://github.com/opendatalab/MLLM-DataEngine.'), Document(metadata={'title': 'Socratis: Are large multimodal models emotionally aware?'}, page_content='Existing emotion prediction benchmarks contain coarse emotion labels which do not consider the diversity of emotions that an image and text can elicit in humans due to various reasons. Learning diverse reactions to multimodal content is important as intelligent machines take a central role in generating and delivering content to society. To address this gap, we propose Socratis, a societal reactions benchmark, where each image-caption (IC) pair is annotated with multiple emotions and the reasons for feeling them. Socratis contains 18K free-form reactions for 980 emotions on 2075 image-caption pairs from 5 widely-read news and image-caption (IC) datasets. We benchmark the capability of state-of-the-art multimodal large language models to generate the reasons for feeling an emotion given an IC pair. Based on a preliminary human study, we observe that humans prefer human-written reasons over 2 times more often than machine-generated ones. This shows our task is harder than standard generation tasks because it starkly contrasts recent findings where humans cannot tell apart machine vs human-written news articles, for instance. We further see that current captioning metrics based on large vision-language models also fail to correlate with human preferences. We hope that these findings and our benchmark will inspire further research on training emotionally aware models.'), Document(metadata={'title': 'MultiWay-Adapater: Adapting large-scale multi-modal models for scalable image-text retrieval'}, page_content=\"As Multimodal Large Language Models (MLLMs) grow in size, adapting them to specialized tasks becomes increasingly challenging due to high computational and memory demands. Indeed, traditional fine-tuning methods are costly, due to the need for extensive, task-specific training. While efficient adaptation methods exist that aim to reduce these costs, in practice they suffer from shallow inter-modal alignment, which severely hurts model effectiveness. To tackle these computational challenges and improve inter-modal alignment, we introduce the MultiWay-Adapter (MWA), a novel framework featuring an 'Alignment Enhancer'. This enhancer deepens inter-modal alignment, enabling high transferability with minimal tuning effort. Our experiments show that unlike prior efficient tuning approaches, MWA maintains model effectiveness, while reducing training time by up-to 57%. MWA is also lightweight, increasing model size by only 2-3% (in terms of parameters) for state-of-the-art foundation models like BEiT-3 Large. These results demonstrate that MWA provides an efficient and effective adaptation method for MLLMs, significantly broadening their applicability.\"), Document(metadata={'title': 'NExT-GPT: Any-to-Any Multimodal LLM'}, page_content='While recently Multimodal Large Language Models (MM-LLMs) have made exciting strides, they mostly fall prey to the limitation of only input-side multimodal understanding, without the ability to produce content in multiple modalities. As we humans always perceive the world and communicate with people through various modalities, developing any-to-any MM-LLMs capable of accepting and delivering content in any modality becomes essential to human-level AI. To fill the gap, we present an end-to-end general-purpose any-to-any MM-LLM system, NExT-GPT. We connect an LLM with multimodal adaptors and different diffusion decoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio. By leveraging the existing well-trained highly-performing encoders and decoders, NExT-GPT is tuned with only a small amount of parameter (1%) of certain projection layers, which not only benefits low-cost training and also facilitates convenient expansion to more potential modalities. Moreover, we introduce a modality-switching instruction tuning (MosIT) and manually curate a high-quality dataset for MosIT, based on which NExT-GPT is empowered with complex cross-modal semantic understanding and content generation. Overall, our research showcases the promising possibility of building an AI agent capable of modeling universal modalities, paving the way for more human-like AI research in the community. Project page: https://next-gpt.github.io/'), Document(metadata={'title': 'HiLM-D: Towards High-Resolution Understanding in Multimodal Large Language Models for Autonomous Driving'}, page_content=\"Autonomous driving systems generally employ separate models for different tasks resulting in intricate designs. For the first time, we leverage singular multimodal large language models (MLLMs) to consolidate multiple autonomous driving tasks from videos, i.e., the Risk Object Localization and Intention and Suggestion Prediction (ROLISP) task. ROLISP uses natural language to simultaneously identify and interpret risk objects, understand ego-vehicle intentions, and provide motion suggestions, eliminating the necessity for task-specific architectures. However, lacking high-resolution (HR) information, existing MLLMs often miss small objects (e.g., traffic cones) and overly focus on salient ones (e.g., large trucks) when applied to ROLISP. We propose HiLM-D (Towards High-Resolution Understanding in MLLMs for Autonomous Driving), an efficient method to incorporate HR information into MLLMs for the ROLISP task. Especially, HiLM-D integrates two branches: (i) the low-resolution reasoning branch, can be any MLLMs, processes low-resolution videos to caption risk objects and discern ego-vehicle intentions/suggestions; (ii) the high-resolution perception branch (HR-PB), prominent to HiLM-D,, ingests HR images to enhance detection by capturing vision-specific HR feature maps and prioritizing all potential risks over merely salient objects. Our HR-PB serves as a plug-and-play module, seamlessly fitting into current MLLMs. Experiments on the ROLISP benchmark reveal HiLM-D's notable advantage over leading MLLMs, with improvements of 4.8% in BLEU-4 for captioning and 17.2% in mIoU for detection.\"), Document(metadata={'title': 'Investigating the Catastrophic Forgetting in Multimodal Large Language Models'}, page_content='Following the success of GPT4, there has been a surge in interest in multimodal large language model (MLLM) research. This line of research focuses on developing general-purpose LLMs through fine-tuning pre-trained LLMs and vision models. However, catastrophic forgetting, a notorious phenomenon where the fine-tuned model fails to retain similar performance compared to the pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM). In this paper, we introduce EMT: Evaluating MulTimodality for evaluating the catastrophic forgetting in MLLMs, by treating each MLLM as an image classifier. We first apply EMT to evaluate several open-source fine-tuned MLLMs and we discover that almost all evaluated MLLMs fail to retain the same performance levels as their vision encoders on standard image classification tasks. Moreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess performance throughout the fine-tuning. Interestingly, our results suggest that early-stage fine-tuning on an image dataset improves performance across other image datasets, by enhancing the alignment of text and visual features. However, as fine-tuning proceeds, the MLLMs begin to hallucinate, resulting in a significant loss of generalizability, even when the image encoder remains frozen. Our results suggest that MLLMs have yet to demonstrate performance on par with their vision models on standard image classification tasks and the current MLLM fine-tuning procedure still has room for improvement.'), Document(metadata={'title': 'DreamLLM: Synergistic Multimodal Comprehension and Creation'}, page_content=\"This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy. Project page: https://dreamllm.github.io.\"), Document(metadata={'title': \"How Robust is Google's Bard to Adversarial Image Attacks?\"}, page_content=\"Multimodal Large Language Models (MLLMs) that integrate text and other modalities (especially vision) have achieved unprecedented performance in various multimodal tasks. However, due to the unsolved adversarial robustness problem of vision models, MLLMs can have more severe safety and security risks by introducing the vision inputs. In this work, we study the adversarial robustness of Google's Bard, a competitive chatbot to ChatGPT that released its multimodal capability recently, to better understand the vulnerabilities of commercial MLLMs. By attacking white-box surrogate vision encoders or MLLMs, the generated adversarial examples can mislead Bard to output wrong image descriptions with a 22% success rate based solely on the transferability. We show that the adversarial examples can also attack other MLLMs, e.g., a 26% attack success rate against Bing Chat and a 86% attack success rate against ERNIE bot. Moreover, we identify two defense mechanisms of Bard, including face detection and toxicity detection of images. We design corresponding attacks to evade these defenses, demonstrating that the current defenses of Bard are also vulnerable. We hope this work can deepen our understanding on the robustness of MLLMs and facilitate future research on defenses. Our code is available at https://github.com/thu-ml/Attack-Bard. Update: GPT-4V is available at October 2023. We further evaluate its robustness under the same set of adversarial examples, achieving a 45% attack success rate.\"), Document(metadata={'title': 'VDC: Versatile Data Cleanser based on Visual-Linguistic Inconsistency by Multimodal Large Language Models'}, page_content='The role of data in building AI systems has recently been emphasized by the emerging concept of data-centric AI. Unfortunately, in the real-world, datasets may contain dirty samples, such as poisoned samples from backdoor attack, noisy labels in crowdsourcing, and even hybrids of them. The presence of such dirty samples makes the DNNs vunerable and unreliable.Hence, it is critical to detect dirty samples to improve the quality and realiability of dataset. Existing detectors only focus on detecting poisoned samples or noisy labels, that are often prone to weak generalization when dealing with dirty samples from other domains.In this paper, we find a commonality of various dirty samples is visual-linguistic inconsistency between images and associated labels. To capture the semantic inconsistency between modalities, we propose versatile data cleanser (VDC) leveraging the surpassing capabilities of multimodal large language models (MLLM) in cross-modal alignment and reasoning.It consists of three consecutive modules: the visual question generation module to generate insightful questions about the image; the visual question answering module to acquire the semantics of the visual content by answering the questions with MLLM; followed by the visual answer evaluation module to evaluate the inconsistency.Extensive experiments demonstrate its superior performance and generalization to various categories and types of dirty samples. The code is available at \\\\url{https://github.com/zihao-ai/vdc}.'), Document(metadata={'title': 'Guiding Instruction-based Image Editing via Multimodal Large Language Models'}, page_content='Instruction-based image editing improves the controllability and flexibility of image manipulation via natural commands without elaborate descriptions or regional masks. However, human instructions are sometimes too brief for current methods to capture and follow. Multimodal large language models (MLLMs) show promising capabilities in cross-modal understanding and visual-aware response generation via LMs. We investigate how MLLMs facilitate edit instructions and present MLLM-Guided Image Editing (MGIE). MGIE learns to derive expressive instructions and provides explicit guidance. The editing model jointly captures this visual imagination and performs manipulation through end-to-end training. We evaluate various aspects of Photoshop-style modification, global photo optimization, and local editing. Extensive experimental results demonstrate that expressive instructions are crucial to instruction-based image editing, and our MGIE can lead to a notable improvement in automatic metrics and human evaluation while maintaining competitive inference efficiency.'), Document(metadata={'title': 'Reformulating Vision-Language Foundation Models and Datasets Towards Universal Multimodal Assistants'}, page_content=\"Recent Multimodal Large Language Models (MLLMs) exhibit impressive abilities to perceive images and follow open-ended instructions. The capabilities of MLLMs depend on two crucial factors: the model architecture to facilitate the feature alignment of visual modules and large language models; the multimodal instruction tuning datasets for human instruction following. (i) For the model architecture, most existing models introduce an external bridge module to connect vision encoders with language models, which needs an additional feature-alignment pre-training. In this work, we discover that compact pre-trained vision language models can inherently serve as ``out-of-the-box'' bridges between vision and language. Based on this, we propose Muffin framework, which directly employs pre-trained vision-language models to act as providers of visual signals. (ii) For the multimodal instruction tuning datasets, existing methods omit the complementary relationship between different datasets and simply mix datasets from different tasks. Instead, we propose UniMM-Chat dataset which explores the complementarities of datasets to generate 1.1M high-quality and diverse multimodal instructions. We merge information describing the same image from diverse datasets and transforms it into more knowledge-intensive conversation data. Experimental results demonstrate the effectiveness of the Muffin framework and UniMM-Chat dataset. Muffin achieves state-of-the-art performance on a wide range of vision-language tasks, significantly surpassing state-of-the-art models like LLaVA and InstructBLIP. Our model and dataset are all accessible at https://github.com/thunlp/muffin.\"), Document(metadata={'title': 'DriveGPT4: Interpretable End-to-end Autonomous Driving via Large Language Model'}, page_content='Multimodal large language models (MLLMs) have emerged as a prominent area of interest within the research community, given their proficiency in handling and reasoning with non-textual data, including images and videos. This study seeks to extend the application of MLLMs to the realm of autonomous driving by introducing DriveGPT4, a novel interpretable end-to-end autonomous driving system based on LLMs. Capable of processing multi-frame video inputs and textual queries, DriveGPT4 facilitates the interpretation of vehicle actions, offers pertinent reasoning, and effectively addresses a diverse range of questions posed by users. Furthermore, DriveGPT4 predicts low-level vehicle control signals in an end-to-end fashion.These advanced capabilities are achieved through the utilization of a bespoke visual instruction tuning dataset, specifically tailored for autonomous driving applications, in conjunction with a mix-finetuning training strategy. DriveGPT4 represents the pioneering effort to leverage LLMs for the development of an interpretable end-to-end autonomous driving solution. Evaluations conducted on the BDD-X dataset showcase the superior qualitative and quantitative performance of DriveGPT4. Additionally, the fine-tuning of domain-specific data enables DriveGPT4 to yield close or even improved results in terms of autonomous driving grounding when contrasted with GPT4-V.'), Document(metadata={'title': 'MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens'}, page_content='The effectiveness of Multimodal Large Language Models (MLLMs) demonstrates a profound capability in multimodal understanding. However, the simultaneous generation of images with coherent texts is still underdeveloped. Addressing this, we introduce a novel interleaved vision-and-language generation method, centered around the concept of ``generative vokens\". These vokens serve as pivotal elements contributing to coherent image-text outputs. Our method is marked by a unique two-stage training strategy for description-free multimodal generation, which does not necessitate extensive descriptions of images. We integrate classifier-free guidance to enhance the alignment of generated images and texts, ensuring more seamless and contextually relevant multimodal interactions. Our model, MiniGPT-5, exhibits substantial improvement over the baseline models on multimodal generation datasets, including MMDialog and VIST. The human evaluation shows MiniGPT-5 is better than the baseline model on more than 56\\\\% cases for multimodal generation, highlighting its efficacy across diverse benchmarks.'), Document(metadata={'title': 'Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond'}, page_content='In this study, we explore the potential of Multimodal Large Language Models (MLLMs) in improving embodied decision-making processes for agents. While Large Language Models (LLMs) have been widely used due to their advanced reasoning skills and vast world knowledge, MLLMs like GPT4-Vision offer enhanced visual understanding and reasoning capabilities. We investigate whether state-of-the-art MLLMs can handle embodied decision-making in an end-to-end manner and whether collaborations between LLMs and MLLMs can enhance decision-making. To address these questions, we introduce a new benchmark called PCA-EVAL, which evaluates embodied decision-making from the perspectives of Perception, Cognition, and Action. Additionally, we propose HOLMES, a multi-agent cooperation framework that allows LLMs to leverage MLLMs and APIs to gather multimodal information for informed decision-making. We compare end-to-end embodied decision-making and HOLMES on our benchmark and find that the GPT4-Vision model demonstrates strong end-to-end embodied decision-making abilities, outperforming GPT4-HOLMES in terms of average decision accuracy (+3%). However, this performance is exclusive to the latest GPT4-Vision model, surpassing the open-source state-of-the-art MLLM by 26%. Our results indicate that powerful MLLMs like GPT4-Vision hold promise for decision-making in embodied agents, offering new avenues for MLLM research. Code and data are open at https://github.com/pkunlp-icler/PCA-EVAL/.'), Document(metadata={'title': 'Kosmos-G: Generating Images in Context with Multimodal Large Language Models'}, page_content='Recent advancements in subject-driven image generation have made significant strides. However, current methods still fall short in diverse application scenarios, as they require test-time tuning and cannot accept interleaved multi-image and text input. These limitations keep them far from the ultimate goal of \"image as a foreign language in image generation.\" This paper presents Kosmos-G, a model that leverages the advanced multimodal perception capabilities of Multimodal Large Language Models (MLLMs) to tackle the aforementioned challenge. Our approach aligns the output space of MLLM with CLIP using the textual modality as an anchor and performs compositional instruction tuning on curated data. Kosmos-G demonstrates an impressive capability of zero-shot subject-driven generation with interleaved multi-image and text input. Notably, the score distillation instruction tuning requires no modifications to the image decoder. This allows for a seamless substitution of CLIP and effortless integration with a myriad of U-Net techniques ranging from fine-grained controls to personalized image decoder variants. We posit Kosmos-G as an initial attempt towards the goal of \"image as a foreign language in image generation.\" The code can be found at https://aka.ms/Kosmos-G'), Document(metadata={'title': 'Fine-grained Audio-Visual Joint Representations for Multimodal Large Language Models'}, page_content='Audio-visual large language models (LLM) have drawn significant attention, yet the fine-grained combination of both input streams is rather under-explored, which is challenging but necessary for LLMs to understand general video inputs. To this end, a fine-grained audio-visual joint representation (FAVOR) learning framework for multimodal LLMs is proposed in this paper, which extends a text-based LLM to simultaneously perceive speech and audio events in the audio input stream and images or videos in the visual input stream, at the frame level. To fuse the audio and visual feature streams into joint representations and to align the joint space with the LLM input embedding space, we propose a causal Q-Former structure with a causal attention module to enhance the capture of causal relations of the audio-visual frames across time. An audio-visual evaluation benchmark (AVEB) is also proposed which comprises six representative single-modal tasks with five cross-modal tasks reflecting audio-visual co-reasoning abilities. While achieving competitive single-modal performance on audio, speech and image tasks in AVEB, FAVOR achieved over 20% accuracy improvements on the video question-answering task when fine-grained information or temporal causal reasoning is required. FAVOR, in addition, demonstrated remarkable video comprehension and reasoning abilities on tasks that are unprecedented by other multimodal LLMs. An interactive demo of FAVOR is available at https://github.com/BriansIDP/AudioVisualLLM.git, and the training code and model checkpoints will be released soon.'), Document(metadata={'title': 'Can We Edit Multimodal Large Language Models?'}, page_content='In this paper, we focus on editing Multimodal Large Language Models (MLLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights. Code and dataset are available in https://github.com/zjunlp/EasyEdit.'), Document(metadata={'title': 'AutoRepo: A general framework for multi-modal LLM-based automated construction reporting'}, page_content='Ensuring the safety, quality, and timely completion of construction projects is paramount, with construction inspections serving as a vital instrument towards these goals. Nevertheless, the predominantly manual approach of present-day inspections frequently results in inefficiencies and inadequate information management. Such methods often fall short of providing holistic, exhaustive assessments, consequently engendering regulatory oversights and potential safety hazards. To address this issue, this paper presents a novel framework named AutoRepo for automated generation of construction inspection reports. The unmanned vehicles efficiently perform construction inspections and collect scene information, while the multimodal large language models (LLMs) are leveraged to automatically generate the inspection reports. The framework was applied and tested on a real-world construction site, demonstrating its potential to expedite the inspection process, significantly reduce resource allocation, and produce high-quality, regulatory standard-compliant inspection reports. This research thus underscores the immense potential of multimodal large language models in revolutionizing construction inspection practices, signaling a significant leap forward towards a more efficient and safer construction management paradigm.'), Document(metadata={'title': 'MM-BigBench: Evaluating Multimodal Models on Multimodal Content Comprehension Tasks'}, page_content=\"The popularity of multimodal large language models (MLLMs) has triggered a recent surge in research efforts dedicated to evaluating these models. Nevertheless, existing evaluation studies of MLLMs primarily focus on the comprehension and reasoning of unimodal (vision) content, neglecting performance evaluations in the domain of multimodal (vision-language) content understanding. Beyond multimodal reasoning, tasks related to multimodal content comprehension necessitate a profound understanding of multimodal contexts, achieved through the multimodal interaction to obtain a final answer. In this paper, we introduce a comprehensive assessment framework called MM-BigBench, which incorporates a diverse range of metrics to offer an extensive evaluation of the performance of various models and instructions across a wide spectrum of diverse multimodal content comprehension tasks. Consequently, our work complements research on the performance of MLLMs in multimodal comprehension tasks, achieving a more comprehensive and holistic evaluation of MLLMs. To begin, we employ the Best Performance metric to ascertain each model's performance upper bound on different datasets. Subsequently, the Mean Relative Gain metric offers an assessment of the overall performance of various models and instructions, while the Stability metric measures their sensitivity. Furthermore, previous research centers on evaluating models independently or solely assessing instructions, neglecting the adaptability between models and instructions. We propose the Adaptability metric to quantify the adaptability between models and instructions. Our paper evaluates a total of 20 language models (14 MLLMs) on 14 multimodal datasets spanning 6 tasks, with 10 instructions for each task, and derives novel insights. Our code will be released at https://github.com/declare-lab/MM-BigBench.\"), Document(metadata={'title': 'RoboLLM: Robotic Vision Tasks Grounded on Multimodal Large Language Models'}, page_content='Robotic vision applications often necessitate a wide range of visual perception tasks, such as object detection, segmentation, and identification. While there have been substantial advances in these individual tasks, integrating specialized models into a unified vision pipeline presents significant engineering challenges and costs. Recently, Multimodal Large Language Models (MLLMs) have emerged as novel backbones for various downstream tasks. We argue that leveraging the pre-training capabilities of MLLMs enables the creation of a simplified framework, thus mitigating the need for task-specific encoders. Specifically, the large-scale pretrained knowledge in MLLMs allows for easier fine-tuning to downstream robotic vision tasks and yields superior performance. We introduce the RoboLLM framework, equipped with a BEiT-3 backbone, to address all visual perception tasks in the ARMBench challenge-a large-scale robotic manipulation dataset about real-world warehouse scenarios. RoboLLM not only outperforms existing baselines but also substantially reduces the engineering burden associated with model selection and tuning. The source code is publicly available at https://github.com/longkukuhi/armbench.'), Document(metadata={'title': 'Benchmarking Sequential Visual Input Reasoning and Prediction in Multimodal Large Language Models'}, page_content=\"Multimodal large language models (MLLMs) have shown great potential in perception and interpretation tasks, but their capabilities in predictive reasoning remain under-explored. To address this gap, we introduce a novel benchmark that assesses the predictive reasoning capabilities of MLLMs across diverse scenarios. Our benchmark targets three important domains: abstract pattern reasoning, human activity prediction, and physical interaction prediction. We further develop three evaluation methods powered by large language model to robustly quantify a model's performance in predicting and reasoning the future based on multi-visual context. Empirical experiments confirm the soundness of the proposed benchmark and evaluation methods via rigorous testing and reveal pros and cons of current popular MLLMs in the task of predictive reasoning. Lastly, our proposed benchmark provides a standardized evaluation framework for MLLMs and can facilitate the development of more advanced models that can reason and predict over complex long sequence of multimodal input.\"), Document(metadata={'title': 'Woodpecker: Hallucination Correction for Multimodal Large Language Models'}, page_content='Hallucination is a big shadow hanging over the rapidly evolving Multimodal Large Language Models (MLLMs), referring to the phenomenon that the generated text is inconsistent with the image content. In order to mitigate hallucinations, existing studies mainly resort to an instruction-tuning manner that requires retraining the models with specific data. In this paper, we pave a different way, introducing a training-free method named Woodpecker. Like a woodpecker heals trees, it picks out and corrects hallucinations from the generated text. Concretely, Woodpecker consists of five stages: key concept extraction, question formulation, visual knowledge validation, visual claim generation, and hallucination correction. Implemented in a post-remedy manner, Woodpecker can easily serve different MLLMs, while being interpretable by accessing intermediate outputs of the five stages. We evaluate Woodpecker both quantitatively and qualitatively and show the huge potential of this new paradigm. On the POPE benchmark, our method obtains a 30.66%/24.33% improvement in accuracy over the baseline MiniGPT-4/mPLUG-Owl. The source code is released at https://github.com/BradyFU/Woodpecker.'), Document(metadata={'title': 'Towards Perceiving Small Visual Details in Zero-shot Visual Question Answering with Multimodal LLMs'}, page_content=\"Multimodal Large Language Models (MLLMs) have recently achieved promising zero-shot accuracy on visual question answering (VQA) -- a fundamental task affecting various downstream applications and domains. Given the great potential for the broad use of these models, it is important to investigate their limitations in dealing with different image and question properties. In this work, we investigate whether MLLMs can perceive small details as well as large details in images. In particular, we show that their zero-shot accuracy in answering visual questions is very sensitive to the size of the visual subject of the question, declining up to 46% with size. Furthermore, we show that this effect is causal by observing that human visual cropping can significantly mitigate their sensitivity to size. Inspired by the usefulness of human cropping, we then propose five automatic visual cropping methods -- leveraging either external localization models or the decision process of the given MLLM itself -- as inference time mechanisms to improve the zero-shot performance of MLLMs. We study their effectiveness on four popular VQA datasets, and a subset of the VQAv2 dataset tailored towards fine visual details. Our findings suggest that MLLMs should be used with caution in detail-sensitive VQA applications, and that visual cropping is a promising direction to improve their zero-shot performance. To facilitate further investigation of MLLMs' behaviors, our code and data are publicly released.\"), Document(metadata={'title': 'ChEF: A Comprehensive Evaluation Framework for Standardized Assessment of Multimodal Large Language Models'}, page_content=\"Multimodal Large Language Models (MLLMs) have shown impressive abilities in interacting with visual content with myriad potential downstream tasks. However, even though a list of benchmarks has been proposed, the capabilities and limitations of MLLMs are still not comprehensively understood, due to a lack of a standardized and holistic evaluation framework. To this end, we present the first Comprehensive Evaluation Framework (ChEF) that can holistically profile each MLLM and fairly compare different MLLMs. First, we structure ChEF as four modular components, i.e., Scenario as scalable multimodal datasets, Instruction as flexible instruction retrieving formulae, Inferencer as reliable question answering strategies, and Metric as indicative task-specific score functions. Based on them, ChEF facilitates versatile evaluations in a standardized framework, and new evaluations can be built by designing new Recipes (systematic selection of these four components). Notably, current MLLM benchmarks can be readily summarized as recipes of ChEF. Second, we introduce 6 new recipes to quantify competent MLLMs' desired capabilities (or called desiderata, i.e., calibration, in-context learning, instruction following, language performance, hallucination, and robustness) as reliable agents that can perform real-world multimodal interactions. Third, we conduct a large-scale evaluation of 9 prominent MLLMs on 9 scenarios and 6 desiderata. Our evaluation summarized over 20 valuable observations concerning the generalizability of MLLMs across various scenarios and the composite capability of MLLMs required for multimodal interactions. We will publicly release all the detailed implementations for further analysis, as well as an easy-to-use modular toolkit for the integration of new recipes and models, so that ChEF can be a growing evaluation framework for the MLLM community.\"), Document(metadata={'title': 'Octavius: Mitigating Task Interference in MLLMs via LoRA-MoE'}, page_content='Recent studies have demonstrated Large Language Models (LLMs) can extend their zero-shot generalization capabilities to multimodal learning through instruction tuning. As more modalities and downstream tasks are introduced, negative conflicts and interference may have a worse impact on performance. While this phenomenon has been overlooked in previous work, we propose a novel and extensible framework, called Octavius, for comprehensive studies and experimentation on multimodal learning with Multimodal Large Language Models (MLLMs). Specifically, we combine the well-known Mixture-of-Experts (MoE) and one of the representative PEFT techniques, i.e., LoRA, designing a novel LLM-based decoder, called LoRA-MoE, for multimodal learning. To the best of our knowledge, we are one of the pioneering efforts to introduce MoE into MLLMs to address this problem. The experimental results (about 20% improvement) have shown the effectiveness and versatility of our design in various 2D and 3D downstream tasks. Code and datasets are available at https://openlamm.github.io/tutorial/.'), Document(metadata={'title': 'What Large Language Models Bring to Text-rich VQA?'}, page_content=\"Text-rich VQA, namely Visual Question Answering based on text recognition in the images, is a cross-modal task that requires both image comprehension and text recognition. In this work, we focus on investigating the advantages and bottlenecks of LLM-based approaches in addressing this problem. To address the above concern, we separate the vision and language modules, where we leverage external OCR models to recognize texts in the image and Large Language Models (LLMs) to answer the question given texts. The whole framework is training-free benefiting from the in-context ability of LLMs. This pipeline achieved superior performance compared to the majority of existing Multimodal Large Language Models (MLLM) on four text-rich VQA datasets. Besides, based on the ablation study, we find that LLM brings stronger comprehension ability and may introduce helpful knowledge for the VQA problem. The bottleneck for LLM to address text-rich VQA problems may primarily lie in visual part. We also combine the OCR module with MLLMs and pleasantly find that the combination of OCR module with MLLM also works. It's worth noting that not all MLLMs can comprehend the OCR information, which provides insights into how to train an MLLM that preserves the abilities of LLM.\")], 'results': [Document(id='9e9ac81d-692f-441b-8926-4b6ba73ad784', metadata={'title': 'A Survey on Multimodal Large Language Models'}, page_content='Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.'), Document(id='128c9e1b-adb3-49e0-a873-881354331b12', metadata={'title': 'A Survey on Multimodal Large Language Models'}, page_content='Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.'), Document(id='01a7f21a-97a8-45a2-a73c-74526485470f', metadata={'title': 'A Survey on Multimodal Large Language Models'}, page_content='Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.')], 'messages': [], 'retry_count': 2}\n{'query': \"latest advances in multimodal learning Consider User's response for the search query: I dont want to know about latest advancements, I want to know about multimodal llms\", 'search_query': 'multimodal large language models', 'papers': [{'id': 'multimodal-large-language-models-for', 'arxiv_id': None, 'nips_id': None, 'url_abs': 'https://aclanthology.org/2022.naacl-srw.26', 'url_pdf': 'https://aclanthology.org/2022.naacl-srw.26.pdf', 'title': 'Multimodal large language models for inclusive collaboration learning tasks', 'abstract': 'This PhD project leverages advancements in multimodal large language models to build an inclusive collaboration feedback loop, in order to facilitate the automated detection, modeling, and feedback for participants developing general collaboration skills. This topic is important given the role of collaboration as an essential 21st century skill, the potential to ground large language models within learning theory and real-world practice, and the expressive potential of transformer models to support equity and inclusion. We address some concerns of integrating advances in natural language processing into downstream tasks such as the learning analytics feedback loop.', 'authors': ['Armanda Lewis'], 'published': None, 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': 'naacl-acl-2022-7'}, {'id': 'personality-aware-human-centric-multimodal', 'arxiv_id': '2304.02313', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2304.02313v2', 'url_pdf': 'https://arxiv.org/pdf/2304.02313v2.pdf', 'title': 'Personality-aware Human-centric Multimodal Reasoning: A New Task, Dataset and Baselines', 'abstract': \"Personality traits, emotions, and beliefs shape individuals' behavioral choices and decision-making processes. However, for one thing, the affective computing community normally focused on predicting personality traits but overlooks their application in behavior prediction. For another, the multimodal reasoning task emphasized the prediction of future states and behaviors but often neglected the incorporation of individual personality traits. In this work, we introduce a new task called Personality-aware Human-centric Multimodal Reasoning (PHMR) (T1), with the goal of forecasting the future behavior of a particular individual using multimodal information from past instances, while integrating personality factors. We accordingly construct a new dataset based on six television shows, encompassing 225 characters and 12k samples. To establish a benchmark for the task, we propose seven baseline methods: three adapted from related tasks, two pre-trained model, and two multimodal large language models. The experimental results demonstrate that incorporating personality traits enhances human-centric multimodal reasoning performance. To further solve the lack of personality annotation in real-life scenes, we introduce an extension task called Personality-predicted Human-centric Multimodal Reasoning task (T2) along with the corresponding dataset and method. We will make our dataset and code available on GitHub.\", 'authors': ['Rui Xia', 'Xiangqing Shen', 'Yaochen Zhu'], 'published': '2023-04-05', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'visual-transformation-telling', 'arxiv_id': '2305.01928', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2305.01928v2', 'url_pdf': 'https://arxiv.org/pdf/2305.01928v2.pdf', 'title': 'Visual Transformation Telling', 'abstract': 'Humans can naturally reason from superficial state differences (e.g. ground wetness) to transformations descriptions (e.g. raining) according to their life experience. In this paper, we propose a new visual reasoning task to test this transformation reasoning ability in real-world scenarios, called \\\\textbf{V}isual \\\\textbf{T}ransformation \\\\textbf{T}elling (VTT). Given a series of states (i.e. images), VTT requires to describe the transformation occurring between every two adjacent states. Different from existing visual reasoning tasks that focus on surface state reasoning, the advantage of VTT is that it captures the underlying causes, e.g. actions or events, behind the differences among states. We collect a novel dataset to support the study of transformation reasoning from two existing instructional video datasets, CrossTask and COIN, comprising 13,547 samples. Each sample involves the key state images along with their transformation descriptions. Our dataset covers diverse real-world activities, providing a rich resource for training and evaluation. To construct an initial benchmark for VTT, we test several models, including traditional visual storytelling methods (CST, GLACNet, Densecap) and advanced multimodal large language models (LLaVA v1.5-7B, Qwen-VL-chat, Gemini Pro Vision, GPT-4o, and GPT-4). Experimental results reveal that even state-of-the-art models still face challenges in VTT, highlighting substantial areas for improvement.', 'authors': ['Liang Pang', 'Xin Hong', 'Wanqing Cui', 'Xueqi Cheng', 'Jiafeng Guo', 'Yanyan Lan'], 'published': '2023-05-03', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'listen-think-and-understand', 'arxiv_id': '2305.10790', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2305.10790v3', 'url_pdf': 'https://arxiv.org/pdf/2305.10790v3.pdf', 'title': 'Listen, Think, and Understand', 'abstract': 'The ability of artificial intelligence (AI) systems to perceive and comprehend audio signals is crucial for many applications. Although significant progress has been made in this area since the development of AudioSet, most existing models are designed to map audio inputs to pre-defined, discrete sound label sets. In contrast, humans possess the ability to not only classify sounds into general categories, but also to listen to the finer details of the sounds, explain the reason for the predictions, think about what the sound infers, and understand the scene and what action needs to be taken, if any. Such capabilities beyond perception are not yet present in existing audio models. On the other hand, modern large language models (LLMs) exhibit emerging reasoning ability but they lack audio perception capabilities. Therefore, we ask the question: can we build a model that has both audio perception and a reasoning ability? In this paper, we propose a new audio foundation model, called LTU (Listen, Think, and Understand). To train LTU, we created a new OpenAQA-5M dataset consisting of 1.9 million closed-ended and 3.7 million open-ended, diverse (audio, question, answer) tuples, and have used an autoregressive training framework with a perception-to-understanding curriculum. LTU demonstrates strong performance and generalization ability on conventional audio tasks such as classification and captioning. More importantly, it exhibits emerging audio reasoning and comprehension abilities that are absent in existing audio models. To the best of our knowledge, LTU is one of the first multimodal large language models that focus on general audio (rather than just speech) understanding.', 'authors': ['James Glass', 'Leonid Karlinsky', 'Alexander H. Liu', 'Hongyin Luo', 'Yuan Gong'], 'published': '2023-05-18', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'what-makes-for-good-visual-tokenizers-for', 'arxiv_id': '2305.12223', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2305.12223v2', 'url_pdf': 'https://arxiv.org/pdf/2305.12223v2.pdf', 'title': 'What Makes for Good Visual Tokenizers for Large Language Models?', 'abstract': 'We empirically investigate proper pre-training methods to build good visual tokenizers, making Large Language Models (LLMs) powerful Multimodal Large Language Models (MLLMs). In our benchmark, which is curated to evaluate MLLMs visual semantic understanding and fine-grained perception capabilities, we discussed different visual tokenizers pre-trained with dominant methods (i.e., DeiT, CLIP, MAE, DINO), and observe that: i) Fully/weakly supervised models capture more semantics than self-supervised models, but the gap is narrowed by scaling up the pre-training dataset. ii) Self-supervised models are better at fine-grained perception, where patch-level supervision is particularly effective. iii) Tuning the visual tokenizer leads to the loss of semantics obtained from large-scale pretraining, which is unfavorable with relatively small-scale instruction-tuning dataset. Given the findings, we reviewed methods that attempted to unify semantics and fine-grained visual understanding, e.g., patch-level feature distillation with semantically-rich targets. We obtain an intriguing insight mask-based strategies that were once all the rage may not be applicable for obtaining good visual tokenizers. Based on this critical observation, we obtain a new MLLM equipped with a tailored Good Visual Tokenizer (GVT), which exhibits strong visual comprehension capability at multiple scales. In particular, without introducing extra parameters and task-specific fine-tuning, GVT achieves superior performance on visual question answering, image captioning, and other fine-grained visual understanding tasks such as object counting and multi-class identification.', 'authors': ['Ying Shan', 'Mohan Kankanhalli', 'Xiaohan Ding', 'Yixiao Ge', 'Guangzhi Wang'], 'published': '2023-05-20', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'interpretable-by-design-visual-question', 'arxiv_id': '2305.14882', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2305.14882v2', 'url_pdf': 'https://arxiv.org/pdf/2305.14882v2.pdf', 'title': 'Dynamic Clue Bottlenecks: Towards Interpretable-by-Design Visual Question Answering', 'abstract': 'Recent advances in multimodal large language models (LLMs) have shown extreme effectiveness in visual question answering (VQA). However, the design nature of these end-to-end models prevents them from being interpretable to humans, undermining trust and applicability in critical domains. While post-hoc rationales offer certain insight into understanding model behavior, these explanations are not guaranteed to be faithful to the model. In this paper, we address these shortcomings by introducing an interpretable by design model that factors model decisions into intermediate human-legible explanations, and allows people to easily understand why a model fails or succeeds. We propose the Dynamic Clue Bottleneck Model ( (DCLUB), a method that is designed towards an inherently interpretable VQA system. DCLUB provides an explainable intermediate space before the VQA decision and is faithful from the beginning, while maintaining comparable performance to black-box systems. Given a question, DCLUB first returns a set of visual clues: natural language statements of visually salient evidence from the image, and then generates the output based solely on the visual clues. To supervise and evaluate the generation of VQA explanations within DCLUB, we collect a dataset of 1.7k reasoning-focused questions with visual clues. Evaluations show that our inherently interpretable system can improve 4.64% over a comparable black-box system in reasoning-focused questions while preserving 99.43% of performance on VQA-v2.', 'authors': ['Dan Roth', 'Mark Yatskar', 'Sihao Chen', 'Ben Zhou', 'Xingyu Fu'], 'published': '2023-05-24', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'pathasst-redefining-pathology-through', 'arxiv_id': '2305.15072', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2305.15072v2', 'url_pdf': 'https://arxiv.org/pdf/2305.15072v2.pdf', 'title': 'PathAsst: A Generative Foundation AI Assistant Towards Artificial General Intelligence of Pathology', 'abstract': \"As advances in large language models (LLMs) and multimodal techniques continue to mature, the development of general-purpose multimodal large language models (MLLMs) has surged, offering significant applications in interpreting natural images. However, the field of pathology has largely remained untapped, particularly in gathering high-quality data and designing comprehensive model frameworks. To bridge the gap in pathology MLLMs, we present PathAsst, a multimodal generative foundation AI assistant to revolutionize diagnostic and predictive analytics in pathology. The development of PathAsst involves three pivotal steps: data acquisition, CLIP model adaptation, and the training of PathAsst's multimodal generative capabilities. Firstly, we collect over 207K high-quality pathology image-text pairs from authoritative sources. Leveraging the advanced power of ChatGPT, we generate over 180K instruction-following samples. Furthermore, we devise additional instruction-following data specifically tailored for invoking eight pathology-specific sub-models we prepared, allowing the PathAsst to effectively collaborate with these models, enhancing its diagnostic ability. Secondly, by leveraging the collected data, we construct PathCLIP, a pathology-dedicated CLIP, to enhance PathAsst's capabilities in interpreting pathology images. Finally, we integrate PathCLIP with the Vicuna-13b and utilize pathology-specific instruction-tuning data to enhance the multimodal generation capacity of PathAsst and bolster its synergistic interactions with sub-models. The experimental results of PathAsst show the potential of harnessing AI-powered generative foundation model to improve pathology diagnosis and treatment processes.\", 'authors': ['Yunlong Zhang', 'Zhongyi Shui', 'Lin Sun', 'Lin Yang', 'Honglin Li', 'Kai Zhang', 'Sunyi Zheng', 'Chenglu Zhu', 'Yuxuan Sun'], 'published': '2023-05-24', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'contextual-object-detection-with-multimodal', 'arxiv_id': '2305.18279', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2305.18279v2', 'url_pdf': 'https://arxiv.org/pdf/2305.18279v2.pdf', 'title': 'Contextual Object Detection with Multimodal Large Language Models', 'abstract': 'Recent Multimodal Large Language Models (MLLMs) are remarkable in vision-language tasks, such as image captioning and question answering, but lack the essential perception ability, i.e., object detection. In this work, we address this limitation by introducing a novel research problem of contextual object detection -- understanding visible objects within different human-AI interactive contexts. Three representative scenarios are investigated, including the language cloze test, visual captioning, and question answering. Moreover, we present ContextDET, a unified multimodal model that is capable of end-to-end differentiable modeling of visual-language contexts, so as to locate, identify, and associate visual objects with language inputs for human-AI interaction. Our ContextDET involves three key submodels: (i) a visual encoder for extracting visual representations, (ii) a pre-trained LLM for multimodal context decoding, and (iii) a visual decoder for predicting bounding boxes given contextual object words. The new generate-then-detect framework enables us to detect object words within human vocabulary. Extensive experiments show the advantages of ContextDET on our proposed CODE benchmark, open-vocabulary detection, and referring image segmentation. Github: https://github.com/yuhangzang/ContextDET.', 'authors': ['Chen Change Loy', 'Kaiyang Zhou', 'Jun Han', 'Wei Li', 'Yuhang Zang'], 'published': '2023-05-29', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'a-survey-on-multimodal-large-language-models', 'arxiv_id': '2306.13549', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2306.13549v4', 'url_pdf': 'https://arxiv.org/pdf/2306.13549v4.pdf', 'title': 'A Survey on Multimodal Large Language Models', 'abstract': 'Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.', 'authors': ['Enhong Chen', 'Tong Xu', 'Xing Sun', 'Ke Li', 'Sirui Zhao', 'Chaoyou Fu', 'Shukang Yin'], 'published': '2023-06-23', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'mme-a-comprehensive-evaluation-benchmark-for', 'arxiv_id': '2306.13394', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2306.13394v4', 'url_pdf': 'https://arxiv.org/pdf/2306.13394v4.pdf', 'title': 'MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models', 'abstract': 'Multimodal Large Language Model (MLLM) relies on the powerful LLM to perform multimodal tasks, showing amazing emergent abilities in recent studies, such as writing poems based on an image. However, it is difficult for these case studies to fully reflect the performance of MLLM, lacking a comprehensive evaluation. In this paper, we fill in this blank, presenting the first comprehensive MLLM Evaluation benchmark MME. It measures both perception and cognition abilities on a total of 14 subtasks. In order to avoid data leakage that may arise from direct use of public datasets for evaluation, the annotations of instruction-answer pairs are all manually designed. The concise instruction design allows us to fairly compare MLLMs, instead of struggling in prompt engineering. Besides, with such an instruction, we can also easily carry out quantitative statistics. A total of 30 advanced MLLMs are comprehensively evaluated on our MME, which not only suggests that existing MLLMs still have a large room for improvement, but also reveals the potential directions for the subsequent model optimization. The data application manner and online leaderboards are released at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation.', 'authors': ['Rongrong Ji', 'Yunsheng Wu', 'Jinrui Yang', 'Xing Sun', 'Ke Li', 'Xiawu Zheng', 'Xu Lin', 'Mengdan Zhang', 'Yulei Qin', 'Yunhang Shen', 'Peixian Chen', 'Chaoyou Fu'], 'published': '2023-06-23', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'kosmos-2-grounding-multimodal-large-language', 'arxiv_id': '2306.14824', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2306.14824v3', 'url_pdf': 'https://arxiv.org/pdf/2306.14824v3.pdf', 'title': 'Kosmos-2: Grounding Multimodal Large Language Models to the World', 'abstract': \"We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. Specifically, we represent refer expressions as links in Markdown, i.e., ``[text span](bounding boxes)'', where object descriptions are sequences of location tokens. Together with multimodal corpora, we construct large-scale data of grounded image-text pairs (called GrIT) to train the model. In addition to the existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), Kosmos-2 integrates the grounding capability into downstream applications. We evaluate Kosmos-2 on a wide range of tasks, including (i) multimodal grounding, such as referring expression comprehension, and phrase grounding, (ii) multimodal referring, such as referring expression generation, (iii) perception-language tasks, and (iv) language understanding and generation. This work lays out the foundation for the development of Embodiment AI and sheds light on the big convergence of language, multimodal perception, action, and world modeling, which is a key step toward artificial general intelligence. Code and pretrained models are available at https://aka.ms/kosmos-2.\", 'authors': ['Furu Wei', 'Shuming Ma', 'Shaohan Huang', 'Yaru Hao', 'Li Dong', 'Wenhui Wang', 'Zhiliang Peng'], 'published': '2023-06-26', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'shikra-unleashing-multimodal-llm-s', 'arxiv_id': '2306.15195', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2306.15195v2', 'url_pdf': 'https://arxiv.org/pdf/2306.15195v2.pdf', 'title': \"Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic\", 'abstract': \"In human conversations, individuals can indicate relevant regions within a scene while addressing others. In turn, the other person can then respond by referring to specific regions if necessary. This natural referential ability in dialogue remains absent in current Multimodal Large Language Models (MLLMs). To fill this gap, this paper proposes an MLLM called Shikra, which can handle spatial coordinate inputs and outputs in natural language. Its architecture consists of a vision encoder, an alignment layer, and a LLM. It is designed to be straightforward and simple, without the need for extra vocabularies, position encoder, pre-/post-detection modules, or external plug-in models. All inputs and outputs are in natural language form. Referential dialogue is a superset of various vision-language (VL) tasks. Shikra can naturally handle location-related tasks like REC and PointQA, as well as conventional VL tasks such as Image Captioning and VQA. Experimental results showcase Shikra's promising performance. Furthermore, it enables numerous exciting applications, like providing mentioned objects' coordinates in chains of thoughts and comparing user-pointed regions similarities. Our code, model and dataset are accessed at https://github.com/shikras/shikra.\", 'authors': ['Rui Zhao', 'Feng Zhu', 'Richong Zhang', 'Weili Zeng', 'Zhao Zhang', 'Keqin Chen'], 'published': '2023-06-27', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'svit-scaling-up-visual-instruction-tuning', 'arxiv_id': '2307.04087', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2307.04087v3', 'url_pdf': 'https://arxiv.org/pdf/2307.04087v3.pdf', 'title': 'SVIT: Scaling up Visual Instruction Tuning', 'abstract': \"Thanks to the emerging of foundation models, the large language and vision models are integrated to acquire the multimodal ability of visual captioning, question answering, etc. Although existing multimodal models present impressive performance of visual understanding and reasoning, their limits are still largely under-explored due to the scarcity of high-quality instruction tuning data. To push the limits of multimodal capability, we Scale up Visual Instruction Tuning (SVIT) by constructing a dataset of 4.2 million visual instruction tuning data including 1.6M conversation question-answer (QA) pairs, 1.6M complex reasoning QA pairs, 1.0M referring QA pairs and 106K detailed image descriptions. Besides the volume, the proposed dataset is also featured by the high quality and rich diversity, which is generated by prompting GPT-4 with the abundant manual annotations of images. We also propose a new data recipe to select subset with better diversity and balance, which evokes model's superior capabilities. Extensive experiments verify that SVIT-v1.5, trained on the proposed dataset, outperforms state-of-the-art Multimodal Large Language Models on popular benchmarks. The data and code are publicly available at https://github.com/BAAI-DCAI/Visual-Instruction-Tuning.\", 'authors': ['Tiejun Huang', 'Muyang He', 'Boya Wu', 'Bo Zhao'], 'published': '2023-07-09', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'chatspot-bootstrapping-multimodal-llms-via', 'arxiv_id': '2307.09474', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2307.09474v1', 'url_pdf': 'https://arxiv.org/pdf/2307.09474v1.pdf', 'title': 'ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning', 'abstract': \"Human-AI interactivity is a critical aspect that reflects the usability of multimodal large language models (MLLMs). However, existing end-to-end MLLMs only allow users to interact with them through language instructions, leading to the limitation of the interactive accuracy and efficiency. In this study, we present precise referring instructions that utilize diverse reference representations such as points and boxes as referring prompts to refer to the special region. This enables MLLMs to focus on the region of interest and achieve finer-grained interaction. Based on precise referring instruction, we propose ChatSpot, a unified end-to-end multimodal large language model that supports diverse forms of interactivity including mouse clicks, drag-and-drop, and drawing boxes, which provides a more flexible and seamless interactive experience. We also construct a multi-grained vision-language instruction-following dataset based on existing datasets and GPT-4 generating. Furthermore, we design a series of evaluation tasks to assess the effectiveness of region recognition and interaction. Experimental results showcase ChatSpot's promising performance.\", 'authors': ['Xiangyu Zhang', 'Chunrui Han', 'Runpei Dong', 'Yuang Peng', 'Jianjian Sun', 'HongYu Zhou', 'Haoran Wei', 'Jinrong Yang', 'Zheng Ge', 'En Yu', 'Liang Zhao'], 'published': '2023-07-18', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'seed-bench-benchmarking-multimodal-llms-with', 'arxiv_id': '2307.16125', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2307.16125v2', 'url_pdf': 'https://arxiv.org/pdf/2307.16125v2.pdf', 'title': 'SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension', 'abstract': 'Based on powerful Large Language Models (LLMs), recent generative Multimodal Large Language Models (MLLMs) have gained prominence as a pivotal research area, exhibiting remarkable capability for both comprehension and generation. In this work, we address the evaluation of generative comprehension in MLLMs as a preliminary step towards a comprehensive assessment of generative models, by introducing a benchmark named SEED-Bench. SEED-Bench consists of 19K multiple choice questions with accurate human annotations (x 6 larger than existing benchmarks), which spans 12 evaluation dimensions including the comprehension of both the image and video modality. We develop an advanced pipeline for generating multiple-choice questions that target specific evaluation dimensions, integrating both automatic filtering and manual verification processes. Multiple-choice questions with groundtruth options derived from human annotation enables an objective and efficient assessment of model performance, eliminating the need for human or GPT intervention during evaluation. We further evaluate the performance of 18 models across all 12 dimensions, covering both the spatial and temporal understanding. By revealing the limitations of existing MLLMs through evaluation results, we aim for SEED-Bench to provide insights for motivating future research. We will launch and consistently maintain a leaderboard to provide a platform for the community to assess and investigate model capability.', 'authors': ['Ying Shan', 'Yixiao Ge', 'Yuying Ge', 'Guangzhi Wang', 'Rui Wang', 'Bohao Li'], 'published': '2023-07-30', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'lisa-reasoning-segmentation-via-large', 'arxiv_id': '2308.00692', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.00692v3', 'url_pdf': 'https://arxiv.org/pdf/2308.00692v3.pdf', 'title': 'LISA: Reasoning Segmentation via Large Language Model', 'abstract': 'Although perception systems have made remarkable advancements in recent years, they still rely on explicit human instruction or pre-defined categories to identify the target objects before executing visual recognition tasks. Such systems cannot actively reason and comprehend implicit user intention. In this work, we propose a new segmentation task -- reasoning segmentation. The task is designed to output a segmentation mask given a complex and implicit query text. Furthermore, we establish a benchmark comprising over one thousand image-instruction-mask data samples, incorporating intricate reasoning and world knowledge for evaluation purposes. Finally, we present LISA: large Language Instructed Segmentation Assistant, which inherits the language generation capabilities of multimodal Large Language Models (LLMs) while also possessing the ability to produce segmentation masks. We expand the original vocabulary with a <SEG> token and propose the embedding-as-mask paradigm to unlock the segmentation capability. Remarkably, LISA can handle cases involving complex reasoning and world knowledge. Also, it demonstrates robust zero-shot capability when trained exclusively on reasoning-free datasets. In addition, fine-tuning the model with merely 239 reasoning segmentation data samples results in further performance enhancement. Both quantitative and qualitative experiments show our method effectively unlocks new reasoning segmentation capabilities for multimodal LLMs. Code, models, and data are available at https://github.com/dvlab-research/LISA.', 'authors': ['Jiaya Jia', 'Shu Liu', 'Yuhui Yuan', 'Yanwei Li', 'Yukang Chen', 'Zhuotao Tian', 'Xin Lai'], 'published': '2023-08-01', 'conference': None, 'conference_url_abs': 'http://openaccess.thecvf.com//content/CVPR2024/html/Lai_LISA_Reasoning_Segmentation_via_Large_Language_Model_CVPR_2024_paper.html', 'conference_url_pdf': 'http://openaccess.thecvf.com//content/CVPR2024/papers/Lai_LISA_Reasoning_Segmentation_via_Large_Language_Model_CVPR_2024_paper.pdf', 'proceeding': 'cvpr-2024-1'}, {'id': 'empowering-vision-language-models-to-follow', 'arxiv_id': '2308.04152', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.04152v4', 'url_pdf': 'https://arxiv.org/pdf/2308.04152v4.pdf', 'title': 'Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions', 'abstract': \"Recent advancements in Multimodal Large Language Models (MLLMs) have been utilizing Visual Prompt Generators (VPGs) to convert visual features into tokens that LLMs can recognize. This is achieved by training the VPGs on millions of image-caption pairs, where the VPG-generated tokens of images are fed into a frozen LLM to generate the corresponding captions. However, this image-captioning based training objective inherently biases the VPG to concentrate solely on the primary visual contents sufficient for caption generation, often neglecting other visual details. This shortcoming results in MLLMs' underperformance in comprehending demonstrative instructions consisting of multiple, interleaved, and multimodal instructions that demonstrate the required context to complete a task. To address this issue, we introduce a generic and lightweight Visual Prompt Generator Complete module (VPG-C), which can infer and complete the missing details essential for comprehending demonstrative instructions. Further, we propose a synthetic discriminative training strategy to fine-tune VPG-C, eliminating the need for supervised demonstrative instructions. As for evaluation, we build DEMON, a comprehensive benchmark for demonstrative instruction understanding. Synthetically trained with the proposed strategy, VPG-C achieves significantly stronger zero-shot performance across all tasks of DEMON. Further evaluation on the MME and OwlEval benchmarks also demonstrate the superiority of VPG-C. Our benchmark, code, and pre-trained models are available at https://github.com/DCDmllm/Cheetah.\", 'authors': ['Wenqiao Zhang', 'Wei Ji', 'Yueting Zhuang', 'Siliang Tang', 'Tat-Seng Chua', 'Hanwang Zhang', 'Minghe Gao', 'Zhiqi Ge', 'Kaihang Pan', 'Juncheng Li'], 'published': '2023-08-08', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'link-context-learning-for-multimodal-llms', 'arxiv_id': '2308.07891', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.07891v1', 'url_pdf': 'https://arxiv.org/pdf/2308.07891v1.pdf', 'title': 'Link-Context Learning for Multimodal LLMs', 'abstract': 'The ability to learn from context with novel concepts, and deliver appropriate responses are essential in human conversations. Despite current Multimodal Large Language Models (MLLMs) and Large Language Models (LLMs) being trained on mega-scale datasets, recognizing unseen images or understanding novel concepts in a training-free manner remains a challenge. In-Context Learning (ICL) explores training-free few-shot learning, where models are encouraged to ``learn to learn\" from limited tasks and generalize to unseen tasks. In this work, we propose link-context learning (LCL), which emphasizes \"reasoning from cause and effect\" to augment the learning capabilities of MLLMs. LCL goes beyond traditional ICL by explicitly strengthening the causal relationship between the support set and the query set. By providing demonstrations with causal links, LCL guides the model to discern not only the analogy but also the underlying causal associations between data points, which empowers MLLMs to recognize unseen images and understand novel concepts more effectively. To facilitate the evaluation of this novel approach, we introduce the ISEKAI dataset, comprising exclusively of unseen generated image-label pairs designed for link-context learning. Extensive experiments show that our LCL-MLLM exhibits strong link-context learning capabilities to novel concepts over vanilla MLLMs. Code and data will be released at https://github.com/isekai-portal/Link-Context-Learning.', 'authors': ['Ziwei Liu', 'Rui Zhao', 'Feng Zhu', 'Zhao Zhang', 'Weichen Fan', 'Yan Tai'], 'published': '2023-08-15', 'conference': None, 'conference_url_abs': 'http://openaccess.thecvf.com//content/CVPR2024/html/Tai_Link-Context_Learning_for_Multimodal_LLMs_CVPR_2024_paper.html', 'conference_url_pdf': 'http://openaccess.thecvf.com//content/CVPR2024/papers/Tai_Link-Context_Learning_for_Multimodal_LLMs_CVPR_2024_paper.pdf', 'proceeding': 'cvpr-2024-1'}, {'id': 'fashionlogo-prompting-multimodal-large', 'arxiv_id': '2308.09012', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.09012v2', 'url_pdf': 'https://arxiv.org/pdf/2308.09012v2.pdf', 'title': 'FashionLOGO: Prompting Multimodal Large Language Models for Fashion Logo Embeddings', 'abstract': \"Logo embedding models convert the product logos in images into vectors, enabling their utilization for logo recognition and detection within e-commerce platforms. This facilitates the enforcement of intellectual property rights and enhances product search capabilities. However, current methods treat logo embedding as a purely visual problem. A noteworthy issue is that visual models capture features more than logos. Instead, we view this as a multimodal task, using text as auxiliary information to facilitate the visual model's understanding of the logo. The emerging Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in both visual and textual understanding. Inspired by this, we propose an approach, \\\\textbf{FashionLOGO}, to explore how to prompt MLLMs to generate appropriate text for product images, which can help visual models achieve better logo embeddings. We adopt a cross-attention transformer block that enables visual embedding to automatically learn supplementary knowledge from textual embedding. Our extensive experiments on real-world datasets prove that FashionLOGO is capable of generating generic and robust logo embeddings, achieving state-of-the-art performance in all benchmarks.\", 'authors': ['Walton Wang', 'Minghui Qiu', 'Min Yang', 'Yulin Su', 'Da Li', 'Zhen Wang'], 'published': '2023-08-17', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'wanjuan-a-comprehensive-multimodal-dataset', 'arxiv_id': '2308.10755', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.10755v3', 'url_pdf': 'https://arxiv.org/pdf/2308.10755v3.pdf', 'title': 'WanJuan: A Comprehensive Multimodal Dataset for Advancing English and Chinese Large Models', 'abstract': 'The rise in popularity of ChatGPT and GPT-4 has significantly accelerated the development of large models, leading to the creation of numerous impressive large language models(LLMs) and multimodal large language models (MLLMs). These cutting-edge models owe their remarkable performance to high-quality data. However, the details of the training data used in leading paradigms are often kept confidential. This lack of transparency, coupled with the scarcity of open-source data, impedes further developments within the community. As a response, this paper presents \"Wan Juan\", a large-scale multimodal dataset composed of both Chinese and English data, collected from a wide range of web sources. The dataset incorporates text, image-text, and video modalities, with a total volume exceeding 2TB. It was utilized in the training of InternLM, a model that demonstrated significant advantages in multi-dimensional evaluations when compared to models of a similar scale. All data can be accessed at https://opendatalab.org.cn/WanJuan1.0.', 'authors': ['Dahua Lin', 'Jiaqi Wang', 'Hang Yan', 'Wei Li', 'Bin Wang', 'Jiantao Qiu', 'Chao Xu', 'Zhenjiang Jin', 'Conghui He'], 'published': '2023-08-21', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'stablellava-enhanced-visual-instruction', 'arxiv_id': '2308.10253', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.10253v2', 'url_pdf': 'https://arxiv.org/pdf/2308.10253v2.pdf', 'title': 'StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data', 'abstract': \"The remarkable multimodal capabilities demonstrated by OpenAI's GPT-4 have sparked significant interest in the development of multimodal Large Language Models (LLMs). A primary research objective of such models is to align visual and textual modalities effectively while comprehending human instructions. Current methodologies often rely on annotations derived from benchmark datasets to construct image-dialogue datasets for training purposes, akin to instruction tuning in LLMs. However, these datasets often exhibit domain bias, potentially constraining the generative capabilities of the models. In an effort to mitigate these limitations, we propose a novel data collection methodology that synchronously synthesizes images and dialogues for visual instruction tuning. This approach harnesses the power of generative models, marrying the abilities of ChatGPT and text-to-image generative models to yield a diverse and controllable dataset with varied image content. Additionally, datasets can be arbitrarily scaled. This not only provides greater flexibility compared to existing methodologies but also significantly enhances several model capabilities. Our research includes comprehensive experiments conducted on various datasets. The results emphasize substantial enhancements in more than ten commonly assessed capabilities. Additionally, our model achieves state-of-the-art results across multiple widely recognized multimodal benchmarks.\", 'authors': ['Yunchao Wei', 'Ling Chen', 'Chunhua Shen', 'Guosheng Lin', 'Bin Fu', 'Zhibin Wang', 'Gang Yu', 'Chi Zhang', 'Yanda Li'], 'published': '2023-08-20', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'instructiongpt-4-a-200-instruction-paradigm', 'arxiv_id': '2308.12067', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.12067v2', 'url_pdf': 'https://arxiv.org/pdf/2308.12067v2.pdf', 'title': 'InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4', 'abstract': 'Multimodal large language models are typically trained in two stages: first pre-training on image-text pairs, and then fine-tuning using supervised vision-language instruction data. Recent studies have shown that large language models can achieve satisfactory results even with a limited amount of high-quality instruction-following data. In this paper, we introduce InstructionGPT-4, which is fine-tuned on a small dataset comprising only 200 examples, amounting to approximately 6\\\\% of the instruction-following data used in the alignment dataset for MiniGPT-4. To achieve this, we first propose several metrics to access the quality of multimodal instruction data. Based on these metrics, we present an effective and trainable data selector to automatically identify and filter low-quality vision-language data. By employing this method, InstructionGPT-4 outperforms the original MiniGPT-4 on various evaluations. Overall, our findings demonstrate that less but high-quality instruction tuning data is efficient in enabling multimodal large language models to generate better output. Our code is available at https://github.com/waltonfuture/InstructionGPT-4.', 'authors': ['Lichao Sun', 'Weiran Huang', 'Zihao Jiang', 'Lai Wei'], 'published': '2023-08-23', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'vigc-visual-instruction-generation-and', 'arxiv_id': '2308.12714', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.12714v3', 'url_pdf': 'https://arxiv.org/pdf/2308.12714v3.pdf', 'title': 'VIGC: Visual Instruction Generation and Correction', 'abstract': \"The integration of visual encoders and large language models (LLMs) has driven recent progress in multimodal large language models (MLLMs). However, the scarcity of high-quality instruction-tuning data for vision-language tasks remains a challenge. The current leading paradigm, such as LLaVA, relies on language-only GPT-4 to generate data, which requires pre-annotated image captions and detection bounding boxes, suffering from understanding image details. A practical solution to this problem would be to utilize the available multimodal large language models (MLLMs) to generate instruction data for vision-language tasks. However, it's worth noting that the currently accessible MLLMs are not as powerful as their LLM counterparts, as they tend to produce inadequate responses and generate false information. As a solution for addressing the current issue, this paper proposes the Visual Instruction Generation and Correction (VIGC) framework that enables multimodal large language models to generate instruction-tuning data and progressively enhance its quality on-the-fly. Specifically, Visual Instruction Generation (VIG) guides the vision-language model to generate diverse instruction-tuning data. To ensure generation quality, Visual Instruction Correction (VIC) adopts an iterative update mechanism to correct any inaccuracies in data produced by VIG, effectively reducing the risk of hallucination. Leveraging the diverse, high-quality data generated by VIGC, we finetune mainstream models and validate data quality based on various evaluations. Experimental results demonstrate that VIGC not only compensates for the shortcomings of language-only data generation methods, but also effectively enhances the benchmark performance. The models, datasets, and code are available at https://opendatalab.github.io/VIGC.\", 'authors': ['Conghui He', 'Jiaqi Wang', 'Wei Li', 'Weijia Li', 'Xiaoyi Dong', 'Pan Zhang', 'Huaping Zhong', 'Jiahui Peng', 'Xiao Han', 'Fan Wu', 'Bin Wang'], 'published': '2023-08-24', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'position-enhanced-visual-instruction-tuning', 'arxiv_id': '2308.13437', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.13437v2', 'url_pdf': 'https://arxiv.org/pdf/2308.13437v2.pdf', 'title': 'Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models', 'abstract': 'Recently, Multimodal Large Language Models (MLLMs) that enable Large Language Models (LLMs) to interpret images through visual instruction tuning have achieved significant success. However, existing visual instruction tuning methods only utilize image-language instruction data to align the language and image modalities, lacking a more fine-grained cross-modal alignment. In this paper, we propose Position-enhanced Visual Instruction Tuning (PVIT), which extends the functionality of MLLMs by integrating an additional region-level vision encoder. This integration promotes a more detailed comprehension of images for the MLLM. In addition, to efficiently achieve a fine-grained alignment between the vision modules and the LLM, we design multiple data generation strategies to construct an image-region-language instruction dataset. Finally, we present both quantitative experiments and qualitative analysis that demonstrate the superiority of the proposed model. Code and data will be released at https://github.com/PVIT-official/PVIT.', 'authors': ['Yang Liu', 'Maosong Sun', 'Peng Li', 'Xiaoyue Mi', 'Fuwen Luo', 'Ruoyu Qin', 'Chi Chen'], 'published': '2023-08-25', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'mllm-dataengine-an-iterative-refinement', 'arxiv_id': '2308.13566', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.13566v2', 'url_pdf': 'https://arxiv.org/pdf/2308.13566v2.pdf', 'title': 'MLLM-DataEngine: An Iterative Refinement Approach for MLLM', 'abstract': 'Despite the great advance of Multimodal Large Language Models (MLLMs) in both instruction dataset building and benchmarking, the independence of training and evaluation makes current MLLMs hard to further improve their capability under the guidance of evaluation results with a relatively low human cost. In this paper, we propose MLLM-DataEngine, a novel closed-loop system that bridges data generation, model training, and evaluation. Within each loop iteration, the MLLM-DataEngine first analyze the weakness of the model based on the evaluation results, then generate a proper incremental dataset for the next training iteration and enhance the model capability iteratively. Compared with previous data collection methods which are separate from the benchmarking, the data generated by MLLM-DataEngine shows better targeting, quality, and correctness. For targeting, we propose an Adaptive Bad-case Sampling module, which adjusts the ratio of different types of data within each incremental dataset based on the benchmarking results. For quality, we resort to GPT-4 to generate high-quality data with each given data type. For correctness, prompt design is critical for the data generation results. Rather than previous hand-crafted prompt, we propose an Interactive Prompt Optimization strategy, which optimizes the prompt with the multi-round interaction between human and GPT, and improve the correctness of generated data greatly. Through extensive experiments, we find our MLLM-DataEngine could boost the MLLM capability in a targeted and automatic manner, with only a few human participation. We hope it could be a general solution for the following MLLMs building. The MLLM-DataEngine has been open-sourced and is now available at https://github.com/opendatalab/MLLM-DataEngine.', 'authors': ['Conghui He', 'Jiaqi Wang', 'Xiaoyi Dong', 'Pan Zhang', 'Siyuan Huang', 'Bin Wang', 'Linke Ouyang', 'Zhiyuan Zhao'], 'published': '2023-08-25', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'socratis-are-large-multimodal-models', 'arxiv_id': '2308.16741', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.16741v3', 'url_pdf': 'https://arxiv.org/pdf/2308.16741v3.pdf', 'title': 'Socratis: Are large multimodal models emotionally aware?', 'abstract': 'Existing emotion prediction benchmarks contain coarse emotion labels which do not consider the diversity of emotions that an image and text can elicit in humans due to various reasons. Learning diverse reactions to multimodal content is important as intelligent machines take a central role in generating and delivering content to society. To address this gap, we propose Socratis, a societal reactions benchmark, where each image-caption (IC) pair is annotated with multiple emotions and the reasons for feeling them. Socratis contains 18K free-form reactions for 980 emotions on 2075 image-caption pairs from 5 widely-read news and image-caption (IC) datasets. We benchmark the capability of state-of-the-art multimodal large language models to generate the reasons for feeling an emotion given an IC pair. Based on a preliminary human study, we observe that humans prefer human-written reasons over 2 times more often than machine-generated ones. This shows our task is harder than standard generation tasks because it starkly contrasts recent findings where humans cannot tell apart machine vs human-written news articles, for instance. We further see that current captioning metrics based on large vision-language models also fail to correlate with human preferences. We hope that these findings and our benchmark will inspire further research on training emotionally aware models.', 'authors': ['Kate Saenko', 'Bryan A. Plummer', 'Saadia Gabriel', 'Reuben Tan', 'Arijit Ray', 'Katherine Deng'], 'published': '2023-08-31', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'multiway-adapater-adapting-large-scale-multi', 'arxiv_id': '2309.01516', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.01516v3', 'url_pdf': 'https://arxiv.org/pdf/2309.01516v3.pdf', 'title': 'MultiWay-Adapater: Adapting large-scale multi-modal models for scalable image-text retrieval', 'abstract': \"As Multimodal Large Language Models (MLLMs) grow in size, adapting them to specialized tasks becomes increasingly challenging due to high computational and memory demands. Indeed, traditional fine-tuning methods are costly, due to the need for extensive, task-specific training. While efficient adaptation methods exist that aim to reduce these costs, in practice they suffer from shallow inter-modal alignment, which severely hurts model effectiveness. To tackle these computational challenges and improve inter-modal alignment, we introduce the MultiWay-Adapter (MWA), a novel framework featuring an 'Alignment Enhancer'. This enhancer deepens inter-modal alignment, enabling high transferability with minimal tuning effort. Our experiments show that unlike prior efficient tuning approaches, MWA maintains model effectiveness, while reducing training time by up-to 57%. MWA is also lightweight, increasing model size by only 2-3% (in terms of parameters) for state-of-the-art foundation models like BEiT-3 Large. These results demonstrate that MWA provides an efficient and effective adaptation method for MLLMs, significantly broadening their applicability.\", 'authors': ['Gerardo Aragon Camarasa', 'Richard McCreadie', 'George Killick', 'Zijun Long'], 'published': '2023-09-04', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'next-gpt-any-to-any-multimodal-llm', 'arxiv_id': '2309.05519', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.05519v3', 'url_pdf': 'https://arxiv.org/pdf/2309.05519v3.pdf', 'title': 'NExT-GPT: Any-to-Any Multimodal LLM', 'abstract': 'While recently Multimodal Large Language Models (MM-LLMs) have made exciting strides, they mostly fall prey to the limitation of only input-side multimodal understanding, without the ability to produce content in multiple modalities. As we humans always perceive the world and communicate with people through various modalities, developing any-to-any MM-LLMs capable of accepting and delivering content in any modality becomes essential to human-level AI. To fill the gap, we present an end-to-end general-purpose any-to-any MM-LLM system, NExT-GPT. We connect an LLM with multimodal adaptors and different diffusion decoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio. By leveraging the existing well-trained highly-performing encoders and decoders, NExT-GPT is tuned with only a small amount of parameter (1%) of certain projection layers, which not only benefits low-cost training and also facilitates convenient expansion to more potential modalities. Moreover, we introduce a modality-switching instruction tuning (MosIT) and manually curate a high-quality dataset for MosIT, based on which NExT-GPT is empowered with complex cross-modal semantic understanding and content generation. Overall, our research showcases the promising possibility of building an AI agent capable of modeling universal modalities, paving the way for more human-like AI research in the community. Project page: https://next-gpt.github.io/', 'authors': ['Tat-Seng Chua', 'Wei Ji', 'Leigang Qu', 'Hao Fei', 'Shengqiong Wu'], 'published': '2023-09-11', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'hilm-d-towards-high-resolution-understanding', 'arxiv_id': '2309.05186', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.05186v1', 'url_pdf': 'https://arxiv.org/pdf/2309.05186v1.pdf', 'title': 'HiLM-D: Towards High-Resolution Understanding in Multimodal Large Language Models for Autonomous Driving', 'abstract': \"Autonomous driving systems generally employ separate models for different tasks resulting in intricate designs. For the first time, we leverage singular multimodal large language models (MLLMs) to consolidate multiple autonomous driving tasks from videos, i.e., the Risk Object Localization and Intention and Suggestion Prediction (ROLISP) task. ROLISP uses natural language to simultaneously identify and interpret risk objects, understand ego-vehicle intentions, and provide motion suggestions, eliminating the necessity for task-specific architectures. However, lacking high-resolution (HR) information, existing MLLMs often miss small objects (e.g., traffic cones) and overly focus on salient ones (e.g., large trucks) when applied to ROLISP. We propose HiLM-D (Towards High-Resolution Understanding in MLLMs for Autonomous Driving), an efficient method to incorporate HR information into MLLMs for the ROLISP task. Especially, HiLM-D integrates two branches: (i) the low-resolution reasoning branch, can be any MLLMs, processes low-resolution videos to caption risk objects and discern ego-vehicle intentions/suggestions; (ii) the high-resolution perception branch (HR-PB), prominent to HiLM-D,, ingests HR images to enhance detection by capturing vision-specific HR feature maps and prioritizing all potential risks over merely salient objects. Our HR-PB serves as a plug-and-play module, seamlessly fitting into current MLLMs. Experiments on the ROLISP benchmark reveal HiLM-D's notable advantage over leading MLLMs, with improvements of 4.8% in BLEU-4 for captioning and 17.2% in mIoU for detection.\", 'authors': ['Xiaomeng Li', 'Wei zhang', 'Hang Xu', 'Jianhua Han', 'Xinpeng Ding'], 'published': '2023-09-11', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'investigating-the-catastrophic-forgetting-in', 'arxiv_id': '2309.10313', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.10313v4', 'url_pdf': 'https://arxiv.org/pdf/2309.10313v4.pdf', 'title': 'Investigating the Catastrophic Forgetting in Multimodal Large Language Models', 'abstract': 'Following the success of GPT4, there has been a surge in interest in multimodal large language model (MLLM) research. This line of research focuses on developing general-purpose LLMs through fine-tuning pre-trained LLMs and vision models. However, catastrophic forgetting, a notorious phenomenon where the fine-tuned model fails to retain similar performance compared to the pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM). In this paper, we introduce EMT: Evaluating MulTimodality for evaluating the catastrophic forgetting in MLLMs, by treating each MLLM as an image classifier. We first apply EMT to evaluate several open-source fine-tuned MLLMs and we discover that almost all evaluated MLLMs fail to retain the same performance levels as their vision encoders on standard image classification tasks. Moreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess performance throughout the fine-tuning. Interestingly, our results suggest that early-stage fine-tuning on an image dataset improves performance across other image datasets, by enhancing the alignment of text and visual features. However, as fine-tuning proceeds, the MLLMs begin to hallucinate, resulting in a significant loss of generalizability, even when the image encoder remains frozen. Our results suggest that MLLMs have yet to demonstrate performance on par with their vision models on standard image classification tasks and the current MLLM fine-tuning procedure still has room for improvement.', 'authors': ['Yi Ma', 'Yong Jae Lee', 'Qing Qu', 'Mu Cai', 'Xiao Li', 'Shengbang Tong', 'Yuexiang Zhai'], 'published': '2023-09-19', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'dreamllm-synergistic-multimodal-comprehension', 'arxiv_id': '2309.11499', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.11499v2', 'url_pdf': 'https://arxiv.org/pdf/2309.11499v2.pdf', 'title': 'DreamLLM: Synergistic Multimodal Comprehension and Creation', 'abstract': \"This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy. Project page: https://dreamllm.github.io.\", 'authors': ['Li Yi', 'Kaisheng Ma', 'Xiangyu Zhang', 'Xiangwen Kong', 'Haoran Wei', 'HongYu Zhou', 'Jianjian Sun', 'Liang Zhao', 'Jinrong Yang', 'Zheng Ge', 'Zekun Qi', 'Yuang Peng', 'Chunrui Han', 'Runpei Dong'], 'published': '2023-09-20', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'how-robust-is-google-s-bard-to-adversarial', 'arxiv_id': '2309.11751', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.11751v2', 'url_pdf': 'https://arxiv.org/pdf/2309.11751v2.pdf', 'title': \"How Robust is Google's Bard to Adversarial Image Attacks?\", 'abstract': \"Multimodal Large Language Models (MLLMs) that integrate text and other modalities (especially vision) have achieved unprecedented performance in various multimodal tasks. However, due to the unsolved adversarial robustness problem of vision models, MLLMs can have more severe safety and security risks by introducing the vision inputs. In this work, we study the adversarial robustness of Google's Bard, a competitive chatbot to ChatGPT that released its multimodal capability recently, to better understand the vulnerabilities of commercial MLLMs. By attacking white-box surrogate vision encoders or MLLMs, the generated adversarial examples can mislead Bard to output wrong image descriptions with a 22% success rate based solely on the transferability. We show that the adversarial examples can also attack other MLLMs, e.g., a 26% attack success rate against Bing Chat and a 86% attack success rate against ERNIE bot. Moreover, we identify two defense mechanisms of Bard, including face detection and toxicity detection of images. We design corresponding attacks to evade these defenses, demonstrating that the current defenses of Bard are also vulnerable. We hope this work can deepen our understanding on the robustness of MLLMs and facilitate future research on defenses. Our code is available at https://github.com/thu-ml/Attack-Bard. Update: GPT-4V is available at October 2023. We further evaluate its robustness under the same set of adversarial examples, achieving a 45% attack success rate.\", 'authors': ['Jun Zhu', 'Hang Su', 'Yu Tian', 'Yichi Zhang', 'Xiao Yang', 'Zhengwei Fang', 'Jiawei Chen', 'Huanran Chen', 'Yinpeng Dong'], 'published': '2023-09-21', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'vdc-versatile-data-cleanser-for-detecting', 'arxiv_id': '2309.16211', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.16211v2', 'url_pdf': 'https://arxiv.org/pdf/2309.16211v2.pdf', 'title': 'VDC: Versatile Data Cleanser based on Visual-Linguistic Inconsistency by Multimodal Large Language Models', 'abstract': 'The role of data in building AI systems has recently been emphasized by the emerging concept of data-centric AI. Unfortunately, in the real-world, datasets may contain dirty samples, such as poisoned samples from backdoor attack, noisy labels in crowdsourcing, and even hybrids of them. The presence of such dirty samples makes the DNNs vunerable and unreliable.Hence, it is critical to detect dirty samples to improve the quality and realiability of dataset. Existing detectors only focus on detecting poisoned samples or noisy labels, that are often prone to weak generalization when dealing with dirty samples from other domains.In this paper, we find a commonality of various dirty samples is visual-linguistic inconsistency between images and associated labels. To capture the semantic inconsistency between modalities, we propose versatile data cleanser (VDC) leveraging the surpassing capabilities of multimodal large language models (MLLM) in cross-modal alignment and reasoning.It consists of three consecutive modules: the visual question generation module to generate insightful questions about the image; the visual question answering module to acquire the semantics of the visual content by answering the questions with MLLM; followed by the visual answer evaluation module to evaluate the inconsistency.Extensive experiments demonstrate its superior performance and generalization to various categories and types of dirty samples. The code is available at \\\\url{https://github.com/zihao-ai/vdc}.', 'authors': ['Baoyuan Wu', 'Bingzhe Wu', 'Shaokui Wei', 'Mingda Zhang', 'Zihao Zhu'], 'published': '2023-09-28', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'guiding-instruction-based-image-editing-via', 'arxiv_id': '2309.17102', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.17102v2', 'url_pdf': 'https://arxiv.org/pdf/2309.17102v2.pdf', 'title': 'Guiding Instruction-based Image Editing via Multimodal Large Language Models', 'abstract': 'Instruction-based image editing improves the controllability and flexibility of image manipulation via natural commands without elaborate descriptions or regional masks. However, human instructions are sometimes too brief for current methods to capture and follow. Multimodal large language models (MLLMs) show promising capabilities in cross-modal understanding and visual-aware response generation via LMs. We investigate how MLLMs facilitate edit instructions and present MLLM-Guided Image Editing (MGIE). MGIE learns to derive expressive instructions and provides explicit guidance. The editing model jointly captures this visual imagination and performs manipulation through end-to-end training. We evaluate various aspects of Photoshop-style modification, global photo optimization, and local editing. Extensive experimental results demonstrate that expressive instructions are crucial to instruction-based image editing, and our MGIE can lead to a notable improvement in automatic metrics and human evaluation while maintaining competitive inference efficiency.', 'authors': ['Zhe Gan', 'Yinfei Yang', 'William Yang Wang', 'Xianzhi Du', 'Wenze Hu', 'Tsu-Jui Fu'], 'published': '2023-09-29', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'reformulating-vision-language-foundation', 'arxiv_id': '2310.00653', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.00653v1', 'url_pdf': 'https://arxiv.org/pdf/2310.00653v1.pdf', 'title': 'Reformulating Vision-Language Foundation Models and Datasets Towards Universal Multimodal Assistants', 'abstract': \"Recent Multimodal Large Language Models (MLLMs) exhibit impressive abilities to perceive images and follow open-ended instructions. The capabilities of MLLMs depend on two crucial factors: the model architecture to facilitate the feature alignment of visual modules and large language models; the multimodal instruction tuning datasets for human instruction following. (i) For the model architecture, most existing models introduce an external bridge module to connect vision encoders with language models, which needs an additional feature-alignment pre-training. In this work, we discover that compact pre-trained vision language models can inherently serve as ``out-of-the-box'' bridges between vision and language. Based on this, we propose Muffin framework, which directly employs pre-trained vision-language models to act as providers of visual signals. (ii) For the multimodal instruction tuning datasets, existing methods omit the complementary relationship between different datasets and simply mix datasets from different tasks. Instead, we propose UniMM-Chat dataset which explores the complementarities of datasets to generate 1.1M high-quality and diverse multimodal instructions. We merge information describing the same image from diverse datasets and transforms it into more knowledge-intensive conversation data. Experimental results demonstrate the effectiveness of the Muffin framework and UniMM-Chat dataset. Muffin achieves state-of-the-art performance on a wide range of vision-language tasks, significantly surpassing state-of-the-art models like LLaVA and InstructBLIP. Our model and dataset are all accessible at https://github.com/thunlp/muffin.\", 'authors': ['Maosong Sun', 'Hai-Tao Zheng', 'Zhiyuan Liu', 'Dahai Li', 'Jiao Xue', 'Yinxv Pan', 'Shan Wang', 'Chongyi Wang', 'Yue Zhao', 'Haoye Zhang', 'Yuan YAO', 'Jinyi Hu', 'Tianyu Yu'], 'published': '2023-10-01', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'drivegpt4-interpretable-end-to-end-autonomous', 'arxiv_id': '2310.01412', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.01412v5', 'url_pdf': 'https://arxiv.org/pdf/2310.01412v5.pdf', 'title': 'DriveGPT4: Interpretable End-to-end Autonomous Driving via Large Language Model', 'abstract': 'Multimodal large language models (MLLMs) have emerged as a prominent area of interest within the research community, given their proficiency in handling and reasoning with non-textual data, including images and videos. This study seeks to extend the application of MLLMs to the realm of autonomous driving by introducing DriveGPT4, a novel interpretable end-to-end autonomous driving system based on LLMs. Capable of processing multi-frame video inputs and textual queries, DriveGPT4 facilitates the interpretation of vehicle actions, offers pertinent reasoning, and effectively addresses a diverse range of questions posed by users. Furthermore, DriveGPT4 predicts low-level vehicle control signals in an end-to-end fashion.These advanced capabilities are achieved through the utilization of a bespoke visual instruction tuning dataset, specifically tailored for autonomous driving applications, in conjunction with a mix-finetuning training strategy. DriveGPT4 represents the pioneering effort to leverage LLMs for the development of an interpretable end-to-end autonomous driving solution. Evaluations conducted on the BDD-X dataset showcase the superior qualitative and quantitative performance of DriveGPT4. Additionally, the fine-tuning of domain-specific data enables DriveGPT4 to yield close or even improved results in terms of autonomous driving grounding when contrasted with GPT4-V.', 'authors': ['Kwan-Yee. K. Wong', 'Hengshuang Zhao', 'Zhenguo Li', 'Yong Guo', 'Zhen Zhao', 'Enze Xie', 'Yujia Zhang', 'Zhenhua Xu'], 'published': '2023-10-02', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'minigpt-5-interleaved-vision-and-language', 'arxiv_id': '2310.02239', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.02239v3', 'url_pdf': 'https://arxiv.org/pdf/2310.02239v3.pdf', 'title': 'MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens', 'abstract': 'The effectiveness of Multimodal Large Language Models (MLLMs) demonstrates a profound capability in multimodal understanding. However, the simultaneous generation of images with coherent texts is still underdeveloped. Addressing this, we introduce a novel interleaved vision-and-language generation method, centered around the concept of ``generative vokens\". These vokens serve as pivotal elements contributing to coherent image-text outputs. Our method is marked by a unique two-stage training strategy for description-free multimodal generation, which does not necessitate extensive descriptions of images. We integrate classifier-free guidance to enhance the alignment of generated images and texts, ensuring more seamless and contextually relevant multimodal interactions. Our model, MiniGPT-5, exhibits substantial improvement over the baseline models on multimodal generation datasets, including MMDialog and VIST. The human evaluation shows MiniGPT-5 is better than the baseline model on more than 56\\\\% cases for multimodal generation, highlighting its efficacy across diverse benchmarks.', 'authors': ['Xin Eric Wang', 'Xuehai He', 'Kaizhi Zheng'], 'published': '2023-10-03', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'towards-end-to-end-embodied-decision-making', 'arxiv_id': '2310.02071', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.02071v4', 'url_pdf': 'https://arxiv.org/pdf/2310.02071v4.pdf', 'title': 'Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond', 'abstract': 'In this study, we explore the potential of Multimodal Large Language Models (MLLMs) in improving embodied decision-making processes for agents. While Large Language Models (LLMs) have been widely used due to their advanced reasoning skills and vast world knowledge, MLLMs like GPT4-Vision offer enhanced visual understanding and reasoning capabilities. We investigate whether state-of-the-art MLLMs can handle embodied decision-making in an end-to-end manner and whether collaborations between LLMs and MLLMs can enhance decision-making. To address these questions, we introduce a new benchmark called PCA-EVAL, which evaluates embodied decision-making from the perspectives of Perception, Cognition, and Action. Additionally, we propose HOLMES, a multi-agent cooperation framework that allows LLMs to leverage MLLMs and APIs to gather multimodal information for informed decision-making. We compare end-to-end embodied decision-making and HOLMES on our benchmark and find that the GPT4-Vision model demonstrates strong end-to-end embodied decision-making abilities, outperforming GPT4-HOLMES in terms of average decision accuracy (+3%). However, this performance is exclusive to the latest GPT4-Vision model, surpassing the open-source state-of-the-art MLLM by 26%. Our results indicate that powerful MLLMs like GPT4-Vision hold promise for decision-making in embodied agents, offering new avenues for MLLM research. Code and data are open at https://github.com/pkunlp-icler/PCA-EVAL/.', 'authors': ['Peiyi Wang', 'Baobao Chang', 'Tianyu Liu', 'Yuchi Wang', 'Zefan Cai', 'Haozhe Zhao', 'Shuhuai Ren', 'Yichi Zhang', 'Liang Chen'], 'published': '2023-10-03', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'kosmos-g-generating-images-in-context-with', 'arxiv_id': '2310.02992', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.02992v3', 'url_pdf': 'https://arxiv.org/pdf/2310.02992v3.pdf', 'title': 'Kosmos-G: Generating Images in Context with Multimodal Large Language Models', 'abstract': 'Recent advancements in subject-driven image generation have made significant strides. However, current methods still fall short in diverse application scenarios, as they require test-time tuning and cannot accept interleaved multi-image and text input. These limitations keep them far from the ultimate goal of \"image as a foreign language in image generation.\" This paper presents Kosmos-G, a model that leverages the advanced multimodal perception capabilities of Multimodal Large Language Models (MLLMs) to tackle the aforementioned challenge. Our approach aligns the output space of MLLM with CLIP using the textual modality as an anchor and performs compositional instruction tuning on curated data. Kosmos-G demonstrates an impressive capability of zero-shot subject-driven generation with interleaved multi-image and text input. Notably, the score distillation instruction tuning requires no modifications to the image decoder. This allows for a seamless substitution of CLIP and effortless integration with a myriad of U-Net techniques ranging from fine-grained controls to personalized image decoder variants. We posit Kosmos-G as an initial attempt towards the goal of \"image as a foreign language in image generation.\" The code can be found at https://aka.ms/Kosmos-G', 'authors': ['Furu Wei', 'Wenhu Chen', 'Zhiliang Peng', 'Shaohan Huang', 'Li Dong', 'Xichen Pan'], 'published': '2023-10-04', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'fine-grained-audio-visual-joint', 'arxiv_id': '2310.05863', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.05863v2', 'url_pdf': 'https://arxiv.org/pdf/2310.05863v2.pdf', 'title': 'Fine-grained Audio-Visual Joint Representations for Multimodal Large Language Models', 'abstract': 'Audio-visual large language models (LLM) have drawn significant attention, yet the fine-grained combination of both input streams is rather under-explored, which is challenging but necessary for LLMs to understand general video inputs. To this end, a fine-grained audio-visual joint representation (FAVOR) learning framework for multimodal LLMs is proposed in this paper, which extends a text-based LLM to simultaneously perceive speech and audio events in the audio input stream and images or videos in the visual input stream, at the frame level. To fuse the audio and visual feature streams into joint representations and to align the joint space with the LLM input embedding space, we propose a causal Q-Former structure with a causal attention module to enhance the capture of causal relations of the audio-visual frames across time. An audio-visual evaluation benchmark (AVEB) is also proposed which comprises six representative single-modal tasks with five cross-modal tasks reflecting audio-visual co-reasoning abilities. While achieving competitive single-modal performance on audio, speech and image tasks in AVEB, FAVOR achieved over 20% accuracy improvements on the video question-answering task when fine-grained information or temporal causal reasoning is required. FAVOR, in addition, demonstrated remarkable video comprehension and reasoning abilities on tasks that are unprecedented by other multimodal LLMs. An interactive demo of FAVOR is available at https://github.com/BriansIDP/AudioVisualLLM.git, and the training code and model checkpoints will be released soon.', 'authors': ['Chao Zhang', 'Zejun Ma', 'Lu Lu', 'Wei Li', 'Tian Tan', 'Xianzhao Chen', 'Changli Tang', 'Wenyi Yu', 'Guangzhi Sun'], 'published': '2023-10-09', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'can-we-edit-multimodal-large-language-models', 'arxiv_id': '2310.08475', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.08475v5', 'url_pdf': 'https://arxiv.org/pdf/2310.08475v5.pdf', 'title': 'Can We Edit Multimodal Large Language Models?', 'abstract': 'In this paper, we focus on editing Multimodal Large Language Models (MLLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights. Code and dataset are available in https://github.com/zjunlp/EasyEdit.', 'authors': ['Ningyu Zhang', 'Huajun Chen', 'Yongheng Wang', 'Xi Chen', 'Qingbin Liu', 'Bozhong Tian', 'Siyuan Cheng'], 'published': '2023-10-12', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'autorepo-a-general-framework-for-multi-modal', 'arxiv_id': '2310.07944', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.07944v2', 'url_pdf': 'https://arxiv.org/pdf/2310.07944v2.pdf', 'title': 'AutoRepo: A general framework for multi-modal LLM-based automated construction reporting', 'abstract': 'Ensuring the safety, quality, and timely completion of construction projects is paramount, with construction inspections serving as a vital instrument towards these goals. Nevertheless, the predominantly manual approach of present-day inspections frequently results in inefficiencies and inadequate information management. Such methods often fall short of providing holistic, exhaustive assessments, consequently engendering regulatory oversights and potential safety hazards. To address this issue, this paper presents a novel framework named AutoRepo for automated generation of construction inspection reports. The unmanned vehicles efficiently perform construction inspections and collect scene information, while the multimodal large language models (LLMs) are leveraged to automatically generate the inspection reports. The framework was applied and tested on a real-world construction site, demonstrating its potential to expedite the inspection process, significantly reduce resource allocation, and produce high-quality, regulatory standard-compliant inspection reports. This research thus underscores the immense potential of multimodal large language models in revolutionizing construction inspection practices, signaling a significant leap forward towards a more efficient and safer construction management paradigm.', 'authors': ['Heng Li', 'Runhao Guo', 'Jing Li', 'Xincong Yang', 'Hongxu Pu'], 'published': '2023-10-11', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'mm-bigbench-evaluating-multimodal-models-on', 'arxiv_id': '2310.09036', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.09036v1', 'url_pdf': 'https://arxiv.org/pdf/2310.09036v1.pdf', 'title': 'MM-BigBench: Evaluating Multimodal Models on Multimodal Content Comprehension Tasks', 'abstract': \"The popularity of multimodal large language models (MLLMs) has triggered a recent surge in research efforts dedicated to evaluating these models. Nevertheless, existing evaluation studies of MLLMs primarily focus on the comprehension and reasoning of unimodal (vision) content, neglecting performance evaluations in the domain of multimodal (vision-language) content understanding. Beyond multimodal reasoning, tasks related to multimodal content comprehension necessitate a profound understanding of multimodal contexts, achieved through the multimodal interaction to obtain a final answer. In this paper, we introduce a comprehensive assessment framework called MM-BigBench, which incorporates a diverse range of metrics to offer an extensive evaluation of the performance of various models and instructions across a wide spectrum of diverse multimodal content comprehension tasks. Consequently, our work complements research on the performance of MLLMs in multimodal comprehension tasks, achieving a more comprehensive and holistic evaluation of MLLMs. To begin, we employ the Best Performance metric to ascertain each model's performance upper bound on different datasets. Subsequently, the Mean Relative Gain metric offers an assessment of the overall performance of various models and instructions, while the Stability metric measures their sensitivity. Furthermore, previous research centers on evaluating models independently or solely assessing instructions, neglecting the adaptability between models and instructions. We propose the Adaptability metric to quantify the adaptability between models and instructions. Our paper evaluates a total of 20 language models (14 MLLMs) on 14 multimodal datasets spanning 6 tasks, with 10 instructions for each task, and derives novel insights. Our code will be released at https://github.com/declare-lab/MM-BigBench.\", 'authors': ['Soujanya Poria', 'XiaoMing Fu', 'Yifei Zhang', 'Qi Sun', 'Yang Li', 'Daling Wang', 'Ming Wang', 'Shi Feng', 'Wenfang Wu', 'Xiaocui Yang'], 'published': '2023-10-13', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'robollm-robotic-vision-tasks-grounded-on', 'arxiv_id': '2310.10221', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.10221v2', 'url_pdf': 'https://arxiv.org/pdf/2310.10221v2.pdf', 'title': 'RoboLLM: Robotic Vision Tasks Grounded on Multimodal Large Language Models', 'abstract': 'Robotic vision applications often necessitate a wide range of visual perception tasks, such as object detection, segmentation, and identification. While there have been substantial advances in these individual tasks, integrating specialized models into a unified vision pipeline presents significant engineering challenges and costs. Recently, Multimodal Large Language Models (MLLMs) have emerged as novel backbones for various downstream tasks. We argue that leveraging the pre-training capabilities of MLLMs enables the creation of a simplified framework, thus mitigating the need for task-specific encoders. Specifically, the large-scale pretrained knowledge in MLLMs allows for easier fine-tuning to downstream robotic vision tasks and yields superior performance. We introduce the RoboLLM framework, equipped with a BEiT-3 backbone, to address all visual perception tasks in the ARMBench challenge-a large-scale robotic manipulation dataset about real-world warehouse scenarios. RoboLLM not only outperforms existing baselines but also substantially reduces the engineering burden associated with model selection and tuning. The source code is publicly available at https://github.com/longkukuhi/armbench.', 'authors': ['Gerardo Aragon Camarasa', 'Richard McCreadie', 'George Killick', 'Zijun Long'], 'published': '2023-10-16', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'benchmarking-sequential-visual-input', 'arxiv_id': '2310.13473', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.13473v1', 'url_pdf': 'https://arxiv.org/pdf/2310.13473v1.pdf', 'title': 'Benchmarking Sequential Visual Input Reasoning and Prediction in Multimodal Large Language Models', 'abstract': \"Multimodal large language models (MLLMs) have shown great potential in perception and interpretation tasks, but their capabilities in predictive reasoning remain under-explored. To address this gap, we introduce a novel benchmark that assesses the predictive reasoning capabilities of MLLMs across diverse scenarios. Our benchmark targets three important domains: abstract pattern reasoning, human activity prediction, and physical interaction prediction. We further develop three evaluation methods powered by large language model to robustly quantify a model's performance in predicting and reasoning the future based on multi-visual context. Empirical experiments confirm the soundness of the proposed benchmark and evaluation methods via rigorous testing and reveal pros and cons of current popular MLLMs in the task of predictive reasoning. Lastly, our proposed benchmark provides a standardized evaluation framework for MLLMs and can facilitate the development of more advanced models that can reason and predict over complex long sequence of multimodal input.\", 'authors': ['Jianwei Yin', 'Tiancheng Zhao', 'Kangjia Zhao', 'Yu Shu', 'Leigang Sha', 'Mingwei Zhu'], 'published': '2023-10-20', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'woodpecker-hallucination-correction-for', 'arxiv_id': '2310.16045', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.16045v2', 'url_pdf': 'https://arxiv.org/pdf/2310.16045v2.pdf', 'title': 'Woodpecker: Hallucination Correction for Multimodal Large Language Models', 'abstract': 'Hallucination is a big shadow hanging over the rapidly evolving Multimodal Large Language Models (MLLMs), referring to the phenomenon that the generated text is inconsistent with the image content. In order to mitigate hallucinations, existing studies mainly resort to an instruction-tuning manner that requires retraining the models with specific data. In this paper, we pave a different way, introducing a training-free method named Woodpecker. Like a woodpecker heals trees, it picks out and corrects hallucinations from the generated text. Concretely, Woodpecker consists of five stages: key concept extraction, question formulation, visual knowledge validation, visual claim generation, and hallucination correction. Implemented in a post-remedy manner, Woodpecker can easily serve different MLLMs, while being interpretable by accessing intermediate outputs of the five stages. We evaluate Woodpecker both quantitatively and qualitatively and show the huge potential of this new paradigm. On the POPE benchmark, our method obtains a 30.66%/24.33% improvement in accuracy over the baseline MiniGPT-4/mPLUG-Owl. The source code is released at https://github.com/BradyFU/Woodpecker.', 'authors': ['Enhong Chen', 'Xing Sun', 'Ke Li', 'Yunhang Shen', 'Dianbo Sui', 'Hao Wang', 'Tong Xu', 'Sirui Zhao', 'Chaoyou Fu', 'Shukang Yin'], 'published': '2023-10-24', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'visual-cropping-improves-zero-shot-question', 'arxiv_id': '2310.16033', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.16033v3', 'url_pdf': 'https://arxiv.org/pdf/2310.16033v3.pdf', 'title': 'Towards Perceiving Small Visual Details in Zero-shot Visual Question Answering with Multimodal LLMs', 'abstract': \"Multimodal Large Language Models (MLLMs) have recently achieved promising zero-shot accuracy on visual question answering (VQA) -- a fundamental task affecting various downstream applications and domains. Given the great potential for the broad use of these models, it is important to investigate their limitations in dealing with different image and question properties. In this work, we investigate whether MLLMs can perceive small details as well as large details in images. In particular, we show that their zero-shot accuracy in answering visual questions is very sensitive to the size of the visual subject of the question, declining up to 46% with size. Furthermore, we show that this effect is causal by observing that human visual cropping can significantly mitigate their sensitivity to size. Inspired by the usefulness of human cropping, we then propose five automatic visual cropping methods -- leveraging either external localization models or the decision process of the given MLLM itself -- as inference time mechanisms to improve the zero-shot performance of MLLMs. We study their effectiveness on four popular VQA datasets, and a subset of the VQAv2 dataset tailored towards fine visual details. Our findings suggest that MLLMs should be used with caution in detail-sensitive VQA applications, and that visual cropping is a promising direction to improve their zero-shot performance. To facilitate further investigation of MLLMs' behaviors, our code and data are publicly released.\", 'authors': ['Filip Ilievski', 'Prateek Chhikara', 'Mahyar Khayatkhoei', 'Jiarui Zhang'], 'published': '2023-10-24', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'chef-a-comprehensive-evaluation-framework-for', 'arxiv_id': '2311.02692', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2311.02692v1', 'url_pdf': 'https://arxiv.org/pdf/2311.02692v1.pdf', 'title': 'ChEF: A Comprehensive Evaluation Framework for Standardized Assessment of Multimodal Large Language Models', 'abstract': \"Multimodal Large Language Models (MLLMs) have shown impressive abilities in interacting with visual content with myriad potential downstream tasks. However, even though a list of benchmarks has been proposed, the capabilities and limitations of MLLMs are still not comprehensively understood, due to a lack of a standardized and holistic evaluation framework. To this end, we present the first Comprehensive Evaluation Framework (ChEF) that can holistically profile each MLLM and fairly compare different MLLMs. First, we structure ChEF as four modular components, i.e., Scenario as scalable multimodal datasets, Instruction as flexible instruction retrieving formulae, Inferencer as reliable question answering strategies, and Metric as indicative task-specific score functions. Based on them, ChEF facilitates versatile evaluations in a standardized framework, and new evaluations can be built by designing new Recipes (systematic selection of these four components). Notably, current MLLM benchmarks can be readily summarized as recipes of ChEF. Second, we introduce 6 new recipes to quantify competent MLLMs' desired capabilities (or called desiderata, i.e., calibration, in-context learning, instruction following, language performance, hallucination, and robustness) as reliable agents that can perform real-world multimodal interactions. Third, we conduct a large-scale evaluation of 9 prominent MLLMs on 9 scenarios and 6 desiderata. Our evaluation summarized over 20 valuable observations concerning the generalizability of MLLMs across various scenarios and the composite capability of MLLMs required for multimodal interactions. We will publicly release all the detailed implementations for further analysis, as well as an easy-to-use modular toolkit for the integration of new recipes and models, so that ChEF can be a growing evaluation framework for the MLLM community.\", 'authors': ['Jing Shao', 'Yu Qiao', 'Lu Sheng', 'Zhenfei Yin', 'Hongxing Fan', 'Zhipin Wang', 'Zhelun Shi'], 'published': '2023-11-05', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'octavius-mitigating-task-interference-in', 'arxiv_id': '2311.02684', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2311.02684v3', 'url_pdf': 'https://arxiv.org/pdf/2311.02684v3.pdf', 'title': 'Octavius: Mitigating Task Interference in MLLMs via LoRA-MoE', 'abstract': 'Recent studies have demonstrated Large Language Models (LLMs) can extend their zero-shot generalization capabilities to multimodal learning through instruction tuning. As more modalities and downstream tasks are introduced, negative conflicts and interference may have a worse impact on performance. While this phenomenon has been overlooked in previous work, we propose a novel and extensible framework, called Octavius, for comprehensive studies and experimentation on multimodal learning with Multimodal Large Language Models (MLLMs). Specifically, we combine the well-known Mixture-of-Experts (MoE) and one of the representative PEFT techniques, i.e., LoRA, designing a novel LLM-based decoder, called LoRA-MoE, for multimodal learning. To the best of our knowledge, we are one of the pioneering efforts to introduce MoE into MLLMs to address this problem. The experimental results (about 20% improvement) have shown the effectiveness and versatility of our design in various 2D and 3D downstream tasks. Code and datasets are available at https://openlamm.github.io/tutorial/.', 'authors': ['Jing Shao', 'Yu Qiao', 'Wanli Ouyang', 'Lu Sheng', 'Si Liu', 'Zhenfei Yin', 'Huayang Liu', 'Zhen Wang', 'Ziqin Wang', 'Zeren Chen'], 'published': '2023-11-05', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'what-large-language-models-bring-to-text-rich', 'arxiv_id': '2311.07306', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2311.07306v1', 'url_pdf': 'https://arxiv.org/pdf/2311.07306v1.pdf', 'title': 'What Large Language Models Bring to Text-rich VQA?', 'abstract': \"Text-rich VQA, namely Visual Question Answering based on text recognition in the images, is a cross-modal task that requires both image comprehension and text recognition. In this work, we focus on investigating the advantages and bottlenecks of LLM-based approaches in addressing this problem. To address the above concern, we separate the vision and language modules, where we leverage external OCR models to recognize texts in the image and Large Language Models (LLMs) to answer the question given texts. The whole framework is training-free benefiting from the in-context ability of LLMs. This pipeline achieved superior performance compared to the majority of existing Multimodal Large Language Models (MLLM) on four text-rich VQA datasets. Besides, based on the ablation study, we find that LLM brings stronger comprehension ability and may introduce helpful knowledge for the VQA problem. The bottleneck for LLM to address text-rich VQA problems may primarily lie in visual part. We also combine the OCR module with MLLMs and pleasantly find that the combination of OCR module with MLLM also works. It's worth noting that not all MLLMs can comprehend the OCR information, which provides insights into how to train an MLLM that preserves the abilities of LLM.\", 'authors': ['Fei Tan', 'Zechao Li', 'Rui Zhao', 'Jinghui Lu', 'Xinzhe Ni', 'Wei Tang', 'Xuejing Liu'], 'published': '2023-11-13', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}], 'docs': [Document(metadata={'title': 'Multimodal large language models for inclusive collaboration learning tasks'}, page_content='This PhD project leverages advancements in multimodal large language models to build an inclusive collaboration feedback loop, in order to facilitate the automated detection, modeling, and feedback for participants developing general collaboration skills. This topic is important given the role of collaboration as an essential 21st century skill, the potential to ground large language models within learning theory and real-world practice, and the expressive potential of transformer models to support equity and inclusion. We address some concerns of integrating advances in natural language processing into downstream tasks such as the learning analytics feedback loop.'), Document(metadata={'title': 'Personality-aware Human-centric Multimodal Reasoning: A New Task, Dataset and Baselines'}, page_content=\"Personality traits, emotions, and beliefs shape individuals' behavioral choices and decision-making processes. However, for one thing, the affective computing community normally focused on predicting personality traits but overlooks their application in behavior prediction. For another, the multimodal reasoning task emphasized the prediction of future states and behaviors but often neglected the incorporation of individual personality traits. In this work, we introduce a new task called Personality-aware Human-centric Multimodal Reasoning (PHMR) (T1), with the goal of forecasting the future behavior of a particular individual using multimodal information from past instances, while integrating personality factors. We accordingly construct a new dataset based on six television shows, encompassing 225 characters and 12k samples. To establish a benchmark for the task, we propose seven baseline methods: three adapted from related tasks, two pre-trained model, and two multimodal large language models. The experimental results demonstrate that incorporating personality traits enhances human-centric multimodal reasoning performance. To further solve the lack of personality annotation in real-life scenes, we introduce an extension task called Personality-predicted Human-centric Multimodal Reasoning task (T2) along with the corresponding dataset and method. We will make our dataset and code available on GitHub.\"), Document(metadata={'title': 'Visual Transformation Telling'}, page_content='Humans can naturally reason from superficial state differences (e.g. ground wetness) to transformations descriptions (e.g. raining) according to their life experience. In this paper, we propose a new visual reasoning task to test this transformation reasoning ability in real-world scenarios, called \\\\textbf{V}isual \\\\textbf{T}ransformation \\\\textbf{T}elling (VTT). Given a series of states (i.e. images), VTT requires to describe the transformation occurring between every two adjacent states. Different from existing visual reasoning tasks that focus on surface state reasoning, the advantage of VTT is that it captures the underlying causes, e.g. actions or events, behind the differences among states. We collect a novel dataset to support the study of transformation reasoning from two existing instructional video datasets, CrossTask and COIN, comprising 13,547 samples. Each sample involves the key state images along with their transformation descriptions. Our dataset covers diverse real-world activities, providing a rich resource for training and evaluation. To construct an initial benchmark for VTT, we test several models, including traditional visual storytelling methods (CST, GLACNet, Densecap) and advanced multimodal large language models (LLaVA v1.5-7B, Qwen-VL-chat, Gemini Pro Vision, GPT-4o, and GPT-4). Experimental results reveal that even state-of-the-art models still face challenges in VTT, highlighting substantial areas for improvement.'), Document(metadata={'title': 'Listen, Think, and Understand'}, page_content='The ability of artificial intelligence (AI) systems to perceive and comprehend audio signals is crucial for many applications. Although significant progress has been made in this area since the development of AudioSet, most existing models are designed to map audio inputs to pre-defined, discrete sound label sets. In contrast, humans possess the ability to not only classify sounds into general categories, but also to listen to the finer details of the sounds, explain the reason for the predictions, think about what the sound infers, and understand the scene and what action needs to be taken, if any. Such capabilities beyond perception are not yet present in existing audio models. On the other hand, modern large language models (LLMs) exhibit emerging reasoning ability but they lack audio perception capabilities. Therefore, we ask the question: can we build a model that has both audio perception and a reasoning ability? In this paper, we propose a new audio foundation model, called LTU (Listen, Think, and Understand). To train LTU, we created a new OpenAQA-5M dataset consisting of 1.9 million closed-ended and 3.7 million open-ended, diverse (audio, question, answer) tuples, and have used an autoregressive training framework with a perception-to-understanding curriculum. LTU demonstrates strong performance and generalization ability on conventional audio tasks such as classification and captioning. More importantly, it exhibits emerging audio reasoning and comprehension abilities that are absent in existing audio models. To the best of our knowledge, LTU is one of the first multimodal large language models that focus on general audio (rather than just speech) understanding.'), Document(metadata={'title': 'What Makes for Good Visual Tokenizers for Large Language Models?'}, page_content='We empirically investigate proper pre-training methods to build good visual tokenizers, making Large Language Models (LLMs) powerful Multimodal Large Language Models (MLLMs). In our benchmark, which is curated to evaluate MLLMs visual semantic understanding and fine-grained perception capabilities, we discussed different visual tokenizers pre-trained with dominant methods (i.e., DeiT, CLIP, MAE, DINO), and observe that: i) Fully/weakly supervised models capture more semantics than self-supervised models, but the gap is narrowed by scaling up the pre-training dataset. ii) Self-supervised models are better at fine-grained perception, where patch-level supervision is particularly effective. iii) Tuning the visual tokenizer leads to the loss of semantics obtained from large-scale pretraining, which is unfavorable with relatively small-scale instruction-tuning dataset. Given the findings, we reviewed methods that attempted to unify semantics and fine-grained visual understanding, e.g., patch-level feature distillation with semantically-rich targets. We obtain an intriguing insight mask-based strategies that were once all the rage may not be applicable for obtaining good visual tokenizers. Based on this critical observation, we obtain a new MLLM equipped with a tailored Good Visual Tokenizer (GVT), which exhibits strong visual comprehension capability at multiple scales. In particular, without introducing extra parameters and task-specific fine-tuning, GVT achieves superior performance on visual question answering, image captioning, and other fine-grained visual understanding tasks such as object counting and multi-class identification.'), Document(metadata={'title': 'Dynamic Clue Bottlenecks: Towards Interpretable-by-Design Visual Question Answering'}, page_content='Recent advances in multimodal large language models (LLMs) have shown extreme effectiveness in visual question answering (VQA). However, the design nature of these end-to-end models prevents them from being interpretable to humans, undermining trust and applicability in critical domains. While post-hoc rationales offer certain insight into understanding model behavior, these explanations are not guaranteed to be faithful to the model. In this paper, we address these shortcomings by introducing an interpretable by design model that factors model decisions into intermediate human-legible explanations, and allows people to easily understand why a model fails or succeeds. We propose the Dynamic Clue Bottleneck Model ( (DCLUB), a method that is designed towards an inherently interpretable VQA system. DCLUB provides an explainable intermediate space before the VQA decision and is faithful from the beginning, while maintaining comparable performance to black-box systems. Given a question, DCLUB first returns a set of visual clues: natural language statements of visually salient evidence from the image, and then generates the output based solely on the visual clues. To supervise and evaluate the generation of VQA explanations within DCLUB, we collect a dataset of 1.7k reasoning-focused questions with visual clues. Evaluations show that our inherently interpretable system can improve 4.64% over a comparable black-box system in reasoning-focused questions while preserving 99.43% of performance on VQA-v2.'), Document(metadata={'title': 'PathAsst: A Generative Foundation AI Assistant Towards Artificial General Intelligence of Pathology'}, page_content=\"As advances in large language models (LLMs) and multimodal techniques continue to mature, the development of general-purpose multimodal large language models (MLLMs) has surged, offering significant applications in interpreting natural images. However, the field of pathology has largely remained untapped, particularly in gathering high-quality data and designing comprehensive model frameworks. To bridge the gap in pathology MLLMs, we present PathAsst, a multimodal generative foundation AI assistant to revolutionize diagnostic and predictive analytics in pathology. The development of PathAsst involves three pivotal steps: data acquisition, CLIP model adaptation, and the training of PathAsst's multimodal generative capabilities. Firstly, we collect over 207K high-quality pathology image-text pairs from authoritative sources. Leveraging the advanced power of ChatGPT, we generate over 180K instruction-following samples. Furthermore, we devise additional instruction-following data specifically tailored for invoking eight pathology-specific sub-models we prepared, allowing the PathAsst to effectively collaborate with these models, enhancing its diagnostic ability. Secondly, by leveraging the collected data, we construct PathCLIP, a pathology-dedicated CLIP, to enhance PathAsst's capabilities in interpreting pathology images. Finally, we integrate PathCLIP with the Vicuna-13b and utilize pathology-specific instruction-tuning data to enhance the multimodal generation capacity of PathAsst and bolster its synergistic interactions with sub-models. The experimental results of PathAsst show the potential of harnessing AI-powered generative foundation model to improve pathology diagnosis and treatment processes.\"), Document(metadata={'title': 'Contextual Object Detection with Multimodal Large Language Models'}, page_content='Recent Multimodal Large Language Models (MLLMs) are remarkable in vision-language tasks, such as image captioning and question answering, but lack the essential perception ability, i.e., object detection. In this work, we address this limitation by introducing a novel research problem of contextual object detection -- understanding visible objects within different human-AI interactive contexts. Three representative scenarios are investigated, including the language cloze test, visual captioning, and question answering. Moreover, we present ContextDET, a unified multimodal model that is capable of end-to-end differentiable modeling of visual-language contexts, so as to locate, identify, and associate visual objects with language inputs for human-AI interaction. Our ContextDET involves three key submodels: (i) a visual encoder for extracting visual representations, (ii) a pre-trained LLM for multimodal context decoding, and (iii) a visual decoder for predicting bounding boxes given contextual object words. The new generate-then-detect framework enables us to detect object words within human vocabulary. Extensive experiments show the advantages of ContextDET on our proposed CODE benchmark, open-vocabulary detection, and referring image segmentation. Github: https://github.com/yuhangzang/ContextDET.'), Document(metadata={'title': 'A Survey on Multimodal Large Language Models'}, page_content='Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.'), Document(metadata={'title': 'MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models'}, page_content='Multimodal Large Language Model (MLLM) relies on the powerful LLM to perform multimodal tasks, showing amazing emergent abilities in recent studies, such as writing poems based on an image. However, it is difficult for these case studies to fully reflect the performance of MLLM, lacking a comprehensive evaluation. In this paper, we fill in this blank, presenting the first comprehensive MLLM Evaluation benchmark MME. It measures both perception and cognition abilities on a total of 14 subtasks. In order to avoid data leakage that may arise from direct use of public datasets for evaluation, the annotations of instruction-answer pairs are all manually designed. The concise instruction design allows us to fairly compare MLLMs, instead of struggling in prompt engineering. Besides, with such an instruction, we can also easily carry out quantitative statistics. A total of 30 advanced MLLMs are comprehensively evaluated on our MME, which not only suggests that existing MLLMs still have a large room for improvement, but also reveals the potential directions for the subsequent model optimization. The data application manner and online leaderboards are released at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation.'), Document(metadata={'title': 'Kosmos-2: Grounding Multimodal Large Language Models to the World'}, page_content=\"We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. Specifically, we represent refer expressions as links in Markdown, i.e., ``[text span](bounding boxes)'', where object descriptions are sequences of location tokens. Together with multimodal corpora, we construct large-scale data of grounded image-text pairs (called GrIT) to train the model. In addition to the existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), Kosmos-2 integrates the grounding capability into downstream applications. We evaluate Kosmos-2 on a wide range of tasks, including (i) multimodal grounding, such as referring expression comprehension, and phrase grounding, (ii) multimodal referring, such as referring expression generation, (iii) perception-language tasks, and (iv) language understanding and generation. This work lays out the foundation for the development of Embodiment AI and sheds light on the big convergence of language, multimodal perception, action, and world modeling, which is a key step toward artificial general intelligence. Code and pretrained models are available at https://aka.ms/kosmos-2.\"), Document(metadata={'title': \"Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic\"}, page_content=\"In human conversations, individuals can indicate relevant regions within a scene while addressing others. In turn, the other person can then respond by referring to specific regions if necessary. This natural referential ability in dialogue remains absent in current Multimodal Large Language Models (MLLMs). To fill this gap, this paper proposes an MLLM called Shikra, which can handle spatial coordinate inputs and outputs in natural language. Its architecture consists of a vision encoder, an alignment layer, and a LLM. It is designed to be straightforward and simple, without the need for extra vocabularies, position encoder, pre-/post-detection modules, or external plug-in models. All inputs and outputs are in natural language form. Referential dialogue is a superset of various vision-language (VL) tasks. Shikra can naturally handle location-related tasks like REC and PointQA, as well as conventional VL tasks such as Image Captioning and VQA. Experimental results showcase Shikra's promising performance. Furthermore, it enables numerous exciting applications, like providing mentioned objects' coordinates in chains of thoughts and comparing user-pointed regions similarities. Our code, model and dataset are accessed at https://github.com/shikras/shikra.\"), Document(metadata={'title': 'SVIT: Scaling up Visual Instruction Tuning'}, page_content=\"Thanks to the emerging of foundation models, the large language and vision models are integrated to acquire the multimodal ability of visual captioning, question answering, etc. Although existing multimodal models present impressive performance of visual understanding and reasoning, their limits are still largely under-explored due to the scarcity of high-quality instruction tuning data. To push the limits of multimodal capability, we Scale up Visual Instruction Tuning (SVIT) by constructing a dataset of 4.2 million visual instruction tuning data including 1.6M conversation question-answer (QA) pairs, 1.6M complex reasoning QA pairs, 1.0M referring QA pairs and 106K detailed image descriptions. Besides the volume, the proposed dataset is also featured by the high quality and rich diversity, which is generated by prompting GPT-4 with the abundant manual annotations of images. We also propose a new data recipe to select subset with better diversity and balance, which evokes model's superior capabilities. Extensive experiments verify that SVIT-v1.5, trained on the proposed dataset, outperforms state-of-the-art Multimodal Large Language Models on popular benchmarks. The data and code are publicly available at https://github.com/BAAI-DCAI/Visual-Instruction-Tuning.\"), Document(metadata={'title': 'ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning'}, page_content=\"Human-AI interactivity is a critical aspect that reflects the usability of multimodal large language models (MLLMs). However, existing end-to-end MLLMs only allow users to interact with them through language instructions, leading to the limitation of the interactive accuracy and efficiency. In this study, we present precise referring instructions that utilize diverse reference representations such as points and boxes as referring prompts to refer to the special region. This enables MLLMs to focus on the region of interest and achieve finer-grained interaction. Based on precise referring instruction, we propose ChatSpot, a unified end-to-end multimodal large language model that supports diverse forms of interactivity including mouse clicks, drag-and-drop, and drawing boxes, which provides a more flexible and seamless interactive experience. We also construct a multi-grained vision-language instruction-following dataset based on existing datasets and GPT-4 generating. Furthermore, we design a series of evaluation tasks to assess the effectiveness of region recognition and interaction. Experimental results showcase ChatSpot's promising performance.\"), Document(metadata={'title': 'SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension'}, page_content='Based on powerful Large Language Models (LLMs), recent generative Multimodal Large Language Models (MLLMs) have gained prominence as a pivotal research area, exhibiting remarkable capability for both comprehension and generation. In this work, we address the evaluation of generative comprehension in MLLMs as a preliminary step towards a comprehensive assessment of generative models, by introducing a benchmark named SEED-Bench. SEED-Bench consists of 19K multiple choice questions with accurate human annotations (x 6 larger than existing benchmarks), which spans 12 evaluation dimensions including the comprehension of both the image and video modality. We develop an advanced pipeline for generating multiple-choice questions that target specific evaluation dimensions, integrating both automatic filtering and manual verification processes. Multiple-choice questions with groundtruth options derived from human annotation enables an objective and efficient assessment of model performance, eliminating the need for human or GPT intervention during evaluation. We further evaluate the performance of 18 models across all 12 dimensions, covering both the spatial and temporal understanding. By revealing the limitations of existing MLLMs through evaluation results, we aim for SEED-Bench to provide insights for motivating future research. We will launch and consistently maintain a leaderboard to provide a platform for the community to assess and investigate model capability.'), Document(metadata={'title': 'LISA: Reasoning Segmentation via Large Language Model'}, page_content='Although perception systems have made remarkable advancements in recent years, they still rely on explicit human instruction or pre-defined categories to identify the target objects before executing visual recognition tasks. Such systems cannot actively reason and comprehend implicit user intention. In this work, we propose a new segmentation task -- reasoning segmentation. The task is designed to output a segmentation mask given a complex and implicit query text. Furthermore, we establish a benchmark comprising over one thousand image-instruction-mask data samples, incorporating intricate reasoning and world knowledge for evaluation purposes. Finally, we present LISA: large Language Instructed Segmentation Assistant, which inherits the language generation capabilities of multimodal Large Language Models (LLMs) while also possessing the ability to produce segmentation masks. We expand the original vocabulary with a <SEG> token and propose the embedding-as-mask paradigm to unlock the segmentation capability. Remarkably, LISA can handle cases involving complex reasoning and world knowledge. Also, it demonstrates robust zero-shot capability when trained exclusively on reasoning-free datasets. In addition, fine-tuning the model with merely 239 reasoning segmentation data samples results in further performance enhancement. Both quantitative and qualitative experiments show our method effectively unlocks new reasoning segmentation capabilities for multimodal LLMs. Code, models, and data are available at https://github.com/dvlab-research/LISA.'), Document(metadata={'title': 'Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions'}, page_content=\"Recent advancements in Multimodal Large Language Models (MLLMs) have been utilizing Visual Prompt Generators (VPGs) to convert visual features into tokens that LLMs can recognize. This is achieved by training the VPGs on millions of image-caption pairs, where the VPG-generated tokens of images are fed into a frozen LLM to generate the corresponding captions. However, this image-captioning based training objective inherently biases the VPG to concentrate solely on the primary visual contents sufficient for caption generation, often neglecting other visual details. This shortcoming results in MLLMs' underperformance in comprehending demonstrative instructions consisting of multiple, interleaved, and multimodal instructions that demonstrate the required context to complete a task. To address this issue, we introduce a generic and lightweight Visual Prompt Generator Complete module (VPG-C), which can infer and complete the missing details essential for comprehending demonstrative instructions. Further, we propose a synthetic discriminative training strategy to fine-tune VPG-C, eliminating the need for supervised demonstrative instructions. As for evaluation, we build DEMON, a comprehensive benchmark for demonstrative instruction understanding. Synthetically trained with the proposed strategy, VPG-C achieves significantly stronger zero-shot performance across all tasks of DEMON. Further evaluation on the MME and OwlEval benchmarks also demonstrate the superiority of VPG-C. Our benchmark, code, and pre-trained models are available at https://github.com/DCDmllm/Cheetah.\"), Document(metadata={'title': 'Link-Context Learning for Multimodal LLMs'}, page_content='The ability to learn from context with novel concepts, and deliver appropriate responses are essential in human conversations. Despite current Multimodal Large Language Models (MLLMs) and Large Language Models (LLMs) being trained on mega-scale datasets, recognizing unseen images or understanding novel concepts in a training-free manner remains a challenge. In-Context Learning (ICL) explores training-free few-shot learning, where models are encouraged to ``learn to learn\" from limited tasks and generalize to unseen tasks. In this work, we propose link-context learning (LCL), which emphasizes \"reasoning from cause and effect\" to augment the learning capabilities of MLLMs. LCL goes beyond traditional ICL by explicitly strengthening the causal relationship between the support set and the query set. By providing demonstrations with causal links, LCL guides the model to discern not only the analogy but also the underlying causal associations between data points, which empowers MLLMs to recognize unseen images and understand novel concepts more effectively. To facilitate the evaluation of this novel approach, we introduce the ISEKAI dataset, comprising exclusively of unseen generated image-label pairs designed for link-context learning. Extensive experiments show that our LCL-MLLM exhibits strong link-context learning capabilities to novel concepts over vanilla MLLMs. Code and data will be released at https://github.com/isekai-portal/Link-Context-Learning.'), Document(metadata={'title': 'FashionLOGO: Prompting Multimodal Large Language Models for Fashion Logo Embeddings'}, page_content=\"Logo embedding models convert the product logos in images into vectors, enabling their utilization for logo recognition and detection within e-commerce platforms. This facilitates the enforcement of intellectual property rights and enhances product search capabilities. However, current methods treat logo embedding as a purely visual problem. A noteworthy issue is that visual models capture features more than logos. Instead, we view this as a multimodal task, using text as auxiliary information to facilitate the visual model's understanding of the logo. The emerging Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in both visual and textual understanding. Inspired by this, we propose an approach, \\\\textbf{FashionLOGO}, to explore how to prompt MLLMs to generate appropriate text for product images, which can help visual models achieve better logo embeddings. We adopt a cross-attention transformer block that enables visual embedding to automatically learn supplementary knowledge from textual embedding. Our extensive experiments on real-world datasets prove that FashionLOGO is capable of generating generic and robust logo embeddings, achieving state-of-the-art performance in all benchmarks.\"), Document(metadata={'title': 'WanJuan: A Comprehensive Multimodal Dataset for Advancing English and Chinese Large Models'}, page_content='The rise in popularity of ChatGPT and GPT-4 has significantly accelerated the development of large models, leading to the creation of numerous impressive large language models(LLMs) and multimodal large language models (MLLMs). These cutting-edge models owe their remarkable performance to high-quality data. However, the details of the training data used in leading paradigms are often kept confidential. This lack of transparency, coupled with the scarcity of open-source data, impedes further developments within the community. As a response, this paper presents \"Wan Juan\", a large-scale multimodal dataset composed of both Chinese and English data, collected from a wide range of web sources. The dataset incorporates text, image-text, and video modalities, with a total volume exceeding 2TB. It was utilized in the training of InternLM, a model that demonstrated significant advantages in multi-dimensional evaluations when compared to models of a similar scale. All data can be accessed at https://opendatalab.org.cn/WanJuan1.0.'), Document(metadata={'title': 'StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data'}, page_content=\"The remarkable multimodal capabilities demonstrated by OpenAI's GPT-4 have sparked significant interest in the development of multimodal Large Language Models (LLMs). A primary research objective of such models is to align visual and textual modalities effectively while comprehending human instructions. Current methodologies often rely on annotations derived from benchmark datasets to construct image-dialogue datasets for training purposes, akin to instruction tuning in LLMs. However, these datasets often exhibit domain bias, potentially constraining the generative capabilities of the models. In an effort to mitigate these limitations, we propose a novel data collection methodology that synchronously synthesizes images and dialogues for visual instruction tuning. This approach harnesses the power of generative models, marrying the abilities of ChatGPT and text-to-image generative models to yield a diverse and controllable dataset with varied image content. Additionally, datasets can be arbitrarily scaled. This not only provides greater flexibility compared to existing methodologies but also significantly enhances several model capabilities. Our research includes comprehensive experiments conducted on various datasets. The results emphasize substantial enhancements in more than ten commonly assessed capabilities. Additionally, our model achieves state-of-the-art results across multiple widely recognized multimodal benchmarks.\"), Document(metadata={'title': 'InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4'}, page_content='Multimodal large language models are typically trained in two stages: first pre-training on image-text pairs, and then fine-tuning using supervised vision-language instruction data. Recent studies have shown that large language models can achieve satisfactory results even with a limited amount of high-quality instruction-following data. In this paper, we introduce InstructionGPT-4, which is fine-tuned on a small dataset comprising only 200 examples, amounting to approximately 6\\\\% of the instruction-following data used in the alignment dataset for MiniGPT-4. To achieve this, we first propose several metrics to access the quality of multimodal instruction data. Based on these metrics, we present an effective and trainable data selector to automatically identify and filter low-quality vision-language data. By employing this method, InstructionGPT-4 outperforms the original MiniGPT-4 on various evaluations. Overall, our findings demonstrate that less but high-quality instruction tuning data is efficient in enabling multimodal large language models to generate better output. Our code is available at https://github.com/waltonfuture/InstructionGPT-4.'), Document(metadata={'title': 'VIGC: Visual Instruction Generation and Correction'}, page_content=\"The integration of visual encoders and large language models (LLMs) has driven recent progress in multimodal large language models (MLLMs). However, the scarcity of high-quality instruction-tuning data for vision-language tasks remains a challenge. The current leading paradigm, such as LLaVA, relies on language-only GPT-4 to generate data, which requires pre-annotated image captions and detection bounding boxes, suffering from understanding image details. A practical solution to this problem would be to utilize the available multimodal large language models (MLLMs) to generate instruction data for vision-language tasks. However, it's worth noting that the currently accessible MLLMs are not as powerful as their LLM counterparts, as they tend to produce inadequate responses and generate false information. As a solution for addressing the current issue, this paper proposes the Visual Instruction Generation and Correction (VIGC) framework that enables multimodal large language models to generate instruction-tuning data and progressively enhance its quality on-the-fly. Specifically, Visual Instruction Generation (VIG) guides the vision-language model to generate diverse instruction-tuning data. To ensure generation quality, Visual Instruction Correction (VIC) adopts an iterative update mechanism to correct any inaccuracies in data produced by VIG, effectively reducing the risk of hallucination. Leveraging the diverse, high-quality data generated by VIGC, we finetune mainstream models and validate data quality based on various evaluations. Experimental results demonstrate that VIGC not only compensates for the shortcomings of language-only data generation methods, but also effectively enhances the benchmark performance. The models, datasets, and code are available at https://opendatalab.github.io/VIGC.\"), Document(metadata={'title': 'Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models'}, page_content='Recently, Multimodal Large Language Models (MLLMs) that enable Large Language Models (LLMs) to interpret images through visual instruction tuning have achieved significant success. However, existing visual instruction tuning methods only utilize image-language instruction data to align the language and image modalities, lacking a more fine-grained cross-modal alignment. In this paper, we propose Position-enhanced Visual Instruction Tuning (PVIT), which extends the functionality of MLLMs by integrating an additional region-level vision encoder. This integration promotes a more detailed comprehension of images for the MLLM. In addition, to efficiently achieve a fine-grained alignment between the vision modules and the LLM, we design multiple data generation strategies to construct an image-region-language instruction dataset. Finally, we present both quantitative experiments and qualitative analysis that demonstrate the superiority of the proposed model. Code and data will be released at https://github.com/PVIT-official/PVIT.'), Document(metadata={'title': 'MLLM-DataEngine: An Iterative Refinement Approach for MLLM'}, page_content='Despite the great advance of Multimodal Large Language Models (MLLMs) in both instruction dataset building and benchmarking, the independence of training and evaluation makes current MLLMs hard to further improve their capability under the guidance of evaluation results with a relatively low human cost. In this paper, we propose MLLM-DataEngine, a novel closed-loop system that bridges data generation, model training, and evaluation. Within each loop iteration, the MLLM-DataEngine first analyze the weakness of the model based on the evaluation results, then generate a proper incremental dataset for the next training iteration and enhance the model capability iteratively. Compared with previous data collection methods which are separate from the benchmarking, the data generated by MLLM-DataEngine shows better targeting, quality, and correctness. For targeting, we propose an Adaptive Bad-case Sampling module, which adjusts the ratio of different types of data within each incremental dataset based on the benchmarking results. For quality, we resort to GPT-4 to generate high-quality data with each given data type. For correctness, prompt design is critical for the data generation results. Rather than previous hand-crafted prompt, we propose an Interactive Prompt Optimization strategy, which optimizes the prompt with the multi-round interaction between human and GPT, and improve the correctness of generated data greatly. Through extensive experiments, we find our MLLM-DataEngine could boost the MLLM capability in a targeted and automatic manner, with only a few human participation. We hope it could be a general solution for the following MLLMs building. The MLLM-DataEngine has been open-sourced and is now available at https://github.com/opendatalab/MLLM-DataEngine.'), Document(metadata={'title': 'Socratis: Are large multimodal models emotionally aware?'}, page_content='Existing emotion prediction benchmarks contain coarse emotion labels which do not consider the diversity of emotions that an image and text can elicit in humans due to various reasons. Learning diverse reactions to multimodal content is important as intelligent machines take a central role in generating and delivering content to society. To address this gap, we propose Socratis, a societal reactions benchmark, where each image-caption (IC) pair is annotated with multiple emotions and the reasons for feeling them. Socratis contains 18K free-form reactions for 980 emotions on 2075 image-caption pairs from 5 widely-read news and image-caption (IC) datasets. We benchmark the capability of state-of-the-art multimodal large language models to generate the reasons for feeling an emotion given an IC pair. Based on a preliminary human study, we observe that humans prefer human-written reasons over 2 times more often than machine-generated ones. This shows our task is harder than standard generation tasks because it starkly contrasts recent findings where humans cannot tell apart machine vs human-written news articles, for instance. We further see that current captioning metrics based on large vision-language models also fail to correlate with human preferences. We hope that these findings and our benchmark will inspire further research on training emotionally aware models.'), Document(metadata={'title': 'MultiWay-Adapater: Adapting large-scale multi-modal models for scalable image-text retrieval'}, page_content=\"As Multimodal Large Language Models (MLLMs) grow in size, adapting them to specialized tasks becomes increasingly challenging due to high computational and memory demands. Indeed, traditional fine-tuning methods are costly, due to the need for extensive, task-specific training. While efficient adaptation methods exist that aim to reduce these costs, in practice they suffer from shallow inter-modal alignment, which severely hurts model effectiveness. To tackle these computational challenges and improve inter-modal alignment, we introduce the MultiWay-Adapter (MWA), a novel framework featuring an 'Alignment Enhancer'. This enhancer deepens inter-modal alignment, enabling high transferability with minimal tuning effort. Our experiments show that unlike prior efficient tuning approaches, MWA maintains model effectiveness, while reducing training time by up-to 57%. MWA is also lightweight, increasing model size by only 2-3% (in terms of parameters) for state-of-the-art foundation models like BEiT-3 Large. These results demonstrate that MWA provides an efficient and effective adaptation method for MLLMs, significantly broadening their applicability.\"), Document(metadata={'title': 'NExT-GPT: Any-to-Any Multimodal LLM'}, page_content='While recently Multimodal Large Language Models (MM-LLMs) have made exciting strides, they mostly fall prey to the limitation of only input-side multimodal understanding, without the ability to produce content in multiple modalities. As we humans always perceive the world and communicate with people through various modalities, developing any-to-any MM-LLMs capable of accepting and delivering content in any modality becomes essential to human-level AI. To fill the gap, we present an end-to-end general-purpose any-to-any MM-LLM system, NExT-GPT. We connect an LLM with multimodal adaptors and different diffusion decoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio. By leveraging the existing well-trained highly-performing encoders and decoders, NExT-GPT is tuned with only a small amount of parameter (1%) of certain projection layers, which not only benefits low-cost training and also facilitates convenient expansion to more potential modalities. Moreover, we introduce a modality-switching instruction tuning (MosIT) and manually curate a high-quality dataset for MosIT, based on which NExT-GPT is empowered with complex cross-modal semantic understanding and content generation. Overall, our research showcases the promising possibility of building an AI agent capable of modeling universal modalities, paving the way for more human-like AI research in the community. Project page: https://next-gpt.github.io/'), Document(metadata={'title': 'HiLM-D: Towards High-Resolution Understanding in Multimodal Large Language Models for Autonomous Driving'}, page_content=\"Autonomous driving systems generally employ separate models for different tasks resulting in intricate designs. For the first time, we leverage singular multimodal large language models (MLLMs) to consolidate multiple autonomous driving tasks from videos, i.e., the Risk Object Localization and Intention and Suggestion Prediction (ROLISP) task. ROLISP uses natural language to simultaneously identify and interpret risk objects, understand ego-vehicle intentions, and provide motion suggestions, eliminating the necessity for task-specific architectures. However, lacking high-resolution (HR) information, existing MLLMs often miss small objects (e.g., traffic cones) and overly focus on salient ones (e.g., large trucks) when applied to ROLISP. We propose HiLM-D (Towards High-Resolution Understanding in MLLMs for Autonomous Driving), an efficient method to incorporate HR information into MLLMs for the ROLISP task. Especially, HiLM-D integrates two branches: (i) the low-resolution reasoning branch, can be any MLLMs, processes low-resolution videos to caption risk objects and discern ego-vehicle intentions/suggestions; (ii) the high-resolution perception branch (HR-PB), prominent to HiLM-D,, ingests HR images to enhance detection by capturing vision-specific HR feature maps and prioritizing all potential risks over merely salient objects. Our HR-PB serves as a plug-and-play module, seamlessly fitting into current MLLMs. Experiments on the ROLISP benchmark reveal HiLM-D's notable advantage over leading MLLMs, with improvements of 4.8% in BLEU-4 for captioning and 17.2% in mIoU for detection.\"), Document(metadata={'title': 'Investigating the Catastrophic Forgetting in Multimodal Large Language Models'}, page_content='Following the success of GPT4, there has been a surge in interest in multimodal large language model (MLLM) research. This line of research focuses on developing general-purpose LLMs through fine-tuning pre-trained LLMs and vision models. However, catastrophic forgetting, a notorious phenomenon where the fine-tuned model fails to retain similar performance compared to the pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM). In this paper, we introduce EMT: Evaluating MulTimodality for evaluating the catastrophic forgetting in MLLMs, by treating each MLLM as an image classifier. We first apply EMT to evaluate several open-source fine-tuned MLLMs and we discover that almost all evaluated MLLMs fail to retain the same performance levels as their vision encoders on standard image classification tasks. Moreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess performance throughout the fine-tuning. Interestingly, our results suggest that early-stage fine-tuning on an image dataset improves performance across other image datasets, by enhancing the alignment of text and visual features. However, as fine-tuning proceeds, the MLLMs begin to hallucinate, resulting in a significant loss of generalizability, even when the image encoder remains frozen. Our results suggest that MLLMs have yet to demonstrate performance on par with their vision models on standard image classification tasks and the current MLLM fine-tuning procedure still has room for improvement.'), Document(metadata={'title': 'DreamLLM: Synergistic Multimodal Comprehension and Creation'}, page_content=\"This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy. Project page: https://dreamllm.github.io.\"), Document(metadata={'title': \"How Robust is Google's Bard to Adversarial Image Attacks?\"}, page_content=\"Multimodal Large Language Models (MLLMs) that integrate text and other modalities (especially vision) have achieved unprecedented performance in various multimodal tasks. However, due to the unsolved adversarial robustness problem of vision models, MLLMs can have more severe safety and security risks by introducing the vision inputs. In this work, we study the adversarial robustness of Google's Bard, a competitive chatbot to ChatGPT that released its multimodal capability recently, to better understand the vulnerabilities of commercial MLLMs. By attacking white-box surrogate vision encoders or MLLMs, the generated adversarial examples can mislead Bard to output wrong image descriptions with a 22% success rate based solely on the transferability. We show that the adversarial examples can also attack other MLLMs, e.g., a 26% attack success rate against Bing Chat and a 86% attack success rate against ERNIE bot. Moreover, we identify two defense mechanisms of Bard, including face detection and toxicity detection of images. We design corresponding attacks to evade these defenses, demonstrating that the current defenses of Bard are also vulnerable. We hope this work can deepen our understanding on the robustness of MLLMs and facilitate future research on defenses. Our code is available at https://github.com/thu-ml/Attack-Bard. Update: GPT-4V is available at October 2023. We further evaluate its robustness under the same set of adversarial examples, achieving a 45% attack success rate.\"), Document(metadata={'title': 'VDC: Versatile Data Cleanser based on Visual-Linguistic Inconsistency by Multimodal Large Language Models'}, page_content='The role of data in building AI systems has recently been emphasized by the emerging concept of data-centric AI. Unfortunately, in the real-world, datasets may contain dirty samples, such as poisoned samples from backdoor attack, noisy labels in crowdsourcing, and even hybrids of them. The presence of such dirty samples makes the DNNs vunerable and unreliable.Hence, it is critical to detect dirty samples to improve the quality and realiability of dataset. Existing detectors only focus on detecting poisoned samples or noisy labels, that are often prone to weak generalization when dealing with dirty samples from other domains.In this paper, we find a commonality of various dirty samples is visual-linguistic inconsistency between images and associated labels. To capture the semantic inconsistency between modalities, we propose versatile data cleanser (VDC) leveraging the surpassing capabilities of multimodal large language models (MLLM) in cross-modal alignment and reasoning.It consists of three consecutive modules: the visual question generation module to generate insightful questions about the image; the visual question answering module to acquire the semantics of the visual content by answering the questions with MLLM; followed by the visual answer evaluation module to evaluate the inconsistency.Extensive experiments demonstrate its superior performance and generalization to various categories and types of dirty samples. The code is available at \\\\url{https://github.com/zihao-ai/vdc}.'), Document(metadata={'title': 'Guiding Instruction-based Image Editing via Multimodal Large Language Models'}, page_content='Instruction-based image editing improves the controllability and flexibility of image manipulation via natural commands without elaborate descriptions or regional masks. However, human instructions are sometimes too brief for current methods to capture and follow. Multimodal large language models (MLLMs) show promising capabilities in cross-modal understanding and visual-aware response generation via LMs. We investigate how MLLMs facilitate edit instructions and present MLLM-Guided Image Editing (MGIE). MGIE learns to derive expressive instructions and provides explicit guidance. The editing model jointly captures this visual imagination and performs manipulation through end-to-end training. We evaluate various aspects of Photoshop-style modification, global photo optimization, and local editing. Extensive experimental results demonstrate that expressive instructions are crucial to instruction-based image editing, and our MGIE can lead to a notable improvement in automatic metrics and human evaluation while maintaining competitive inference efficiency.'), Document(metadata={'title': 'Reformulating Vision-Language Foundation Models and Datasets Towards Universal Multimodal Assistants'}, page_content=\"Recent Multimodal Large Language Models (MLLMs) exhibit impressive abilities to perceive images and follow open-ended instructions. The capabilities of MLLMs depend on two crucial factors: the model architecture to facilitate the feature alignment of visual modules and large language models; the multimodal instruction tuning datasets for human instruction following. (i) For the model architecture, most existing models introduce an external bridge module to connect vision encoders with language models, which needs an additional feature-alignment pre-training. In this work, we discover that compact pre-trained vision language models can inherently serve as ``out-of-the-box'' bridges between vision and language. Based on this, we propose Muffin framework, which directly employs pre-trained vision-language models to act as providers of visual signals. (ii) For the multimodal instruction tuning datasets, existing methods omit the complementary relationship between different datasets and simply mix datasets from different tasks. Instead, we propose UniMM-Chat dataset which explores the complementarities of datasets to generate 1.1M high-quality and diverse multimodal instructions. We merge information describing the same image from diverse datasets and transforms it into more knowledge-intensive conversation data. Experimental results demonstrate the effectiveness of the Muffin framework and UniMM-Chat dataset. Muffin achieves state-of-the-art performance on a wide range of vision-language tasks, significantly surpassing state-of-the-art models like LLaVA and InstructBLIP. Our model and dataset are all accessible at https://github.com/thunlp/muffin.\"), Document(metadata={'title': 'DriveGPT4: Interpretable End-to-end Autonomous Driving via Large Language Model'}, page_content='Multimodal large language models (MLLMs) have emerged as a prominent area of interest within the research community, given their proficiency in handling and reasoning with non-textual data, including images and videos. This study seeks to extend the application of MLLMs to the realm of autonomous driving by introducing DriveGPT4, a novel interpretable end-to-end autonomous driving system based on LLMs. Capable of processing multi-frame video inputs and textual queries, DriveGPT4 facilitates the interpretation of vehicle actions, offers pertinent reasoning, and effectively addresses a diverse range of questions posed by users. Furthermore, DriveGPT4 predicts low-level vehicle control signals in an end-to-end fashion.These advanced capabilities are achieved through the utilization of a bespoke visual instruction tuning dataset, specifically tailored for autonomous driving applications, in conjunction with a mix-finetuning training strategy. DriveGPT4 represents the pioneering effort to leverage LLMs for the development of an interpretable end-to-end autonomous driving solution. Evaluations conducted on the BDD-X dataset showcase the superior qualitative and quantitative performance of DriveGPT4. Additionally, the fine-tuning of domain-specific data enables DriveGPT4 to yield close or even improved results in terms of autonomous driving grounding when contrasted with GPT4-V.'), Document(metadata={'title': 'MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens'}, page_content='The effectiveness of Multimodal Large Language Models (MLLMs) demonstrates a profound capability in multimodal understanding. However, the simultaneous generation of images with coherent texts is still underdeveloped. Addressing this, we introduce a novel interleaved vision-and-language generation method, centered around the concept of ``generative vokens\". These vokens serve as pivotal elements contributing to coherent image-text outputs. Our method is marked by a unique two-stage training strategy for description-free multimodal generation, which does not necessitate extensive descriptions of images. We integrate classifier-free guidance to enhance the alignment of generated images and texts, ensuring more seamless and contextually relevant multimodal interactions. Our model, MiniGPT-5, exhibits substantial improvement over the baseline models on multimodal generation datasets, including MMDialog and VIST. The human evaluation shows MiniGPT-5 is better than the baseline model on more than 56\\\\% cases for multimodal generation, highlighting its efficacy across diverse benchmarks.'), Document(metadata={'title': 'Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond'}, page_content='In this study, we explore the potential of Multimodal Large Language Models (MLLMs) in improving embodied decision-making processes for agents. While Large Language Models (LLMs) have been widely used due to their advanced reasoning skills and vast world knowledge, MLLMs like GPT4-Vision offer enhanced visual understanding and reasoning capabilities. We investigate whether state-of-the-art MLLMs can handle embodied decision-making in an end-to-end manner and whether collaborations between LLMs and MLLMs can enhance decision-making. To address these questions, we introduce a new benchmark called PCA-EVAL, which evaluates embodied decision-making from the perspectives of Perception, Cognition, and Action. Additionally, we propose HOLMES, a multi-agent cooperation framework that allows LLMs to leverage MLLMs and APIs to gather multimodal information for informed decision-making. We compare end-to-end embodied decision-making and HOLMES on our benchmark and find that the GPT4-Vision model demonstrates strong end-to-end embodied decision-making abilities, outperforming GPT4-HOLMES in terms of average decision accuracy (+3%). However, this performance is exclusive to the latest GPT4-Vision model, surpassing the open-source state-of-the-art MLLM by 26%. Our results indicate that powerful MLLMs like GPT4-Vision hold promise for decision-making in embodied agents, offering new avenues for MLLM research. Code and data are open at https://github.com/pkunlp-icler/PCA-EVAL/.'), Document(metadata={'title': 'Kosmos-G: Generating Images in Context with Multimodal Large Language Models'}, page_content='Recent advancements in subject-driven image generation have made significant strides. However, current methods still fall short in diverse application scenarios, as they require test-time tuning and cannot accept interleaved multi-image and text input. These limitations keep them far from the ultimate goal of \"image as a foreign language in image generation.\" This paper presents Kosmos-G, a model that leverages the advanced multimodal perception capabilities of Multimodal Large Language Models (MLLMs) to tackle the aforementioned challenge. Our approach aligns the output space of MLLM with CLIP using the textual modality as an anchor and performs compositional instruction tuning on curated data. Kosmos-G demonstrates an impressive capability of zero-shot subject-driven generation with interleaved multi-image and text input. Notably, the score distillation instruction tuning requires no modifications to the image decoder. This allows for a seamless substitution of CLIP and effortless integration with a myriad of U-Net techniques ranging from fine-grained controls to personalized image decoder variants. We posit Kosmos-G as an initial attempt towards the goal of \"image as a foreign language in image generation.\" The code can be found at https://aka.ms/Kosmos-G'), Document(metadata={'title': 'Fine-grained Audio-Visual Joint Representations for Multimodal Large Language Models'}, page_content='Audio-visual large language models (LLM) have drawn significant attention, yet the fine-grained combination of both input streams is rather under-explored, which is challenging but necessary for LLMs to understand general video inputs. To this end, a fine-grained audio-visual joint representation (FAVOR) learning framework for multimodal LLMs is proposed in this paper, which extends a text-based LLM to simultaneously perceive speech and audio events in the audio input stream and images or videos in the visual input stream, at the frame level. To fuse the audio and visual feature streams into joint representations and to align the joint space with the LLM input embedding space, we propose a causal Q-Former structure with a causal attention module to enhance the capture of causal relations of the audio-visual frames across time. An audio-visual evaluation benchmark (AVEB) is also proposed which comprises six representative single-modal tasks with five cross-modal tasks reflecting audio-visual co-reasoning abilities. While achieving competitive single-modal performance on audio, speech and image tasks in AVEB, FAVOR achieved over 20% accuracy improvements on the video question-answering task when fine-grained information or temporal causal reasoning is required. FAVOR, in addition, demonstrated remarkable video comprehension and reasoning abilities on tasks that are unprecedented by other multimodal LLMs. An interactive demo of FAVOR is available at https://github.com/BriansIDP/AudioVisualLLM.git, and the training code and model checkpoints will be released soon.'), Document(metadata={'title': 'Can We Edit Multimodal Large Language Models?'}, page_content='In this paper, we focus on editing Multimodal Large Language Models (MLLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights. Code and dataset are available in https://github.com/zjunlp/EasyEdit.'), Document(metadata={'title': 'AutoRepo: A general framework for multi-modal LLM-based automated construction reporting'}, page_content='Ensuring the safety, quality, and timely completion of construction projects is paramount, with construction inspections serving as a vital instrument towards these goals. Nevertheless, the predominantly manual approach of present-day inspections frequently results in inefficiencies and inadequate information management. Such methods often fall short of providing holistic, exhaustive assessments, consequently engendering regulatory oversights and potential safety hazards. To address this issue, this paper presents a novel framework named AutoRepo for automated generation of construction inspection reports. The unmanned vehicles efficiently perform construction inspections and collect scene information, while the multimodal large language models (LLMs) are leveraged to automatically generate the inspection reports. The framework was applied and tested on a real-world construction site, demonstrating its potential to expedite the inspection process, significantly reduce resource allocation, and produce high-quality, regulatory standard-compliant inspection reports. This research thus underscores the immense potential of multimodal large language models in revolutionizing construction inspection practices, signaling a significant leap forward towards a more efficient and safer construction management paradigm.'), Document(metadata={'title': 'MM-BigBench: Evaluating Multimodal Models on Multimodal Content Comprehension Tasks'}, page_content=\"The popularity of multimodal large language models (MLLMs) has triggered a recent surge in research efforts dedicated to evaluating these models. Nevertheless, existing evaluation studies of MLLMs primarily focus on the comprehension and reasoning of unimodal (vision) content, neglecting performance evaluations in the domain of multimodal (vision-language) content understanding. Beyond multimodal reasoning, tasks related to multimodal content comprehension necessitate a profound understanding of multimodal contexts, achieved through the multimodal interaction to obtain a final answer. In this paper, we introduce a comprehensive assessment framework called MM-BigBench, which incorporates a diverse range of metrics to offer an extensive evaluation of the performance of various models and instructions across a wide spectrum of diverse multimodal content comprehension tasks. Consequently, our work complements research on the performance of MLLMs in multimodal comprehension tasks, achieving a more comprehensive and holistic evaluation of MLLMs. To begin, we employ the Best Performance metric to ascertain each model's performance upper bound on different datasets. Subsequently, the Mean Relative Gain metric offers an assessment of the overall performance of various models and instructions, while the Stability metric measures their sensitivity. Furthermore, previous research centers on evaluating models independently or solely assessing instructions, neglecting the adaptability between models and instructions. We propose the Adaptability metric to quantify the adaptability between models and instructions. Our paper evaluates a total of 20 language models (14 MLLMs) on 14 multimodal datasets spanning 6 tasks, with 10 instructions for each task, and derives novel insights. Our code will be released at https://github.com/declare-lab/MM-BigBench.\"), Document(metadata={'title': 'RoboLLM: Robotic Vision Tasks Grounded on Multimodal Large Language Models'}, page_content='Robotic vision applications often necessitate a wide range of visual perception tasks, such as object detection, segmentation, and identification. While there have been substantial advances in these individual tasks, integrating specialized models into a unified vision pipeline presents significant engineering challenges and costs. Recently, Multimodal Large Language Models (MLLMs) have emerged as novel backbones for various downstream tasks. We argue that leveraging the pre-training capabilities of MLLMs enables the creation of a simplified framework, thus mitigating the need for task-specific encoders. Specifically, the large-scale pretrained knowledge in MLLMs allows for easier fine-tuning to downstream robotic vision tasks and yields superior performance. We introduce the RoboLLM framework, equipped with a BEiT-3 backbone, to address all visual perception tasks in the ARMBench challenge-a large-scale robotic manipulation dataset about real-world warehouse scenarios. RoboLLM not only outperforms existing baselines but also substantially reduces the engineering burden associated with model selection and tuning. The source code is publicly available at https://github.com/longkukuhi/armbench.'), Document(metadata={'title': 'Benchmarking Sequential Visual Input Reasoning and Prediction in Multimodal Large Language Models'}, page_content=\"Multimodal large language models (MLLMs) have shown great potential in perception and interpretation tasks, but their capabilities in predictive reasoning remain under-explored. To address this gap, we introduce a novel benchmark that assesses the predictive reasoning capabilities of MLLMs across diverse scenarios. Our benchmark targets three important domains: abstract pattern reasoning, human activity prediction, and physical interaction prediction. We further develop three evaluation methods powered by large language model to robustly quantify a model's performance in predicting and reasoning the future based on multi-visual context. Empirical experiments confirm the soundness of the proposed benchmark and evaluation methods via rigorous testing and reveal pros and cons of current popular MLLMs in the task of predictive reasoning. Lastly, our proposed benchmark provides a standardized evaluation framework for MLLMs and can facilitate the development of more advanced models that can reason and predict over complex long sequence of multimodal input.\"), Document(metadata={'title': 'Woodpecker: Hallucination Correction for Multimodal Large Language Models'}, page_content='Hallucination is a big shadow hanging over the rapidly evolving Multimodal Large Language Models (MLLMs), referring to the phenomenon that the generated text is inconsistent with the image content. In order to mitigate hallucinations, existing studies mainly resort to an instruction-tuning manner that requires retraining the models with specific data. In this paper, we pave a different way, introducing a training-free method named Woodpecker. Like a woodpecker heals trees, it picks out and corrects hallucinations from the generated text. Concretely, Woodpecker consists of five stages: key concept extraction, question formulation, visual knowledge validation, visual claim generation, and hallucination correction. Implemented in a post-remedy manner, Woodpecker can easily serve different MLLMs, while being interpretable by accessing intermediate outputs of the five stages. We evaluate Woodpecker both quantitatively and qualitatively and show the huge potential of this new paradigm. On the POPE benchmark, our method obtains a 30.66%/24.33% improvement in accuracy over the baseline MiniGPT-4/mPLUG-Owl. The source code is released at https://github.com/BradyFU/Woodpecker.'), Document(metadata={'title': 'Towards Perceiving Small Visual Details in Zero-shot Visual Question Answering with Multimodal LLMs'}, page_content=\"Multimodal Large Language Models (MLLMs) have recently achieved promising zero-shot accuracy on visual question answering (VQA) -- a fundamental task affecting various downstream applications and domains. Given the great potential for the broad use of these models, it is important to investigate their limitations in dealing with different image and question properties. In this work, we investigate whether MLLMs can perceive small details as well as large details in images. In particular, we show that their zero-shot accuracy in answering visual questions is very sensitive to the size of the visual subject of the question, declining up to 46% with size. Furthermore, we show that this effect is causal by observing that human visual cropping can significantly mitigate their sensitivity to size. Inspired by the usefulness of human cropping, we then propose five automatic visual cropping methods -- leveraging either external localization models or the decision process of the given MLLM itself -- as inference time mechanisms to improve the zero-shot performance of MLLMs. We study their effectiveness on four popular VQA datasets, and a subset of the VQAv2 dataset tailored towards fine visual details. Our findings suggest that MLLMs should be used with caution in detail-sensitive VQA applications, and that visual cropping is a promising direction to improve their zero-shot performance. To facilitate further investigation of MLLMs' behaviors, our code and data are publicly released.\"), Document(metadata={'title': 'ChEF: A Comprehensive Evaluation Framework for Standardized Assessment of Multimodal Large Language Models'}, page_content=\"Multimodal Large Language Models (MLLMs) have shown impressive abilities in interacting with visual content with myriad potential downstream tasks. However, even though a list of benchmarks has been proposed, the capabilities and limitations of MLLMs are still not comprehensively understood, due to a lack of a standardized and holistic evaluation framework. To this end, we present the first Comprehensive Evaluation Framework (ChEF) that can holistically profile each MLLM and fairly compare different MLLMs. First, we structure ChEF as four modular components, i.e., Scenario as scalable multimodal datasets, Instruction as flexible instruction retrieving formulae, Inferencer as reliable question answering strategies, and Metric as indicative task-specific score functions. Based on them, ChEF facilitates versatile evaluations in a standardized framework, and new evaluations can be built by designing new Recipes (systematic selection of these four components). Notably, current MLLM benchmarks can be readily summarized as recipes of ChEF. Second, we introduce 6 new recipes to quantify competent MLLMs' desired capabilities (or called desiderata, i.e., calibration, in-context learning, instruction following, language performance, hallucination, and robustness) as reliable agents that can perform real-world multimodal interactions. Third, we conduct a large-scale evaluation of 9 prominent MLLMs on 9 scenarios and 6 desiderata. Our evaluation summarized over 20 valuable observations concerning the generalizability of MLLMs across various scenarios and the composite capability of MLLMs required for multimodal interactions. We will publicly release all the detailed implementations for further analysis, as well as an easy-to-use modular toolkit for the integration of new recipes and models, so that ChEF can be a growing evaluation framework for the MLLM community.\"), Document(metadata={'title': 'Octavius: Mitigating Task Interference in MLLMs via LoRA-MoE'}, page_content='Recent studies have demonstrated Large Language Models (LLMs) can extend their zero-shot generalization capabilities to multimodal learning through instruction tuning. As more modalities and downstream tasks are introduced, negative conflicts and interference may have a worse impact on performance. While this phenomenon has been overlooked in previous work, we propose a novel and extensible framework, called Octavius, for comprehensive studies and experimentation on multimodal learning with Multimodal Large Language Models (MLLMs). Specifically, we combine the well-known Mixture-of-Experts (MoE) and one of the representative PEFT techniques, i.e., LoRA, designing a novel LLM-based decoder, called LoRA-MoE, for multimodal learning. To the best of our knowledge, we are one of the pioneering efforts to introduce MoE into MLLMs to address this problem. The experimental results (about 20% improvement) have shown the effectiveness and versatility of our design in various 2D and 3D downstream tasks. Code and datasets are available at https://openlamm.github.io/tutorial/.'), Document(metadata={'title': 'What Large Language Models Bring to Text-rich VQA?'}, page_content=\"Text-rich VQA, namely Visual Question Answering based on text recognition in the images, is a cross-modal task that requires both image comprehension and text recognition. In this work, we focus on investigating the advantages and bottlenecks of LLM-based approaches in addressing this problem. To address the above concern, we separate the vision and language modules, where we leverage external OCR models to recognize texts in the image and Large Language Models (LLMs) to answer the question given texts. The whole framework is training-free benefiting from the in-context ability of LLMs. This pipeline achieved superior performance compared to the majority of existing Multimodal Large Language Models (MLLM) on four text-rich VQA datasets. Besides, based on the ablation study, we find that LLM brings stronger comprehension ability and may introduce helpful knowledge for the VQA problem. The bottleneck for LLM to address text-rich VQA problems may primarily lie in visual part. We also combine the OCR module with MLLMs and pleasantly find that the combination of OCR module with MLLM also works. It's worth noting that not all MLLMs can comprehend the OCR information, which provides insights into how to train an MLLM that preserves the abilities of LLM.\")], 'results': [Document(id='9e9ac81d-692f-441b-8926-4b6ba73ad784', metadata={'title': 'A Survey on Multimodal Large Language Models'}, page_content='Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.'), Document(id='128c9e1b-adb3-49e0-a873-881354331b12', metadata={'title': 'A Survey on Multimodal Large Language Models'}, page_content='Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.'), Document(id='01a7f21a-97a8-45a2-a73c-74526485470f', metadata={'title': 'A Survey on Multimodal Large Language Models'}, page_content='Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.')], 'context': '[1] A Survey on Multimodal Large Language Models\\nRecently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.\\n\\n[2] A Survey on Multimodal Large Language Models\\nRecently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.\\n\\n[3] A Survey on Multimodal Large Language Models\\nRecently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.', 'messages': [], 'retry_count': 2}\n{'query': \"latest advances in multimodal learning Consider User's response for the search query: I dont want to know about latest advancements, I want to know about multimodal llms\", 'search_query': 'multimodal large language models', 'papers': [{'id': 'multimodal-large-language-models-for', 'arxiv_id': None, 'nips_id': None, 'url_abs': 'https://aclanthology.org/2022.naacl-srw.26', 'url_pdf': 'https://aclanthology.org/2022.naacl-srw.26.pdf', 'title': 'Multimodal large language models for inclusive collaboration learning tasks', 'abstract': 'This PhD project leverages advancements in multimodal large language models to build an inclusive collaboration feedback loop, in order to facilitate the automated detection, modeling, and feedback for participants developing general collaboration skills. This topic is important given the role of collaboration as an essential 21st century skill, the potential to ground large language models within learning theory and real-world practice, and the expressive potential of transformer models to support equity and inclusion. We address some concerns of integrating advances in natural language processing into downstream tasks such as the learning analytics feedback loop.', 'authors': ['Armanda Lewis'], 'published': None, 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': 'naacl-acl-2022-7'}, {'id': 'personality-aware-human-centric-multimodal', 'arxiv_id': '2304.02313', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2304.02313v2', 'url_pdf': 'https://arxiv.org/pdf/2304.02313v2.pdf', 'title': 'Personality-aware Human-centric Multimodal Reasoning: A New Task, Dataset and Baselines', 'abstract': \"Personality traits, emotions, and beliefs shape individuals' behavioral choices and decision-making processes. However, for one thing, the affective computing community normally focused on predicting personality traits but overlooks their application in behavior prediction. For another, the multimodal reasoning task emphasized the prediction of future states and behaviors but often neglected the incorporation of individual personality traits. In this work, we introduce a new task called Personality-aware Human-centric Multimodal Reasoning (PHMR) (T1), with the goal of forecasting the future behavior of a particular individual using multimodal information from past instances, while integrating personality factors. We accordingly construct a new dataset based on six television shows, encompassing 225 characters and 12k samples. To establish a benchmark for the task, we propose seven baseline methods: three adapted from related tasks, two pre-trained model, and two multimodal large language models. The experimental results demonstrate that incorporating personality traits enhances human-centric multimodal reasoning performance. To further solve the lack of personality annotation in real-life scenes, we introduce an extension task called Personality-predicted Human-centric Multimodal Reasoning task (T2) along with the corresponding dataset and method. We will make our dataset and code available on GitHub.\", 'authors': ['Rui Xia', 'Xiangqing Shen', 'Yaochen Zhu'], 'published': '2023-04-05', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'visual-transformation-telling', 'arxiv_id': '2305.01928', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2305.01928v2', 'url_pdf': 'https://arxiv.org/pdf/2305.01928v2.pdf', 'title': 'Visual Transformation Telling', 'abstract': 'Humans can naturally reason from superficial state differences (e.g. ground wetness) to transformations descriptions (e.g. raining) according to their life experience. In this paper, we propose a new visual reasoning task to test this transformation reasoning ability in real-world scenarios, called \\\\textbf{V}isual \\\\textbf{T}ransformation \\\\textbf{T}elling (VTT). Given a series of states (i.e. images), VTT requires to describe the transformation occurring between every two adjacent states. Different from existing visual reasoning tasks that focus on surface state reasoning, the advantage of VTT is that it captures the underlying causes, e.g. actions or events, behind the differences among states. We collect a novel dataset to support the study of transformation reasoning from two existing instructional video datasets, CrossTask and COIN, comprising 13,547 samples. Each sample involves the key state images along with their transformation descriptions. Our dataset covers diverse real-world activities, providing a rich resource for training and evaluation. To construct an initial benchmark for VTT, we test several models, including traditional visual storytelling methods (CST, GLACNet, Densecap) and advanced multimodal large language models (LLaVA v1.5-7B, Qwen-VL-chat, Gemini Pro Vision, GPT-4o, and GPT-4). Experimental results reveal that even state-of-the-art models still face challenges in VTT, highlighting substantial areas for improvement.', 'authors': ['Liang Pang', 'Xin Hong', 'Wanqing Cui', 'Xueqi Cheng', 'Jiafeng Guo', 'Yanyan Lan'], 'published': '2023-05-03', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'listen-think-and-understand', 'arxiv_id': '2305.10790', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2305.10790v3', 'url_pdf': 'https://arxiv.org/pdf/2305.10790v3.pdf', 'title': 'Listen, Think, and Understand', 'abstract': 'The ability of artificial intelligence (AI) systems to perceive and comprehend audio signals is crucial for many applications. Although significant progress has been made in this area since the development of AudioSet, most existing models are designed to map audio inputs to pre-defined, discrete sound label sets. In contrast, humans possess the ability to not only classify sounds into general categories, but also to listen to the finer details of the sounds, explain the reason for the predictions, think about what the sound infers, and understand the scene and what action needs to be taken, if any. Such capabilities beyond perception are not yet present in existing audio models. On the other hand, modern large language models (LLMs) exhibit emerging reasoning ability but they lack audio perception capabilities. Therefore, we ask the question: can we build a model that has both audio perception and a reasoning ability? In this paper, we propose a new audio foundation model, called LTU (Listen, Think, and Understand). To train LTU, we created a new OpenAQA-5M dataset consisting of 1.9 million closed-ended and 3.7 million open-ended, diverse (audio, question, answer) tuples, and have used an autoregressive training framework with a perception-to-understanding curriculum. LTU demonstrates strong performance and generalization ability on conventional audio tasks such as classification and captioning. More importantly, it exhibits emerging audio reasoning and comprehension abilities that are absent in existing audio models. To the best of our knowledge, LTU is one of the first multimodal large language models that focus on general audio (rather than just speech) understanding.', 'authors': ['James Glass', 'Leonid Karlinsky', 'Alexander H. Liu', 'Hongyin Luo', 'Yuan Gong'], 'published': '2023-05-18', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'what-makes-for-good-visual-tokenizers-for', 'arxiv_id': '2305.12223', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2305.12223v2', 'url_pdf': 'https://arxiv.org/pdf/2305.12223v2.pdf', 'title': 'What Makes for Good Visual Tokenizers for Large Language Models?', 'abstract': 'We empirically investigate proper pre-training methods to build good visual tokenizers, making Large Language Models (LLMs) powerful Multimodal Large Language Models (MLLMs). In our benchmark, which is curated to evaluate MLLMs visual semantic understanding and fine-grained perception capabilities, we discussed different visual tokenizers pre-trained with dominant methods (i.e., DeiT, CLIP, MAE, DINO), and observe that: i) Fully/weakly supervised models capture more semantics than self-supervised models, but the gap is narrowed by scaling up the pre-training dataset. ii) Self-supervised models are better at fine-grained perception, where patch-level supervision is particularly effective. iii) Tuning the visual tokenizer leads to the loss of semantics obtained from large-scale pretraining, which is unfavorable with relatively small-scale instruction-tuning dataset. Given the findings, we reviewed methods that attempted to unify semantics and fine-grained visual understanding, e.g., patch-level feature distillation with semantically-rich targets. We obtain an intriguing insight mask-based strategies that were once all the rage may not be applicable for obtaining good visual tokenizers. Based on this critical observation, we obtain a new MLLM equipped with a tailored Good Visual Tokenizer (GVT), which exhibits strong visual comprehension capability at multiple scales. In particular, without introducing extra parameters and task-specific fine-tuning, GVT achieves superior performance on visual question answering, image captioning, and other fine-grained visual understanding tasks such as object counting and multi-class identification.', 'authors': ['Ying Shan', 'Mohan Kankanhalli', 'Xiaohan Ding', 'Yixiao Ge', 'Guangzhi Wang'], 'published': '2023-05-20', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'interpretable-by-design-visual-question', 'arxiv_id': '2305.14882', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2305.14882v2', 'url_pdf': 'https://arxiv.org/pdf/2305.14882v2.pdf', 'title': 'Dynamic Clue Bottlenecks: Towards Interpretable-by-Design Visual Question Answering', 'abstract': 'Recent advances in multimodal large language models (LLMs) have shown extreme effectiveness in visual question answering (VQA). However, the design nature of these end-to-end models prevents them from being interpretable to humans, undermining trust and applicability in critical domains. While post-hoc rationales offer certain insight into understanding model behavior, these explanations are not guaranteed to be faithful to the model. In this paper, we address these shortcomings by introducing an interpretable by design model that factors model decisions into intermediate human-legible explanations, and allows people to easily understand why a model fails or succeeds. We propose the Dynamic Clue Bottleneck Model ( (DCLUB), a method that is designed towards an inherently interpretable VQA system. DCLUB provides an explainable intermediate space before the VQA decision and is faithful from the beginning, while maintaining comparable performance to black-box systems. Given a question, DCLUB first returns a set of visual clues: natural language statements of visually salient evidence from the image, and then generates the output based solely on the visual clues. To supervise and evaluate the generation of VQA explanations within DCLUB, we collect a dataset of 1.7k reasoning-focused questions with visual clues. Evaluations show that our inherently interpretable system can improve 4.64% over a comparable black-box system in reasoning-focused questions while preserving 99.43% of performance on VQA-v2.', 'authors': ['Dan Roth', 'Mark Yatskar', 'Sihao Chen', 'Ben Zhou', 'Xingyu Fu'], 'published': '2023-05-24', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'pathasst-redefining-pathology-through', 'arxiv_id': '2305.15072', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2305.15072v2', 'url_pdf': 'https://arxiv.org/pdf/2305.15072v2.pdf', 'title': 'PathAsst: A Generative Foundation AI Assistant Towards Artificial General Intelligence of Pathology', 'abstract': \"As advances in large language models (LLMs) and multimodal techniques continue to mature, the development of general-purpose multimodal large language models (MLLMs) has surged, offering significant applications in interpreting natural images. However, the field of pathology has largely remained untapped, particularly in gathering high-quality data and designing comprehensive model frameworks. To bridge the gap in pathology MLLMs, we present PathAsst, a multimodal generative foundation AI assistant to revolutionize diagnostic and predictive analytics in pathology. The development of PathAsst involves three pivotal steps: data acquisition, CLIP model adaptation, and the training of PathAsst's multimodal generative capabilities. Firstly, we collect over 207K high-quality pathology image-text pairs from authoritative sources. Leveraging the advanced power of ChatGPT, we generate over 180K instruction-following samples. Furthermore, we devise additional instruction-following data specifically tailored for invoking eight pathology-specific sub-models we prepared, allowing the PathAsst to effectively collaborate with these models, enhancing its diagnostic ability. Secondly, by leveraging the collected data, we construct PathCLIP, a pathology-dedicated CLIP, to enhance PathAsst's capabilities in interpreting pathology images. Finally, we integrate PathCLIP with the Vicuna-13b and utilize pathology-specific instruction-tuning data to enhance the multimodal generation capacity of PathAsst and bolster its synergistic interactions with sub-models. The experimental results of PathAsst show the potential of harnessing AI-powered generative foundation model to improve pathology diagnosis and treatment processes.\", 'authors': ['Yunlong Zhang', 'Zhongyi Shui', 'Lin Sun', 'Lin Yang', 'Honglin Li', 'Kai Zhang', 'Sunyi Zheng', 'Chenglu Zhu', 'Yuxuan Sun'], 'published': '2023-05-24', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'contextual-object-detection-with-multimodal', 'arxiv_id': '2305.18279', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2305.18279v2', 'url_pdf': 'https://arxiv.org/pdf/2305.18279v2.pdf', 'title': 'Contextual Object Detection with Multimodal Large Language Models', 'abstract': 'Recent Multimodal Large Language Models (MLLMs) are remarkable in vision-language tasks, such as image captioning and question answering, but lack the essential perception ability, i.e., object detection. In this work, we address this limitation by introducing a novel research problem of contextual object detection -- understanding visible objects within different human-AI interactive contexts. Three representative scenarios are investigated, including the language cloze test, visual captioning, and question answering. Moreover, we present ContextDET, a unified multimodal model that is capable of end-to-end differentiable modeling of visual-language contexts, so as to locate, identify, and associate visual objects with language inputs for human-AI interaction. Our ContextDET involves three key submodels: (i) a visual encoder for extracting visual representations, (ii) a pre-trained LLM for multimodal context decoding, and (iii) a visual decoder for predicting bounding boxes given contextual object words. The new generate-then-detect framework enables us to detect object words within human vocabulary. Extensive experiments show the advantages of ContextDET on our proposed CODE benchmark, open-vocabulary detection, and referring image segmentation. Github: https://github.com/yuhangzang/ContextDET.', 'authors': ['Chen Change Loy', 'Kaiyang Zhou', 'Jun Han', 'Wei Li', 'Yuhang Zang'], 'published': '2023-05-29', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'a-survey-on-multimodal-large-language-models', 'arxiv_id': '2306.13549', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2306.13549v4', 'url_pdf': 'https://arxiv.org/pdf/2306.13549v4.pdf', 'title': 'A Survey on Multimodal Large Language Models', 'abstract': 'Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.', 'authors': ['Enhong Chen', 'Tong Xu', 'Xing Sun', 'Ke Li', 'Sirui Zhao', 'Chaoyou Fu', 'Shukang Yin'], 'published': '2023-06-23', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'mme-a-comprehensive-evaluation-benchmark-for', 'arxiv_id': '2306.13394', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2306.13394v4', 'url_pdf': 'https://arxiv.org/pdf/2306.13394v4.pdf', 'title': 'MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models', 'abstract': 'Multimodal Large Language Model (MLLM) relies on the powerful LLM to perform multimodal tasks, showing amazing emergent abilities in recent studies, such as writing poems based on an image. However, it is difficult for these case studies to fully reflect the performance of MLLM, lacking a comprehensive evaluation. In this paper, we fill in this blank, presenting the first comprehensive MLLM Evaluation benchmark MME. It measures both perception and cognition abilities on a total of 14 subtasks. In order to avoid data leakage that may arise from direct use of public datasets for evaluation, the annotations of instruction-answer pairs are all manually designed. The concise instruction design allows us to fairly compare MLLMs, instead of struggling in prompt engineering. Besides, with such an instruction, we can also easily carry out quantitative statistics. A total of 30 advanced MLLMs are comprehensively evaluated on our MME, which not only suggests that existing MLLMs still have a large room for improvement, but also reveals the potential directions for the subsequent model optimization. The data application manner and online leaderboards are released at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation.', 'authors': ['Rongrong Ji', 'Yunsheng Wu', 'Jinrui Yang', 'Xing Sun', 'Ke Li', 'Xiawu Zheng', 'Xu Lin', 'Mengdan Zhang', 'Yulei Qin', 'Yunhang Shen', 'Peixian Chen', 'Chaoyou Fu'], 'published': '2023-06-23', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'kosmos-2-grounding-multimodal-large-language', 'arxiv_id': '2306.14824', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2306.14824v3', 'url_pdf': 'https://arxiv.org/pdf/2306.14824v3.pdf', 'title': 'Kosmos-2: Grounding Multimodal Large Language Models to the World', 'abstract': \"We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. Specifically, we represent refer expressions as links in Markdown, i.e., ``[text span](bounding boxes)'', where object descriptions are sequences of location tokens. Together with multimodal corpora, we construct large-scale data of grounded image-text pairs (called GrIT) to train the model. In addition to the existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), Kosmos-2 integrates the grounding capability into downstream applications. We evaluate Kosmos-2 on a wide range of tasks, including (i) multimodal grounding, such as referring expression comprehension, and phrase grounding, (ii) multimodal referring, such as referring expression generation, (iii) perception-language tasks, and (iv) language understanding and generation. This work lays out the foundation for the development of Embodiment AI and sheds light on the big convergence of language, multimodal perception, action, and world modeling, which is a key step toward artificial general intelligence. Code and pretrained models are available at https://aka.ms/kosmos-2.\", 'authors': ['Furu Wei', 'Shuming Ma', 'Shaohan Huang', 'Yaru Hao', 'Li Dong', 'Wenhui Wang', 'Zhiliang Peng'], 'published': '2023-06-26', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'shikra-unleashing-multimodal-llm-s', 'arxiv_id': '2306.15195', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2306.15195v2', 'url_pdf': 'https://arxiv.org/pdf/2306.15195v2.pdf', 'title': \"Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic\", 'abstract': \"In human conversations, individuals can indicate relevant regions within a scene while addressing others. In turn, the other person can then respond by referring to specific regions if necessary. This natural referential ability in dialogue remains absent in current Multimodal Large Language Models (MLLMs). To fill this gap, this paper proposes an MLLM called Shikra, which can handle spatial coordinate inputs and outputs in natural language. Its architecture consists of a vision encoder, an alignment layer, and a LLM. It is designed to be straightforward and simple, without the need for extra vocabularies, position encoder, pre-/post-detection modules, or external plug-in models. All inputs and outputs are in natural language form. Referential dialogue is a superset of various vision-language (VL) tasks. Shikra can naturally handle location-related tasks like REC and PointQA, as well as conventional VL tasks such as Image Captioning and VQA. Experimental results showcase Shikra's promising performance. Furthermore, it enables numerous exciting applications, like providing mentioned objects' coordinates in chains of thoughts and comparing user-pointed regions similarities. Our code, model and dataset are accessed at https://github.com/shikras/shikra.\", 'authors': ['Rui Zhao', 'Feng Zhu', 'Richong Zhang', 'Weili Zeng', 'Zhao Zhang', 'Keqin Chen'], 'published': '2023-06-27', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'svit-scaling-up-visual-instruction-tuning', 'arxiv_id': '2307.04087', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2307.04087v3', 'url_pdf': 'https://arxiv.org/pdf/2307.04087v3.pdf', 'title': 'SVIT: Scaling up Visual Instruction Tuning', 'abstract': \"Thanks to the emerging of foundation models, the large language and vision models are integrated to acquire the multimodal ability of visual captioning, question answering, etc. Although existing multimodal models present impressive performance of visual understanding and reasoning, their limits are still largely under-explored due to the scarcity of high-quality instruction tuning data. To push the limits of multimodal capability, we Scale up Visual Instruction Tuning (SVIT) by constructing a dataset of 4.2 million visual instruction tuning data including 1.6M conversation question-answer (QA) pairs, 1.6M complex reasoning QA pairs, 1.0M referring QA pairs and 106K detailed image descriptions. Besides the volume, the proposed dataset is also featured by the high quality and rich diversity, which is generated by prompting GPT-4 with the abundant manual annotations of images. We also propose a new data recipe to select subset with better diversity and balance, which evokes model's superior capabilities. Extensive experiments verify that SVIT-v1.5, trained on the proposed dataset, outperforms state-of-the-art Multimodal Large Language Models on popular benchmarks. The data and code are publicly available at https://github.com/BAAI-DCAI/Visual-Instruction-Tuning.\", 'authors': ['Tiejun Huang', 'Muyang He', 'Boya Wu', 'Bo Zhao'], 'published': '2023-07-09', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'chatspot-bootstrapping-multimodal-llms-via', 'arxiv_id': '2307.09474', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2307.09474v1', 'url_pdf': 'https://arxiv.org/pdf/2307.09474v1.pdf', 'title': 'ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning', 'abstract': \"Human-AI interactivity is a critical aspect that reflects the usability of multimodal large language models (MLLMs). However, existing end-to-end MLLMs only allow users to interact with them through language instructions, leading to the limitation of the interactive accuracy and efficiency. In this study, we present precise referring instructions that utilize diverse reference representations such as points and boxes as referring prompts to refer to the special region. This enables MLLMs to focus on the region of interest and achieve finer-grained interaction. Based on precise referring instruction, we propose ChatSpot, a unified end-to-end multimodal large language model that supports diverse forms of interactivity including mouse clicks, drag-and-drop, and drawing boxes, which provides a more flexible and seamless interactive experience. We also construct a multi-grained vision-language instruction-following dataset based on existing datasets and GPT-4 generating. Furthermore, we design a series of evaluation tasks to assess the effectiveness of region recognition and interaction. Experimental results showcase ChatSpot's promising performance.\", 'authors': ['Xiangyu Zhang', 'Chunrui Han', 'Runpei Dong', 'Yuang Peng', 'Jianjian Sun', 'HongYu Zhou', 'Haoran Wei', 'Jinrong Yang', 'Zheng Ge', 'En Yu', 'Liang Zhao'], 'published': '2023-07-18', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'seed-bench-benchmarking-multimodal-llms-with', 'arxiv_id': '2307.16125', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2307.16125v2', 'url_pdf': 'https://arxiv.org/pdf/2307.16125v2.pdf', 'title': 'SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension', 'abstract': 'Based on powerful Large Language Models (LLMs), recent generative Multimodal Large Language Models (MLLMs) have gained prominence as a pivotal research area, exhibiting remarkable capability for both comprehension and generation. In this work, we address the evaluation of generative comprehension in MLLMs as a preliminary step towards a comprehensive assessment of generative models, by introducing a benchmark named SEED-Bench. SEED-Bench consists of 19K multiple choice questions with accurate human annotations (x 6 larger than existing benchmarks), which spans 12 evaluation dimensions including the comprehension of both the image and video modality. We develop an advanced pipeline for generating multiple-choice questions that target specific evaluation dimensions, integrating both automatic filtering and manual verification processes. Multiple-choice questions with groundtruth options derived from human annotation enables an objective and efficient assessment of model performance, eliminating the need for human or GPT intervention during evaluation. We further evaluate the performance of 18 models across all 12 dimensions, covering both the spatial and temporal understanding. By revealing the limitations of existing MLLMs through evaluation results, we aim for SEED-Bench to provide insights for motivating future research. We will launch and consistently maintain a leaderboard to provide a platform for the community to assess and investigate model capability.', 'authors': ['Ying Shan', 'Yixiao Ge', 'Yuying Ge', 'Guangzhi Wang', 'Rui Wang', 'Bohao Li'], 'published': '2023-07-30', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'lisa-reasoning-segmentation-via-large', 'arxiv_id': '2308.00692', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.00692v3', 'url_pdf': 'https://arxiv.org/pdf/2308.00692v3.pdf', 'title': 'LISA: Reasoning Segmentation via Large Language Model', 'abstract': 'Although perception systems have made remarkable advancements in recent years, they still rely on explicit human instruction or pre-defined categories to identify the target objects before executing visual recognition tasks. Such systems cannot actively reason and comprehend implicit user intention. In this work, we propose a new segmentation task -- reasoning segmentation. The task is designed to output a segmentation mask given a complex and implicit query text. Furthermore, we establish a benchmark comprising over one thousand image-instruction-mask data samples, incorporating intricate reasoning and world knowledge for evaluation purposes. Finally, we present LISA: large Language Instructed Segmentation Assistant, which inherits the language generation capabilities of multimodal Large Language Models (LLMs) while also possessing the ability to produce segmentation masks. We expand the original vocabulary with a <SEG> token and propose the embedding-as-mask paradigm to unlock the segmentation capability. Remarkably, LISA can handle cases involving complex reasoning and world knowledge. Also, it demonstrates robust zero-shot capability when trained exclusively on reasoning-free datasets. In addition, fine-tuning the model with merely 239 reasoning segmentation data samples results in further performance enhancement. Both quantitative and qualitative experiments show our method effectively unlocks new reasoning segmentation capabilities for multimodal LLMs. Code, models, and data are available at https://github.com/dvlab-research/LISA.', 'authors': ['Jiaya Jia', 'Shu Liu', 'Yuhui Yuan', 'Yanwei Li', 'Yukang Chen', 'Zhuotao Tian', 'Xin Lai'], 'published': '2023-08-01', 'conference': None, 'conference_url_abs': 'http://openaccess.thecvf.com//content/CVPR2024/html/Lai_LISA_Reasoning_Segmentation_via_Large_Language_Model_CVPR_2024_paper.html', 'conference_url_pdf': 'http://openaccess.thecvf.com//content/CVPR2024/papers/Lai_LISA_Reasoning_Segmentation_via_Large_Language_Model_CVPR_2024_paper.pdf', 'proceeding': 'cvpr-2024-1'}, {'id': 'empowering-vision-language-models-to-follow', 'arxiv_id': '2308.04152', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.04152v4', 'url_pdf': 'https://arxiv.org/pdf/2308.04152v4.pdf', 'title': 'Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions', 'abstract': \"Recent advancements in Multimodal Large Language Models (MLLMs) have been utilizing Visual Prompt Generators (VPGs) to convert visual features into tokens that LLMs can recognize. This is achieved by training the VPGs on millions of image-caption pairs, where the VPG-generated tokens of images are fed into a frozen LLM to generate the corresponding captions. However, this image-captioning based training objective inherently biases the VPG to concentrate solely on the primary visual contents sufficient for caption generation, often neglecting other visual details. This shortcoming results in MLLMs' underperformance in comprehending demonstrative instructions consisting of multiple, interleaved, and multimodal instructions that demonstrate the required context to complete a task. To address this issue, we introduce a generic and lightweight Visual Prompt Generator Complete module (VPG-C), which can infer and complete the missing details essential for comprehending demonstrative instructions. Further, we propose a synthetic discriminative training strategy to fine-tune VPG-C, eliminating the need for supervised demonstrative instructions. As for evaluation, we build DEMON, a comprehensive benchmark for demonstrative instruction understanding. Synthetically trained with the proposed strategy, VPG-C achieves significantly stronger zero-shot performance across all tasks of DEMON. Further evaluation on the MME and OwlEval benchmarks also demonstrate the superiority of VPG-C. Our benchmark, code, and pre-trained models are available at https://github.com/DCDmllm/Cheetah.\", 'authors': ['Wenqiao Zhang', 'Wei Ji', 'Yueting Zhuang', 'Siliang Tang', 'Tat-Seng Chua', 'Hanwang Zhang', 'Minghe Gao', 'Zhiqi Ge', 'Kaihang Pan', 'Juncheng Li'], 'published': '2023-08-08', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'link-context-learning-for-multimodal-llms', 'arxiv_id': '2308.07891', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.07891v1', 'url_pdf': 'https://arxiv.org/pdf/2308.07891v1.pdf', 'title': 'Link-Context Learning for Multimodal LLMs', 'abstract': 'The ability to learn from context with novel concepts, and deliver appropriate responses are essential in human conversations. Despite current Multimodal Large Language Models (MLLMs) and Large Language Models (LLMs) being trained on mega-scale datasets, recognizing unseen images or understanding novel concepts in a training-free manner remains a challenge. In-Context Learning (ICL) explores training-free few-shot learning, where models are encouraged to ``learn to learn\" from limited tasks and generalize to unseen tasks. In this work, we propose link-context learning (LCL), which emphasizes \"reasoning from cause and effect\" to augment the learning capabilities of MLLMs. LCL goes beyond traditional ICL by explicitly strengthening the causal relationship between the support set and the query set. By providing demonstrations with causal links, LCL guides the model to discern not only the analogy but also the underlying causal associations between data points, which empowers MLLMs to recognize unseen images and understand novel concepts more effectively. To facilitate the evaluation of this novel approach, we introduce the ISEKAI dataset, comprising exclusively of unseen generated image-label pairs designed for link-context learning. Extensive experiments show that our LCL-MLLM exhibits strong link-context learning capabilities to novel concepts over vanilla MLLMs. Code and data will be released at https://github.com/isekai-portal/Link-Context-Learning.', 'authors': ['Ziwei Liu', 'Rui Zhao', 'Feng Zhu', 'Zhao Zhang', 'Weichen Fan', 'Yan Tai'], 'published': '2023-08-15', 'conference': None, 'conference_url_abs': 'http://openaccess.thecvf.com//content/CVPR2024/html/Tai_Link-Context_Learning_for_Multimodal_LLMs_CVPR_2024_paper.html', 'conference_url_pdf': 'http://openaccess.thecvf.com//content/CVPR2024/papers/Tai_Link-Context_Learning_for_Multimodal_LLMs_CVPR_2024_paper.pdf', 'proceeding': 'cvpr-2024-1'}, {'id': 'fashionlogo-prompting-multimodal-large', 'arxiv_id': '2308.09012', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.09012v2', 'url_pdf': 'https://arxiv.org/pdf/2308.09012v2.pdf', 'title': 'FashionLOGO: Prompting Multimodal Large Language Models for Fashion Logo Embeddings', 'abstract': \"Logo embedding models convert the product logos in images into vectors, enabling their utilization for logo recognition and detection within e-commerce platforms. This facilitates the enforcement of intellectual property rights and enhances product search capabilities. However, current methods treat logo embedding as a purely visual problem. A noteworthy issue is that visual models capture features more than logos. Instead, we view this as a multimodal task, using text as auxiliary information to facilitate the visual model's understanding of the logo. The emerging Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in both visual and textual understanding. Inspired by this, we propose an approach, \\\\textbf{FashionLOGO}, to explore how to prompt MLLMs to generate appropriate text for product images, which can help visual models achieve better logo embeddings. We adopt a cross-attention transformer block that enables visual embedding to automatically learn supplementary knowledge from textual embedding. Our extensive experiments on real-world datasets prove that FashionLOGO is capable of generating generic and robust logo embeddings, achieving state-of-the-art performance in all benchmarks.\", 'authors': ['Walton Wang', 'Minghui Qiu', 'Min Yang', 'Yulin Su', 'Da Li', 'Zhen Wang'], 'published': '2023-08-17', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'wanjuan-a-comprehensive-multimodal-dataset', 'arxiv_id': '2308.10755', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.10755v3', 'url_pdf': 'https://arxiv.org/pdf/2308.10755v3.pdf', 'title': 'WanJuan: A Comprehensive Multimodal Dataset for Advancing English and Chinese Large Models', 'abstract': 'The rise in popularity of ChatGPT and GPT-4 has significantly accelerated the development of large models, leading to the creation of numerous impressive large language models(LLMs) and multimodal large language models (MLLMs). These cutting-edge models owe their remarkable performance to high-quality data. However, the details of the training data used in leading paradigms are often kept confidential. This lack of transparency, coupled with the scarcity of open-source data, impedes further developments within the community. As a response, this paper presents \"Wan Juan\", a large-scale multimodal dataset composed of both Chinese and English data, collected from a wide range of web sources. The dataset incorporates text, image-text, and video modalities, with a total volume exceeding 2TB. It was utilized in the training of InternLM, a model that demonstrated significant advantages in multi-dimensional evaluations when compared to models of a similar scale. All data can be accessed at https://opendatalab.org.cn/WanJuan1.0.', 'authors': ['Dahua Lin', 'Jiaqi Wang', 'Hang Yan', 'Wei Li', 'Bin Wang', 'Jiantao Qiu', 'Chao Xu', 'Zhenjiang Jin', 'Conghui He'], 'published': '2023-08-21', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'stablellava-enhanced-visual-instruction', 'arxiv_id': '2308.10253', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.10253v2', 'url_pdf': 'https://arxiv.org/pdf/2308.10253v2.pdf', 'title': 'StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data', 'abstract': \"The remarkable multimodal capabilities demonstrated by OpenAI's GPT-4 have sparked significant interest in the development of multimodal Large Language Models (LLMs). A primary research objective of such models is to align visual and textual modalities effectively while comprehending human instructions. Current methodologies often rely on annotations derived from benchmark datasets to construct image-dialogue datasets for training purposes, akin to instruction tuning in LLMs. However, these datasets often exhibit domain bias, potentially constraining the generative capabilities of the models. In an effort to mitigate these limitations, we propose a novel data collection methodology that synchronously synthesizes images and dialogues for visual instruction tuning. This approach harnesses the power of generative models, marrying the abilities of ChatGPT and text-to-image generative models to yield a diverse and controllable dataset with varied image content. Additionally, datasets can be arbitrarily scaled. This not only provides greater flexibility compared to existing methodologies but also significantly enhances several model capabilities. Our research includes comprehensive experiments conducted on various datasets. The results emphasize substantial enhancements in more than ten commonly assessed capabilities. Additionally, our model achieves state-of-the-art results across multiple widely recognized multimodal benchmarks.\", 'authors': ['Yunchao Wei', 'Ling Chen', 'Chunhua Shen', 'Guosheng Lin', 'Bin Fu', 'Zhibin Wang', 'Gang Yu', 'Chi Zhang', 'Yanda Li'], 'published': '2023-08-20', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'instructiongpt-4-a-200-instruction-paradigm', 'arxiv_id': '2308.12067', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.12067v2', 'url_pdf': 'https://arxiv.org/pdf/2308.12067v2.pdf', 'title': 'InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4', 'abstract': 'Multimodal large language models are typically trained in two stages: first pre-training on image-text pairs, and then fine-tuning using supervised vision-language instruction data. Recent studies have shown that large language models can achieve satisfactory results even with a limited amount of high-quality instruction-following data. In this paper, we introduce InstructionGPT-4, which is fine-tuned on a small dataset comprising only 200 examples, amounting to approximately 6\\\\% of the instruction-following data used in the alignment dataset for MiniGPT-4. To achieve this, we first propose several metrics to access the quality of multimodal instruction data. Based on these metrics, we present an effective and trainable data selector to automatically identify and filter low-quality vision-language data. By employing this method, InstructionGPT-4 outperforms the original MiniGPT-4 on various evaluations. Overall, our findings demonstrate that less but high-quality instruction tuning data is efficient in enabling multimodal large language models to generate better output. Our code is available at https://github.com/waltonfuture/InstructionGPT-4.', 'authors': ['Lichao Sun', 'Weiran Huang', 'Zihao Jiang', 'Lai Wei'], 'published': '2023-08-23', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'vigc-visual-instruction-generation-and', 'arxiv_id': '2308.12714', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.12714v3', 'url_pdf': 'https://arxiv.org/pdf/2308.12714v3.pdf', 'title': 'VIGC: Visual Instruction Generation and Correction', 'abstract': \"The integration of visual encoders and large language models (LLMs) has driven recent progress in multimodal large language models (MLLMs). However, the scarcity of high-quality instruction-tuning data for vision-language tasks remains a challenge. The current leading paradigm, such as LLaVA, relies on language-only GPT-4 to generate data, which requires pre-annotated image captions and detection bounding boxes, suffering from understanding image details. A practical solution to this problem would be to utilize the available multimodal large language models (MLLMs) to generate instruction data for vision-language tasks. However, it's worth noting that the currently accessible MLLMs are not as powerful as their LLM counterparts, as they tend to produce inadequate responses and generate false information. As a solution for addressing the current issue, this paper proposes the Visual Instruction Generation and Correction (VIGC) framework that enables multimodal large language models to generate instruction-tuning data and progressively enhance its quality on-the-fly. Specifically, Visual Instruction Generation (VIG) guides the vision-language model to generate diverse instruction-tuning data. To ensure generation quality, Visual Instruction Correction (VIC) adopts an iterative update mechanism to correct any inaccuracies in data produced by VIG, effectively reducing the risk of hallucination. Leveraging the diverse, high-quality data generated by VIGC, we finetune mainstream models and validate data quality based on various evaluations. Experimental results demonstrate that VIGC not only compensates for the shortcomings of language-only data generation methods, but also effectively enhances the benchmark performance. The models, datasets, and code are available at https://opendatalab.github.io/VIGC.\", 'authors': ['Conghui He', 'Jiaqi Wang', 'Wei Li', 'Weijia Li', 'Xiaoyi Dong', 'Pan Zhang', 'Huaping Zhong', 'Jiahui Peng', 'Xiao Han', 'Fan Wu', 'Bin Wang'], 'published': '2023-08-24', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'position-enhanced-visual-instruction-tuning', 'arxiv_id': '2308.13437', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.13437v2', 'url_pdf': 'https://arxiv.org/pdf/2308.13437v2.pdf', 'title': 'Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models', 'abstract': 'Recently, Multimodal Large Language Models (MLLMs) that enable Large Language Models (LLMs) to interpret images through visual instruction tuning have achieved significant success. However, existing visual instruction tuning methods only utilize image-language instruction data to align the language and image modalities, lacking a more fine-grained cross-modal alignment. In this paper, we propose Position-enhanced Visual Instruction Tuning (PVIT), which extends the functionality of MLLMs by integrating an additional region-level vision encoder. This integration promotes a more detailed comprehension of images for the MLLM. In addition, to efficiently achieve a fine-grained alignment between the vision modules and the LLM, we design multiple data generation strategies to construct an image-region-language instruction dataset. Finally, we present both quantitative experiments and qualitative analysis that demonstrate the superiority of the proposed model. Code and data will be released at https://github.com/PVIT-official/PVIT.', 'authors': ['Yang Liu', 'Maosong Sun', 'Peng Li', 'Xiaoyue Mi', 'Fuwen Luo', 'Ruoyu Qin', 'Chi Chen'], 'published': '2023-08-25', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'mllm-dataengine-an-iterative-refinement', 'arxiv_id': '2308.13566', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.13566v2', 'url_pdf': 'https://arxiv.org/pdf/2308.13566v2.pdf', 'title': 'MLLM-DataEngine: An Iterative Refinement Approach for MLLM', 'abstract': 'Despite the great advance of Multimodal Large Language Models (MLLMs) in both instruction dataset building and benchmarking, the independence of training and evaluation makes current MLLMs hard to further improve their capability under the guidance of evaluation results with a relatively low human cost. In this paper, we propose MLLM-DataEngine, a novel closed-loop system that bridges data generation, model training, and evaluation. Within each loop iteration, the MLLM-DataEngine first analyze the weakness of the model based on the evaluation results, then generate a proper incremental dataset for the next training iteration and enhance the model capability iteratively. Compared with previous data collection methods which are separate from the benchmarking, the data generated by MLLM-DataEngine shows better targeting, quality, and correctness. For targeting, we propose an Adaptive Bad-case Sampling module, which adjusts the ratio of different types of data within each incremental dataset based on the benchmarking results. For quality, we resort to GPT-4 to generate high-quality data with each given data type. For correctness, prompt design is critical for the data generation results. Rather than previous hand-crafted prompt, we propose an Interactive Prompt Optimization strategy, which optimizes the prompt with the multi-round interaction between human and GPT, and improve the correctness of generated data greatly. Through extensive experiments, we find our MLLM-DataEngine could boost the MLLM capability in a targeted and automatic manner, with only a few human participation. We hope it could be a general solution for the following MLLMs building. The MLLM-DataEngine has been open-sourced and is now available at https://github.com/opendatalab/MLLM-DataEngine.', 'authors': ['Conghui He', 'Jiaqi Wang', 'Xiaoyi Dong', 'Pan Zhang', 'Siyuan Huang', 'Bin Wang', 'Linke Ouyang', 'Zhiyuan Zhao'], 'published': '2023-08-25', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'socratis-are-large-multimodal-models', 'arxiv_id': '2308.16741', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.16741v3', 'url_pdf': 'https://arxiv.org/pdf/2308.16741v3.pdf', 'title': 'Socratis: Are large multimodal models emotionally aware?', 'abstract': 'Existing emotion prediction benchmarks contain coarse emotion labels which do not consider the diversity of emotions that an image and text can elicit in humans due to various reasons. Learning diverse reactions to multimodal content is important as intelligent machines take a central role in generating and delivering content to society. To address this gap, we propose Socratis, a societal reactions benchmark, where each image-caption (IC) pair is annotated with multiple emotions and the reasons for feeling them. Socratis contains 18K free-form reactions for 980 emotions on 2075 image-caption pairs from 5 widely-read news and image-caption (IC) datasets. We benchmark the capability of state-of-the-art multimodal large language models to generate the reasons for feeling an emotion given an IC pair. Based on a preliminary human study, we observe that humans prefer human-written reasons over 2 times more often than machine-generated ones. This shows our task is harder than standard generation tasks because it starkly contrasts recent findings where humans cannot tell apart machine vs human-written news articles, for instance. We further see that current captioning metrics based on large vision-language models also fail to correlate with human preferences. We hope that these findings and our benchmark will inspire further research on training emotionally aware models.', 'authors': ['Kate Saenko', 'Bryan A. Plummer', 'Saadia Gabriel', 'Reuben Tan', 'Arijit Ray', 'Katherine Deng'], 'published': '2023-08-31', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'multiway-adapater-adapting-large-scale-multi', 'arxiv_id': '2309.01516', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.01516v3', 'url_pdf': 'https://arxiv.org/pdf/2309.01516v3.pdf', 'title': 'MultiWay-Adapater: Adapting large-scale multi-modal models for scalable image-text retrieval', 'abstract': \"As Multimodal Large Language Models (MLLMs) grow in size, adapting them to specialized tasks becomes increasingly challenging due to high computational and memory demands. Indeed, traditional fine-tuning methods are costly, due to the need for extensive, task-specific training. While efficient adaptation methods exist that aim to reduce these costs, in practice they suffer from shallow inter-modal alignment, which severely hurts model effectiveness. To tackle these computational challenges and improve inter-modal alignment, we introduce the MultiWay-Adapter (MWA), a novel framework featuring an 'Alignment Enhancer'. This enhancer deepens inter-modal alignment, enabling high transferability with minimal tuning effort. Our experiments show that unlike prior efficient tuning approaches, MWA maintains model effectiveness, while reducing training time by up-to 57%. MWA is also lightweight, increasing model size by only 2-3% (in terms of parameters) for state-of-the-art foundation models like BEiT-3 Large. These results demonstrate that MWA provides an efficient and effective adaptation method for MLLMs, significantly broadening their applicability.\", 'authors': ['Gerardo Aragon Camarasa', 'Richard McCreadie', 'George Killick', 'Zijun Long'], 'published': '2023-09-04', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'next-gpt-any-to-any-multimodal-llm', 'arxiv_id': '2309.05519', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.05519v3', 'url_pdf': 'https://arxiv.org/pdf/2309.05519v3.pdf', 'title': 'NExT-GPT: Any-to-Any Multimodal LLM', 'abstract': 'While recently Multimodal Large Language Models (MM-LLMs) have made exciting strides, they mostly fall prey to the limitation of only input-side multimodal understanding, without the ability to produce content in multiple modalities. As we humans always perceive the world and communicate with people through various modalities, developing any-to-any MM-LLMs capable of accepting and delivering content in any modality becomes essential to human-level AI. To fill the gap, we present an end-to-end general-purpose any-to-any MM-LLM system, NExT-GPT. We connect an LLM with multimodal adaptors and different diffusion decoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio. By leveraging the existing well-trained highly-performing encoders and decoders, NExT-GPT is tuned with only a small amount of parameter (1%) of certain projection layers, which not only benefits low-cost training and also facilitates convenient expansion to more potential modalities. Moreover, we introduce a modality-switching instruction tuning (MosIT) and manually curate a high-quality dataset for MosIT, based on which NExT-GPT is empowered with complex cross-modal semantic understanding and content generation. Overall, our research showcases the promising possibility of building an AI agent capable of modeling universal modalities, paving the way for more human-like AI research in the community. Project page: https://next-gpt.github.io/', 'authors': ['Tat-Seng Chua', 'Wei Ji', 'Leigang Qu', 'Hao Fei', 'Shengqiong Wu'], 'published': '2023-09-11', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'hilm-d-towards-high-resolution-understanding', 'arxiv_id': '2309.05186', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.05186v1', 'url_pdf': 'https://arxiv.org/pdf/2309.05186v1.pdf', 'title': 'HiLM-D: Towards High-Resolution Understanding in Multimodal Large Language Models for Autonomous Driving', 'abstract': \"Autonomous driving systems generally employ separate models for different tasks resulting in intricate designs. For the first time, we leverage singular multimodal large language models (MLLMs) to consolidate multiple autonomous driving tasks from videos, i.e., the Risk Object Localization and Intention and Suggestion Prediction (ROLISP) task. ROLISP uses natural language to simultaneously identify and interpret risk objects, understand ego-vehicle intentions, and provide motion suggestions, eliminating the necessity for task-specific architectures. However, lacking high-resolution (HR) information, existing MLLMs often miss small objects (e.g., traffic cones) and overly focus on salient ones (e.g., large trucks) when applied to ROLISP. We propose HiLM-D (Towards High-Resolution Understanding in MLLMs for Autonomous Driving), an efficient method to incorporate HR information into MLLMs for the ROLISP task. Especially, HiLM-D integrates two branches: (i) the low-resolution reasoning branch, can be any MLLMs, processes low-resolution videos to caption risk objects and discern ego-vehicle intentions/suggestions; (ii) the high-resolution perception branch (HR-PB), prominent to HiLM-D,, ingests HR images to enhance detection by capturing vision-specific HR feature maps and prioritizing all potential risks over merely salient objects. Our HR-PB serves as a plug-and-play module, seamlessly fitting into current MLLMs. Experiments on the ROLISP benchmark reveal HiLM-D's notable advantage over leading MLLMs, with improvements of 4.8% in BLEU-4 for captioning and 17.2% in mIoU for detection.\", 'authors': ['Xiaomeng Li', 'Wei zhang', 'Hang Xu', 'Jianhua Han', 'Xinpeng Ding'], 'published': '2023-09-11', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'investigating-the-catastrophic-forgetting-in', 'arxiv_id': '2309.10313', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.10313v4', 'url_pdf': 'https://arxiv.org/pdf/2309.10313v4.pdf', 'title': 'Investigating the Catastrophic Forgetting in Multimodal Large Language Models', 'abstract': 'Following the success of GPT4, there has been a surge in interest in multimodal large language model (MLLM) research. This line of research focuses on developing general-purpose LLMs through fine-tuning pre-trained LLMs and vision models. However, catastrophic forgetting, a notorious phenomenon where the fine-tuned model fails to retain similar performance compared to the pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM). In this paper, we introduce EMT: Evaluating MulTimodality for evaluating the catastrophic forgetting in MLLMs, by treating each MLLM as an image classifier. We first apply EMT to evaluate several open-source fine-tuned MLLMs and we discover that almost all evaluated MLLMs fail to retain the same performance levels as their vision encoders on standard image classification tasks. Moreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess performance throughout the fine-tuning. Interestingly, our results suggest that early-stage fine-tuning on an image dataset improves performance across other image datasets, by enhancing the alignment of text and visual features. However, as fine-tuning proceeds, the MLLMs begin to hallucinate, resulting in a significant loss of generalizability, even when the image encoder remains frozen. Our results suggest that MLLMs have yet to demonstrate performance on par with their vision models on standard image classification tasks and the current MLLM fine-tuning procedure still has room for improvement.', 'authors': ['Yi Ma', 'Yong Jae Lee', 'Qing Qu', 'Mu Cai', 'Xiao Li', 'Shengbang Tong', 'Yuexiang Zhai'], 'published': '2023-09-19', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'dreamllm-synergistic-multimodal-comprehension', 'arxiv_id': '2309.11499', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.11499v2', 'url_pdf': 'https://arxiv.org/pdf/2309.11499v2.pdf', 'title': 'DreamLLM: Synergistic Multimodal Comprehension and Creation', 'abstract': \"This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy. Project page: https://dreamllm.github.io.\", 'authors': ['Li Yi', 'Kaisheng Ma', 'Xiangyu Zhang', 'Xiangwen Kong', 'Haoran Wei', 'HongYu Zhou', 'Jianjian Sun', 'Liang Zhao', 'Jinrong Yang', 'Zheng Ge', 'Zekun Qi', 'Yuang Peng', 'Chunrui Han', 'Runpei Dong'], 'published': '2023-09-20', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'how-robust-is-google-s-bard-to-adversarial', 'arxiv_id': '2309.11751', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.11751v2', 'url_pdf': 'https://arxiv.org/pdf/2309.11751v2.pdf', 'title': \"How Robust is Google's Bard to Adversarial Image Attacks?\", 'abstract': \"Multimodal Large Language Models (MLLMs) that integrate text and other modalities (especially vision) have achieved unprecedented performance in various multimodal tasks. However, due to the unsolved adversarial robustness problem of vision models, MLLMs can have more severe safety and security risks by introducing the vision inputs. In this work, we study the adversarial robustness of Google's Bard, a competitive chatbot to ChatGPT that released its multimodal capability recently, to better understand the vulnerabilities of commercial MLLMs. By attacking white-box surrogate vision encoders or MLLMs, the generated adversarial examples can mislead Bard to output wrong image descriptions with a 22% success rate based solely on the transferability. We show that the adversarial examples can also attack other MLLMs, e.g., a 26% attack success rate against Bing Chat and a 86% attack success rate against ERNIE bot. Moreover, we identify two defense mechanisms of Bard, including face detection and toxicity detection of images. We design corresponding attacks to evade these defenses, demonstrating that the current defenses of Bard are also vulnerable. We hope this work can deepen our understanding on the robustness of MLLMs and facilitate future research on defenses. Our code is available at https://github.com/thu-ml/Attack-Bard. Update: GPT-4V is available at October 2023. We further evaluate its robustness under the same set of adversarial examples, achieving a 45% attack success rate.\", 'authors': ['Jun Zhu', 'Hang Su', 'Yu Tian', 'Yichi Zhang', 'Xiao Yang', 'Zhengwei Fang', 'Jiawei Chen', 'Huanran Chen', 'Yinpeng Dong'], 'published': '2023-09-21', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'vdc-versatile-data-cleanser-for-detecting', 'arxiv_id': '2309.16211', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.16211v2', 'url_pdf': 'https://arxiv.org/pdf/2309.16211v2.pdf', 'title': 'VDC: Versatile Data Cleanser based on Visual-Linguistic Inconsistency by Multimodal Large Language Models', 'abstract': 'The role of data in building AI systems has recently been emphasized by the emerging concept of data-centric AI. Unfortunately, in the real-world, datasets may contain dirty samples, such as poisoned samples from backdoor attack, noisy labels in crowdsourcing, and even hybrids of them. The presence of such dirty samples makes the DNNs vunerable and unreliable.Hence, it is critical to detect dirty samples to improve the quality and realiability of dataset. Existing detectors only focus on detecting poisoned samples or noisy labels, that are often prone to weak generalization when dealing with dirty samples from other domains.In this paper, we find a commonality of various dirty samples is visual-linguistic inconsistency between images and associated labels. To capture the semantic inconsistency between modalities, we propose versatile data cleanser (VDC) leveraging the surpassing capabilities of multimodal large language models (MLLM) in cross-modal alignment and reasoning.It consists of three consecutive modules: the visual question generation module to generate insightful questions about the image; the visual question answering module to acquire the semantics of the visual content by answering the questions with MLLM; followed by the visual answer evaluation module to evaluate the inconsistency.Extensive experiments demonstrate its superior performance and generalization to various categories and types of dirty samples. The code is available at \\\\url{https://github.com/zihao-ai/vdc}.', 'authors': ['Baoyuan Wu', 'Bingzhe Wu', 'Shaokui Wei', 'Mingda Zhang', 'Zihao Zhu'], 'published': '2023-09-28', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'guiding-instruction-based-image-editing-via', 'arxiv_id': '2309.17102', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.17102v2', 'url_pdf': 'https://arxiv.org/pdf/2309.17102v2.pdf', 'title': 'Guiding Instruction-based Image Editing via Multimodal Large Language Models', 'abstract': 'Instruction-based image editing improves the controllability and flexibility of image manipulation via natural commands without elaborate descriptions or regional masks. However, human instructions are sometimes too brief for current methods to capture and follow. Multimodal large language models (MLLMs) show promising capabilities in cross-modal understanding and visual-aware response generation via LMs. We investigate how MLLMs facilitate edit instructions and present MLLM-Guided Image Editing (MGIE). MGIE learns to derive expressive instructions and provides explicit guidance. The editing model jointly captures this visual imagination and performs manipulation through end-to-end training. We evaluate various aspects of Photoshop-style modification, global photo optimization, and local editing. Extensive experimental results demonstrate that expressive instructions are crucial to instruction-based image editing, and our MGIE can lead to a notable improvement in automatic metrics and human evaluation while maintaining competitive inference efficiency.', 'authors': ['Zhe Gan', 'Yinfei Yang', 'William Yang Wang', 'Xianzhi Du', 'Wenze Hu', 'Tsu-Jui Fu'], 'published': '2023-09-29', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'reformulating-vision-language-foundation', 'arxiv_id': '2310.00653', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.00653v1', 'url_pdf': 'https://arxiv.org/pdf/2310.00653v1.pdf', 'title': 'Reformulating Vision-Language Foundation Models and Datasets Towards Universal Multimodal Assistants', 'abstract': \"Recent Multimodal Large Language Models (MLLMs) exhibit impressive abilities to perceive images and follow open-ended instructions. The capabilities of MLLMs depend on two crucial factors: the model architecture to facilitate the feature alignment of visual modules and large language models; the multimodal instruction tuning datasets for human instruction following. (i) For the model architecture, most existing models introduce an external bridge module to connect vision encoders with language models, which needs an additional feature-alignment pre-training. In this work, we discover that compact pre-trained vision language models can inherently serve as ``out-of-the-box'' bridges between vision and language. Based on this, we propose Muffin framework, which directly employs pre-trained vision-language models to act as providers of visual signals. (ii) For the multimodal instruction tuning datasets, existing methods omit the complementary relationship between different datasets and simply mix datasets from different tasks. Instead, we propose UniMM-Chat dataset which explores the complementarities of datasets to generate 1.1M high-quality and diverse multimodal instructions. We merge information describing the same image from diverse datasets and transforms it into more knowledge-intensive conversation data. Experimental results demonstrate the effectiveness of the Muffin framework and UniMM-Chat dataset. Muffin achieves state-of-the-art performance on a wide range of vision-language tasks, significantly surpassing state-of-the-art models like LLaVA and InstructBLIP. Our model and dataset are all accessible at https://github.com/thunlp/muffin.\", 'authors': ['Maosong Sun', 'Hai-Tao Zheng', 'Zhiyuan Liu', 'Dahai Li', 'Jiao Xue', 'Yinxv Pan', 'Shan Wang', 'Chongyi Wang', 'Yue Zhao', 'Haoye Zhang', 'Yuan YAO', 'Jinyi Hu', 'Tianyu Yu'], 'published': '2023-10-01', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'drivegpt4-interpretable-end-to-end-autonomous', 'arxiv_id': '2310.01412', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.01412v5', 'url_pdf': 'https://arxiv.org/pdf/2310.01412v5.pdf', 'title': 'DriveGPT4: Interpretable End-to-end Autonomous Driving via Large Language Model', 'abstract': 'Multimodal large language models (MLLMs) have emerged as a prominent area of interest within the research community, given their proficiency in handling and reasoning with non-textual data, including images and videos. This study seeks to extend the application of MLLMs to the realm of autonomous driving by introducing DriveGPT4, a novel interpretable end-to-end autonomous driving system based on LLMs. Capable of processing multi-frame video inputs and textual queries, DriveGPT4 facilitates the interpretation of vehicle actions, offers pertinent reasoning, and effectively addresses a diverse range of questions posed by users. Furthermore, DriveGPT4 predicts low-level vehicle control signals in an end-to-end fashion.These advanced capabilities are achieved through the utilization of a bespoke visual instruction tuning dataset, specifically tailored for autonomous driving applications, in conjunction with a mix-finetuning training strategy. DriveGPT4 represents the pioneering effort to leverage LLMs for the development of an interpretable end-to-end autonomous driving solution. Evaluations conducted on the BDD-X dataset showcase the superior qualitative and quantitative performance of DriveGPT4. Additionally, the fine-tuning of domain-specific data enables DriveGPT4 to yield close or even improved results in terms of autonomous driving grounding when contrasted with GPT4-V.', 'authors': ['Kwan-Yee. K. Wong', 'Hengshuang Zhao', 'Zhenguo Li', 'Yong Guo', 'Zhen Zhao', 'Enze Xie', 'Yujia Zhang', 'Zhenhua Xu'], 'published': '2023-10-02', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'minigpt-5-interleaved-vision-and-language', 'arxiv_id': '2310.02239', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.02239v3', 'url_pdf': 'https://arxiv.org/pdf/2310.02239v3.pdf', 'title': 'MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens', 'abstract': 'The effectiveness of Multimodal Large Language Models (MLLMs) demonstrates a profound capability in multimodal understanding. However, the simultaneous generation of images with coherent texts is still underdeveloped. Addressing this, we introduce a novel interleaved vision-and-language generation method, centered around the concept of ``generative vokens\". These vokens serve as pivotal elements contributing to coherent image-text outputs. Our method is marked by a unique two-stage training strategy for description-free multimodal generation, which does not necessitate extensive descriptions of images. We integrate classifier-free guidance to enhance the alignment of generated images and texts, ensuring more seamless and contextually relevant multimodal interactions. Our model, MiniGPT-5, exhibits substantial improvement over the baseline models on multimodal generation datasets, including MMDialog and VIST. The human evaluation shows MiniGPT-5 is better than the baseline model on more than 56\\\\% cases for multimodal generation, highlighting its efficacy across diverse benchmarks.', 'authors': ['Xin Eric Wang', 'Xuehai He', 'Kaizhi Zheng'], 'published': '2023-10-03', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'towards-end-to-end-embodied-decision-making', 'arxiv_id': '2310.02071', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.02071v4', 'url_pdf': 'https://arxiv.org/pdf/2310.02071v4.pdf', 'title': 'Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond', 'abstract': 'In this study, we explore the potential of Multimodal Large Language Models (MLLMs) in improving embodied decision-making processes for agents. While Large Language Models (LLMs) have been widely used due to their advanced reasoning skills and vast world knowledge, MLLMs like GPT4-Vision offer enhanced visual understanding and reasoning capabilities. We investigate whether state-of-the-art MLLMs can handle embodied decision-making in an end-to-end manner and whether collaborations between LLMs and MLLMs can enhance decision-making. To address these questions, we introduce a new benchmark called PCA-EVAL, which evaluates embodied decision-making from the perspectives of Perception, Cognition, and Action. Additionally, we propose HOLMES, a multi-agent cooperation framework that allows LLMs to leverage MLLMs and APIs to gather multimodal information for informed decision-making. We compare end-to-end embodied decision-making and HOLMES on our benchmark and find that the GPT4-Vision model demonstrates strong end-to-end embodied decision-making abilities, outperforming GPT4-HOLMES in terms of average decision accuracy (+3%). However, this performance is exclusive to the latest GPT4-Vision model, surpassing the open-source state-of-the-art MLLM by 26%. Our results indicate that powerful MLLMs like GPT4-Vision hold promise for decision-making in embodied agents, offering new avenues for MLLM research. Code and data are open at https://github.com/pkunlp-icler/PCA-EVAL/.', 'authors': ['Peiyi Wang', 'Baobao Chang', 'Tianyu Liu', 'Yuchi Wang', 'Zefan Cai', 'Haozhe Zhao', 'Shuhuai Ren', 'Yichi Zhang', 'Liang Chen'], 'published': '2023-10-03', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'kosmos-g-generating-images-in-context-with', 'arxiv_id': '2310.02992', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.02992v3', 'url_pdf': 'https://arxiv.org/pdf/2310.02992v3.pdf', 'title': 'Kosmos-G: Generating Images in Context with Multimodal Large Language Models', 'abstract': 'Recent advancements in subject-driven image generation have made significant strides. However, current methods still fall short in diverse application scenarios, as they require test-time tuning and cannot accept interleaved multi-image and text input. These limitations keep them far from the ultimate goal of \"image as a foreign language in image generation.\" This paper presents Kosmos-G, a model that leverages the advanced multimodal perception capabilities of Multimodal Large Language Models (MLLMs) to tackle the aforementioned challenge. Our approach aligns the output space of MLLM with CLIP using the textual modality as an anchor and performs compositional instruction tuning on curated data. Kosmos-G demonstrates an impressive capability of zero-shot subject-driven generation with interleaved multi-image and text input. Notably, the score distillation instruction tuning requires no modifications to the image decoder. This allows for a seamless substitution of CLIP and effortless integration with a myriad of U-Net techniques ranging from fine-grained controls to personalized image decoder variants. We posit Kosmos-G as an initial attempt towards the goal of \"image as a foreign language in image generation.\" The code can be found at https://aka.ms/Kosmos-G', 'authors': ['Furu Wei', 'Wenhu Chen', 'Zhiliang Peng', 'Shaohan Huang', 'Li Dong', 'Xichen Pan'], 'published': '2023-10-04', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'fine-grained-audio-visual-joint', 'arxiv_id': '2310.05863', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.05863v2', 'url_pdf': 'https://arxiv.org/pdf/2310.05863v2.pdf', 'title': 'Fine-grained Audio-Visual Joint Representations for Multimodal Large Language Models', 'abstract': 'Audio-visual large language models (LLM) have drawn significant attention, yet the fine-grained combination of both input streams is rather under-explored, which is challenging but necessary for LLMs to understand general video inputs. To this end, a fine-grained audio-visual joint representation (FAVOR) learning framework for multimodal LLMs is proposed in this paper, which extends a text-based LLM to simultaneously perceive speech and audio events in the audio input stream and images or videos in the visual input stream, at the frame level. To fuse the audio and visual feature streams into joint representations and to align the joint space with the LLM input embedding space, we propose a causal Q-Former structure with a causal attention module to enhance the capture of causal relations of the audio-visual frames across time. An audio-visual evaluation benchmark (AVEB) is also proposed which comprises six representative single-modal tasks with five cross-modal tasks reflecting audio-visual co-reasoning abilities. While achieving competitive single-modal performance on audio, speech and image tasks in AVEB, FAVOR achieved over 20% accuracy improvements on the video question-answering task when fine-grained information or temporal causal reasoning is required. FAVOR, in addition, demonstrated remarkable video comprehension and reasoning abilities on tasks that are unprecedented by other multimodal LLMs. An interactive demo of FAVOR is available at https://github.com/BriansIDP/AudioVisualLLM.git, and the training code and model checkpoints will be released soon.', 'authors': ['Chao Zhang', 'Zejun Ma', 'Lu Lu', 'Wei Li', 'Tian Tan', 'Xianzhao Chen', 'Changli Tang', 'Wenyi Yu', 'Guangzhi Sun'], 'published': '2023-10-09', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'can-we-edit-multimodal-large-language-models', 'arxiv_id': '2310.08475', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.08475v5', 'url_pdf': 'https://arxiv.org/pdf/2310.08475v5.pdf', 'title': 'Can We Edit Multimodal Large Language Models?', 'abstract': 'In this paper, we focus on editing Multimodal Large Language Models (MLLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights. Code and dataset are available in https://github.com/zjunlp/EasyEdit.', 'authors': ['Ningyu Zhang', 'Huajun Chen', 'Yongheng Wang', 'Xi Chen', 'Qingbin Liu', 'Bozhong Tian', 'Siyuan Cheng'], 'published': '2023-10-12', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'autorepo-a-general-framework-for-multi-modal', 'arxiv_id': '2310.07944', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.07944v2', 'url_pdf': 'https://arxiv.org/pdf/2310.07944v2.pdf', 'title': 'AutoRepo: A general framework for multi-modal LLM-based automated construction reporting', 'abstract': 'Ensuring the safety, quality, and timely completion of construction projects is paramount, with construction inspections serving as a vital instrument towards these goals. Nevertheless, the predominantly manual approach of present-day inspections frequently results in inefficiencies and inadequate information management. Such methods often fall short of providing holistic, exhaustive assessments, consequently engendering regulatory oversights and potential safety hazards. To address this issue, this paper presents a novel framework named AutoRepo for automated generation of construction inspection reports. The unmanned vehicles efficiently perform construction inspections and collect scene information, while the multimodal large language models (LLMs) are leveraged to automatically generate the inspection reports. The framework was applied and tested on a real-world construction site, demonstrating its potential to expedite the inspection process, significantly reduce resource allocation, and produce high-quality, regulatory standard-compliant inspection reports. This research thus underscores the immense potential of multimodal large language models in revolutionizing construction inspection practices, signaling a significant leap forward towards a more efficient and safer construction management paradigm.', 'authors': ['Heng Li', 'Runhao Guo', 'Jing Li', 'Xincong Yang', 'Hongxu Pu'], 'published': '2023-10-11', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'mm-bigbench-evaluating-multimodal-models-on', 'arxiv_id': '2310.09036', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.09036v1', 'url_pdf': 'https://arxiv.org/pdf/2310.09036v1.pdf', 'title': 'MM-BigBench: Evaluating Multimodal Models on Multimodal Content Comprehension Tasks', 'abstract': \"The popularity of multimodal large language models (MLLMs) has triggered a recent surge in research efforts dedicated to evaluating these models. Nevertheless, existing evaluation studies of MLLMs primarily focus on the comprehension and reasoning of unimodal (vision) content, neglecting performance evaluations in the domain of multimodal (vision-language) content understanding. Beyond multimodal reasoning, tasks related to multimodal content comprehension necessitate a profound understanding of multimodal contexts, achieved through the multimodal interaction to obtain a final answer. In this paper, we introduce a comprehensive assessment framework called MM-BigBench, which incorporates a diverse range of metrics to offer an extensive evaluation of the performance of various models and instructions across a wide spectrum of diverse multimodal content comprehension tasks. Consequently, our work complements research on the performance of MLLMs in multimodal comprehension tasks, achieving a more comprehensive and holistic evaluation of MLLMs. To begin, we employ the Best Performance metric to ascertain each model's performance upper bound on different datasets. Subsequently, the Mean Relative Gain metric offers an assessment of the overall performance of various models and instructions, while the Stability metric measures their sensitivity. Furthermore, previous research centers on evaluating models independently or solely assessing instructions, neglecting the adaptability between models and instructions. We propose the Adaptability metric to quantify the adaptability between models and instructions. Our paper evaluates a total of 20 language models (14 MLLMs) on 14 multimodal datasets spanning 6 tasks, with 10 instructions for each task, and derives novel insights. Our code will be released at https://github.com/declare-lab/MM-BigBench.\", 'authors': ['Soujanya Poria', 'XiaoMing Fu', 'Yifei Zhang', 'Qi Sun', 'Yang Li', 'Daling Wang', 'Ming Wang', 'Shi Feng', 'Wenfang Wu', 'Xiaocui Yang'], 'published': '2023-10-13', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'robollm-robotic-vision-tasks-grounded-on', 'arxiv_id': '2310.10221', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.10221v2', 'url_pdf': 'https://arxiv.org/pdf/2310.10221v2.pdf', 'title': 'RoboLLM: Robotic Vision Tasks Grounded on Multimodal Large Language Models', 'abstract': 'Robotic vision applications often necessitate a wide range of visual perception tasks, such as object detection, segmentation, and identification. While there have been substantial advances in these individual tasks, integrating specialized models into a unified vision pipeline presents significant engineering challenges and costs. Recently, Multimodal Large Language Models (MLLMs) have emerged as novel backbones for various downstream tasks. We argue that leveraging the pre-training capabilities of MLLMs enables the creation of a simplified framework, thus mitigating the need for task-specific encoders. Specifically, the large-scale pretrained knowledge in MLLMs allows for easier fine-tuning to downstream robotic vision tasks and yields superior performance. We introduce the RoboLLM framework, equipped with a BEiT-3 backbone, to address all visual perception tasks in the ARMBench challenge-a large-scale robotic manipulation dataset about real-world warehouse scenarios. RoboLLM not only outperforms existing baselines but also substantially reduces the engineering burden associated with model selection and tuning. The source code is publicly available at https://github.com/longkukuhi/armbench.', 'authors': ['Gerardo Aragon Camarasa', 'Richard McCreadie', 'George Killick', 'Zijun Long'], 'published': '2023-10-16', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'benchmarking-sequential-visual-input', 'arxiv_id': '2310.13473', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.13473v1', 'url_pdf': 'https://arxiv.org/pdf/2310.13473v1.pdf', 'title': 'Benchmarking Sequential Visual Input Reasoning and Prediction in Multimodal Large Language Models', 'abstract': \"Multimodal large language models (MLLMs) have shown great potential in perception and interpretation tasks, but their capabilities in predictive reasoning remain under-explored. To address this gap, we introduce a novel benchmark that assesses the predictive reasoning capabilities of MLLMs across diverse scenarios. Our benchmark targets three important domains: abstract pattern reasoning, human activity prediction, and physical interaction prediction. We further develop three evaluation methods powered by large language model to robustly quantify a model's performance in predicting and reasoning the future based on multi-visual context. Empirical experiments confirm the soundness of the proposed benchmark and evaluation methods via rigorous testing and reveal pros and cons of current popular MLLMs in the task of predictive reasoning. Lastly, our proposed benchmark provides a standardized evaluation framework for MLLMs and can facilitate the development of more advanced models that can reason and predict over complex long sequence of multimodal input.\", 'authors': ['Jianwei Yin', 'Tiancheng Zhao', 'Kangjia Zhao', 'Yu Shu', 'Leigang Sha', 'Mingwei Zhu'], 'published': '2023-10-20', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'woodpecker-hallucination-correction-for', 'arxiv_id': '2310.16045', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.16045v2', 'url_pdf': 'https://arxiv.org/pdf/2310.16045v2.pdf', 'title': 'Woodpecker: Hallucination Correction for Multimodal Large Language Models', 'abstract': 'Hallucination is a big shadow hanging over the rapidly evolving Multimodal Large Language Models (MLLMs), referring to the phenomenon that the generated text is inconsistent with the image content. In order to mitigate hallucinations, existing studies mainly resort to an instruction-tuning manner that requires retraining the models with specific data. In this paper, we pave a different way, introducing a training-free method named Woodpecker. Like a woodpecker heals trees, it picks out and corrects hallucinations from the generated text. Concretely, Woodpecker consists of five stages: key concept extraction, question formulation, visual knowledge validation, visual claim generation, and hallucination correction. Implemented in a post-remedy manner, Woodpecker can easily serve different MLLMs, while being interpretable by accessing intermediate outputs of the five stages. We evaluate Woodpecker both quantitatively and qualitatively and show the huge potential of this new paradigm. On the POPE benchmark, our method obtains a 30.66%/24.33% improvement in accuracy over the baseline MiniGPT-4/mPLUG-Owl. The source code is released at https://github.com/BradyFU/Woodpecker.', 'authors': ['Enhong Chen', 'Xing Sun', 'Ke Li', 'Yunhang Shen', 'Dianbo Sui', 'Hao Wang', 'Tong Xu', 'Sirui Zhao', 'Chaoyou Fu', 'Shukang Yin'], 'published': '2023-10-24', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'visual-cropping-improves-zero-shot-question', 'arxiv_id': '2310.16033', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.16033v3', 'url_pdf': 'https://arxiv.org/pdf/2310.16033v3.pdf', 'title': 'Towards Perceiving Small Visual Details in Zero-shot Visual Question Answering with Multimodal LLMs', 'abstract': \"Multimodal Large Language Models (MLLMs) have recently achieved promising zero-shot accuracy on visual question answering (VQA) -- a fundamental task affecting various downstream applications and domains. Given the great potential for the broad use of these models, it is important to investigate their limitations in dealing with different image and question properties. In this work, we investigate whether MLLMs can perceive small details as well as large details in images. In particular, we show that their zero-shot accuracy in answering visual questions is very sensitive to the size of the visual subject of the question, declining up to 46% with size. Furthermore, we show that this effect is causal by observing that human visual cropping can significantly mitigate their sensitivity to size. Inspired by the usefulness of human cropping, we then propose five automatic visual cropping methods -- leveraging either external localization models or the decision process of the given MLLM itself -- as inference time mechanisms to improve the zero-shot performance of MLLMs. We study their effectiveness on four popular VQA datasets, and a subset of the VQAv2 dataset tailored towards fine visual details. Our findings suggest that MLLMs should be used with caution in detail-sensitive VQA applications, and that visual cropping is a promising direction to improve their zero-shot performance. To facilitate further investigation of MLLMs' behaviors, our code and data are publicly released.\", 'authors': ['Filip Ilievski', 'Prateek Chhikara', 'Mahyar Khayatkhoei', 'Jiarui Zhang'], 'published': '2023-10-24', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'chef-a-comprehensive-evaluation-framework-for', 'arxiv_id': '2311.02692', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2311.02692v1', 'url_pdf': 'https://arxiv.org/pdf/2311.02692v1.pdf', 'title': 'ChEF: A Comprehensive Evaluation Framework for Standardized Assessment of Multimodal Large Language Models', 'abstract': \"Multimodal Large Language Models (MLLMs) have shown impressive abilities in interacting with visual content with myriad potential downstream tasks. However, even though a list of benchmarks has been proposed, the capabilities and limitations of MLLMs are still not comprehensively understood, due to a lack of a standardized and holistic evaluation framework. To this end, we present the first Comprehensive Evaluation Framework (ChEF) that can holistically profile each MLLM and fairly compare different MLLMs. First, we structure ChEF as four modular components, i.e., Scenario as scalable multimodal datasets, Instruction as flexible instruction retrieving formulae, Inferencer as reliable question answering strategies, and Metric as indicative task-specific score functions. Based on them, ChEF facilitates versatile evaluations in a standardized framework, and new evaluations can be built by designing new Recipes (systematic selection of these four components). Notably, current MLLM benchmarks can be readily summarized as recipes of ChEF. Second, we introduce 6 new recipes to quantify competent MLLMs' desired capabilities (or called desiderata, i.e., calibration, in-context learning, instruction following, language performance, hallucination, and robustness) as reliable agents that can perform real-world multimodal interactions. Third, we conduct a large-scale evaluation of 9 prominent MLLMs on 9 scenarios and 6 desiderata. Our evaluation summarized over 20 valuable observations concerning the generalizability of MLLMs across various scenarios and the composite capability of MLLMs required for multimodal interactions. We will publicly release all the detailed implementations for further analysis, as well as an easy-to-use modular toolkit for the integration of new recipes and models, so that ChEF can be a growing evaluation framework for the MLLM community.\", 'authors': ['Jing Shao', 'Yu Qiao', 'Lu Sheng', 'Zhenfei Yin', 'Hongxing Fan', 'Zhipin Wang', 'Zhelun Shi'], 'published': '2023-11-05', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'octavius-mitigating-task-interference-in', 'arxiv_id': '2311.02684', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2311.02684v3', 'url_pdf': 'https://arxiv.org/pdf/2311.02684v3.pdf', 'title': 'Octavius: Mitigating Task Interference in MLLMs via LoRA-MoE', 'abstract': 'Recent studies have demonstrated Large Language Models (LLMs) can extend their zero-shot generalization capabilities to multimodal learning through instruction tuning. As more modalities and downstream tasks are introduced, negative conflicts and interference may have a worse impact on performance. While this phenomenon has been overlooked in previous work, we propose a novel and extensible framework, called Octavius, for comprehensive studies and experimentation on multimodal learning with Multimodal Large Language Models (MLLMs). Specifically, we combine the well-known Mixture-of-Experts (MoE) and one of the representative PEFT techniques, i.e., LoRA, designing a novel LLM-based decoder, called LoRA-MoE, for multimodal learning. To the best of our knowledge, we are one of the pioneering efforts to introduce MoE into MLLMs to address this problem. The experimental results (about 20% improvement) have shown the effectiveness and versatility of our design in various 2D and 3D downstream tasks. Code and datasets are available at https://openlamm.github.io/tutorial/.', 'authors': ['Jing Shao', 'Yu Qiao', 'Wanli Ouyang', 'Lu Sheng', 'Si Liu', 'Zhenfei Yin', 'Huayang Liu', 'Zhen Wang', 'Ziqin Wang', 'Zeren Chen'], 'published': '2023-11-05', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'what-large-language-models-bring-to-text-rich', 'arxiv_id': '2311.07306', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2311.07306v1', 'url_pdf': 'https://arxiv.org/pdf/2311.07306v1.pdf', 'title': 'What Large Language Models Bring to Text-rich VQA?', 'abstract': \"Text-rich VQA, namely Visual Question Answering based on text recognition in the images, is a cross-modal task that requires both image comprehension and text recognition. In this work, we focus on investigating the advantages and bottlenecks of LLM-based approaches in addressing this problem. To address the above concern, we separate the vision and language modules, where we leverage external OCR models to recognize texts in the image and Large Language Models (LLMs) to answer the question given texts. The whole framework is training-free benefiting from the in-context ability of LLMs. This pipeline achieved superior performance compared to the majority of existing Multimodal Large Language Models (MLLM) on four text-rich VQA datasets. Besides, based on the ablation study, we find that LLM brings stronger comprehension ability and may introduce helpful knowledge for the VQA problem. The bottleneck for LLM to address text-rich VQA problems may primarily lie in visual part. We also combine the OCR module with MLLMs and pleasantly find that the combination of OCR module with MLLM also works. It's worth noting that not all MLLMs can comprehend the OCR information, which provides insights into how to train an MLLM that preserves the abilities of LLM.\", 'authors': ['Fei Tan', 'Zechao Li', 'Rui Zhao', 'Jinghui Lu', 'Xinzhe Ni', 'Wei Tang', 'Xuejing Liu'], 'published': '2023-11-13', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}], 'docs': [Document(metadata={'title': 'Multimodal large language models for inclusive collaboration learning tasks'}, page_content='This PhD project leverages advancements in multimodal large language models to build an inclusive collaboration feedback loop, in order to facilitate the automated detection, modeling, and feedback for participants developing general collaboration skills. This topic is important given the role of collaboration as an essential 21st century skill, the potential to ground large language models within learning theory and real-world practice, and the expressive potential of transformer models to support equity and inclusion. We address some concerns of integrating advances in natural language processing into downstream tasks such as the learning analytics feedback loop.'), Document(metadata={'title': 'Personality-aware Human-centric Multimodal Reasoning: A New Task, Dataset and Baselines'}, page_content=\"Personality traits, emotions, and beliefs shape individuals' behavioral choices and decision-making processes. However, for one thing, the affective computing community normally focused on predicting personality traits but overlooks their application in behavior prediction. For another, the multimodal reasoning task emphasized the prediction of future states and behaviors but often neglected the incorporation of individual personality traits. In this work, we introduce a new task called Personality-aware Human-centric Multimodal Reasoning (PHMR) (T1), with the goal of forecasting the future behavior of a particular individual using multimodal information from past instances, while integrating personality factors. We accordingly construct a new dataset based on six television shows, encompassing 225 characters and 12k samples. To establish a benchmark for the task, we propose seven baseline methods: three adapted from related tasks, two pre-trained model, and two multimodal large language models. The experimental results demonstrate that incorporating personality traits enhances human-centric multimodal reasoning performance. To further solve the lack of personality annotation in real-life scenes, we introduce an extension task called Personality-predicted Human-centric Multimodal Reasoning task (T2) along with the corresponding dataset and method. We will make our dataset and code available on GitHub.\"), Document(metadata={'title': 'Visual Transformation Telling'}, page_content='Humans can naturally reason from superficial state differences (e.g. ground wetness) to transformations descriptions (e.g. raining) according to their life experience. In this paper, we propose a new visual reasoning task to test this transformation reasoning ability in real-world scenarios, called \\\\textbf{V}isual \\\\textbf{T}ransformation \\\\textbf{T}elling (VTT). Given a series of states (i.e. images), VTT requires to describe the transformation occurring between every two adjacent states. Different from existing visual reasoning tasks that focus on surface state reasoning, the advantage of VTT is that it captures the underlying causes, e.g. actions or events, behind the differences among states. We collect a novel dataset to support the study of transformation reasoning from two existing instructional video datasets, CrossTask and COIN, comprising 13,547 samples. Each sample involves the key state images along with their transformation descriptions. Our dataset covers diverse real-world activities, providing a rich resource for training and evaluation. To construct an initial benchmark for VTT, we test several models, including traditional visual storytelling methods (CST, GLACNet, Densecap) and advanced multimodal large language models (LLaVA v1.5-7B, Qwen-VL-chat, Gemini Pro Vision, GPT-4o, and GPT-4). Experimental results reveal that even state-of-the-art models still face challenges in VTT, highlighting substantial areas for improvement.'), Document(metadata={'title': 'Listen, Think, and Understand'}, page_content='The ability of artificial intelligence (AI) systems to perceive and comprehend audio signals is crucial for many applications. Although significant progress has been made in this area since the development of AudioSet, most existing models are designed to map audio inputs to pre-defined, discrete sound label sets. In contrast, humans possess the ability to not only classify sounds into general categories, but also to listen to the finer details of the sounds, explain the reason for the predictions, think about what the sound infers, and understand the scene and what action needs to be taken, if any. Such capabilities beyond perception are not yet present in existing audio models. On the other hand, modern large language models (LLMs) exhibit emerging reasoning ability but they lack audio perception capabilities. Therefore, we ask the question: can we build a model that has both audio perception and a reasoning ability? In this paper, we propose a new audio foundation model, called LTU (Listen, Think, and Understand). To train LTU, we created a new OpenAQA-5M dataset consisting of 1.9 million closed-ended and 3.7 million open-ended, diverse (audio, question, answer) tuples, and have used an autoregressive training framework with a perception-to-understanding curriculum. LTU demonstrates strong performance and generalization ability on conventional audio tasks such as classification and captioning. More importantly, it exhibits emerging audio reasoning and comprehension abilities that are absent in existing audio models. To the best of our knowledge, LTU is one of the first multimodal large language models that focus on general audio (rather than just speech) understanding.'), Document(metadata={'title': 'What Makes for Good Visual Tokenizers for Large Language Models?'}, page_content='We empirically investigate proper pre-training methods to build good visual tokenizers, making Large Language Models (LLMs) powerful Multimodal Large Language Models (MLLMs). In our benchmark, which is curated to evaluate MLLMs visual semantic understanding and fine-grained perception capabilities, we discussed different visual tokenizers pre-trained with dominant methods (i.e., DeiT, CLIP, MAE, DINO), and observe that: i) Fully/weakly supervised models capture more semantics than self-supervised models, but the gap is narrowed by scaling up the pre-training dataset. ii) Self-supervised models are better at fine-grained perception, where patch-level supervision is particularly effective. iii) Tuning the visual tokenizer leads to the loss of semantics obtained from large-scale pretraining, which is unfavorable with relatively small-scale instruction-tuning dataset. Given the findings, we reviewed methods that attempted to unify semantics and fine-grained visual understanding, e.g., patch-level feature distillation with semantically-rich targets. We obtain an intriguing insight mask-based strategies that were once all the rage may not be applicable for obtaining good visual tokenizers. Based on this critical observation, we obtain a new MLLM equipped with a tailored Good Visual Tokenizer (GVT), which exhibits strong visual comprehension capability at multiple scales. In particular, without introducing extra parameters and task-specific fine-tuning, GVT achieves superior performance on visual question answering, image captioning, and other fine-grained visual understanding tasks such as object counting and multi-class identification.'), Document(metadata={'title': 'Dynamic Clue Bottlenecks: Towards Interpretable-by-Design Visual Question Answering'}, page_content='Recent advances in multimodal large language models (LLMs) have shown extreme effectiveness in visual question answering (VQA). However, the design nature of these end-to-end models prevents them from being interpretable to humans, undermining trust and applicability in critical domains. While post-hoc rationales offer certain insight into understanding model behavior, these explanations are not guaranteed to be faithful to the model. In this paper, we address these shortcomings by introducing an interpretable by design model that factors model decisions into intermediate human-legible explanations, and allows people to easily understand why a model fails or succeeds. We propose the Dynamic Clue Bottleneck Model ( (DCLUB), a method that is designed towards an inherently interpretable VQA system. DCLUB provides an explainable intermediate space before the VQA decision and is faithful from the beginning, while maintaining comparable performance to black-box systems. Given a question, DCLUB first returns a set of visual clues: natural language statements of visually salient evidence from the image, and then generates the output based solely on the visual clues. To supervise and evaluate the generation of VQA explanations within DCLUB, we collect a dataset of 1.7k reasoning-focused questions with visual clues. Evaluations show that our inherently interpretable system can improve 4.64% over a comparable black-box system in reasoning-focused questions while preserving 99.43% of performance on VQA-v2.'), Document(metadata={'title': 'PathAsst: A Generative Foundation AI Assistant Towards Artificial General Intelligence of Pathology'}, page_content=\"As advances in large language models (LLMs) and multimodal techniques continue to mature, the development of general-purpose multimodal large language models (MLLMs) has surged, offering significant applications in interpreting natural images. However, the field of pathology has largely remained untapped, particularly in gathering high-quality data and designing comprehensive model frameworks. To bridge the gap in pathology MLLMs, we present PathAsst, a multimodal generative foundation AI assistant to revolutionize diagnostic and predictive analytics in pathology. The development of PathAsst involves three pivotal steps: data acquisition, CLIP model adaptation, and the training of PathAsst's multimodal generative capabilities. Firstly, we collect over 207K high-quality pathology image-text pairs from authoritative sources. Leveraging the advanced power of ChatGPT, we generate over 180K instruction-following samples. Furthermore, we devise additional instruction-following data specifically tailored for invoking eight pathology-specific sub-models we prepared, allowing the PathAsst to effectively collaborate with these models, enhancing its diagnostic ability. Secondly, by leveraging the collected data, we construct PathCLIP, a pathology-dedicated CLIP, to enhance PathAsst's capabilities in interpreting pathology images. Finally, we integrate PathCLIP with the Vicuna-13b and utilize pathology-specific instruction-tuning data to enhance the multimodal generation capacity of PathAsst and bolster its synergistic interactions with sub-models. The experimental results of PathAsst show the potential of harnessing AI-powered generative foundation model to improve pathology diagnosis and treatment processes.\"), Document(metadata={'title': 'Contextual Object Detection with Multimodal Large Language Models'}, page_content='Recent Multimodal Large Language Models (MLLMs) are remarkable in vision-language tasks, such as image captioning and question answering, but lack the essential perception ability, i.e., object detection. In this work, we address this limitation by introducing a novel research problem of contextual object detection -- understanding visible objects within different human-AI interactive contexts. Three representative scenarios are investigated, including the language cloze test, visual captioning, and question answering. Moreover, we present ContextDET, a unified multimodal model that is capable of end-to-end differentiable modeling of visual-language contexts, so as to locate, identify, and associate visual objects with language inputs for human-AI interaction. Our ContextDET involves three key submodels: (i) a visual encoder for extracting visual representations, (ii) a pre-trained LLM for multimodal context decoding, and (iii) a visual decoder for predicting bounding boxes given contextual object words. The new generate-then-detect framework enables us to detect object words within human vocabulary. Extensive experiments show the advantages of ContextDET on our proposed CODE benchmark, open-vocabulary detection, and referring image segmentation. Github: https://github.com/yuhangzang/ContextDET.'), Document(metadata={'title': 'A Survey on Multimodal Large Language Models'}, page_content='Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.'), Document(metadata={'title': 'MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models'}, page_content='Multimodal Large Language Model (MLLM) relies on the powerful LLM to perform multimodal tasks, showing amazing emergent abilities in recent studies, such as writing poems based on an image. However, it is difficult for these case studies to fully reflect the performance of MLLM, lacking a comprehensive evaluation. In this paper, we fill in this blank, presenting the first comprehensive MLLM Evaluation benchmark MME. It measures both perception and cognition abilities on a total of 14 subtasks. In order to avoid data leakage that may arise from direct use of public datasets for evaluation, the annotations of instruction-answer pairs are all manually designed. The concise instruction design allows us to fairly compare MLLMs, instead of struggling in prompt engineering. Besides, with such an instruction, we can also easily carry out quantitative statistics. A total of 30 advanced MLLMs are comprehensively evaluated on our MME, which not only suggests that existing MLLMs still have a large room for improvement, but also reveals the potential directions for the subsequent model optimization. The data application manner and online leaderboards are released at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation.'), Document(metadata={'title': 'Kosmos-2: Grounding Multimodal Large Language Models to the World'}, page_content=\"We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. Specifically, we represent refer expressions as links in Markdown, i.e., ``[text span](bounding boxes)'', where object descriptions are sequences of location tokens. Together with multimodal corpora, we construct large-scale data of grounded image-text pairs (called GrIT) to train the model. In addition to the existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), Kosmos-2 integrates the grounding capability into downstream applications. We evaluate Kosmos-2 on a wide range of tasks, including (i) multimodal grounding, such as referring expression comprehension, and phrase grounding, (ii) multimodal referring, such as referring expression generation, (iii) perception-language tasks, and (iv) language understanding and generation. This work lays out the foundation for the development of Embodiment AI and sheds light on the big convergence of language, multimodal perception, action, and world modeling, which is a key step toward artificial general intelligence. Code and pretrained models are available at https://aka.ms/kosmos-2.\"), Document(metadata={'title': \"Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic\"}, page_content=\"In human conversations, individuals can indicate relevant regions within a scene while addressing others. In turn, the other person can then respond by referring to specific regions if necessary. This natural referential ability in dialogue remains absent in current Multimodal Large Language Models (MLLMs). To fill this gap, this paper proposes an MLLM called Shikra, which can handle spatial coordinate inputs and outputs in natural language. Its architecture consists of a vision encoder, an alignment layer, and a LLM. It is designed to be straightforward and simple, without the need for extra vocabularies, position encoder, pre-/post-detection modules, or external plug-in models. All inputs and outputs are in natural language form. Referential dialogue is a superset of various vision-language (VL) tasks. Shikra can naturally handle location-related tasks like REC and PointQA, as well as conventional VL tasks such as Image Captioning and VQA. Experimental results showcase Shikra's promising performance. Furthermore, it enables numerous exciting applications, like providing mentioned objects' coordinates in chains of thoughts and comparing user-pointed regions similarities. Our code, model and dataset are accessed at https://github.com/shikras/shikra.\"), Document(metadata={'title': 'SVIT: Scaling up Visual Instruction Tuning'}, page_content=\"Thanks to the emerging of foundation models, the large language and vision models are integrated to acquire the multimodal ability of visual captioning, question answering, etc. Although existing multimodal models present impressive performance of visual understanding and reasoning, their limits are still largely under-explored due to the scarcity of high-quality instruction tuning data. To push the limits of multimodal capability, we Scale up Visual Instruction Tuning (SVIT) by constructing a dataset of 4.2 million visual instruction tuning data including 1.6M conversation question-answer (QA) pairs, 1.6M complex reasoning QA pairs, 1.0M referring QA pairs and 106K detailed image descriptions. Besides the volume, the proposed dataset is also featured by the high quality and rich diversity, which is generated by prompting GPT-4 with the abundant manual annotations of images. We also propose a new data recipe to select subset with better diversity and balance, which evokes model's superior capabilities. Extensive experiments verify that SVIT-v1.5, trained on the proposed dataset, outperforms state-of-the-art Multimodal Large Language Models on popular benchmarks. The data and code are publicly available at https://github.com/BAAI-DCAI/Visual-Instruction-Tuning.\"), Document(metadata={'title': 'ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning'}, page_content=\"Human-AI interactivity is a critical aspect that reflects the usability of multimodal large language models (MLLMs). However, existing end-to-end MLLMs only allow users to interact with them through language instructions, leading to the limitation of the interactive accuracy and efficiency. In this study, we present precise referring instructions that utilize diverse reference representations such as points and boxes as referring prompts to refer to the special region. This enables MLLMs to focus on the region of interest and achieve finer-grained interaction. Based on precise referring instruction, we propose ChatSpot, a unified end-to-end multimodal large language model that supports diverse forms of interactivity including mouse clicks, drag-and-drop, and drawing boxes, which provides a more flexible and seamless interactive experience. We also construct a multi-grained vision-language instruction-following dataset based on existing datasets and GPT-4 generating. Furthermore, we design a series of evaluation tasks to assess the effectiveness of region recognition and interaction. Experimental results showcase ChatSpot's promising performance.\"), Document(metadata={'title': 'SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension'}, page_content='Based on powerful Large Language Models (LLMs), recent generative Multimodal Large Language Models (MLLMs) have gained prominence as a pivotal research area, exhibiting remarkable capability for both comprehension and generation. In this work, we address the evaluation of generative comprehension in MLLMs as a preliminary step towards a comprehensive assessment of generative models, by introducing a benchmark named SEED-Bench. SEED-Bench consists of 19K multiple choice questions with accurate human annotations (x 6 larger than existing benchmarks), which spans 12 evaluation dimensions including the comprehension of both the image and video modality. We develop an advanced pipeline for generating multiple-choice questions that target specific evaluation dimensions, integrating both automatic filtering and manual verification processes. Multiple-choice questions with groundtruth options derived from human annotation enables an objective and efficient assessment of model performance, eliminating the need for human or GPT intervention during evaluation. We further evaluate the performance of 18 models across all 12 dimensions, covering both the spatial and temporal understanding. By revealing the limitations of existing MLLMs through evaluation results, we aim for SEED-Bench to provide insights for motivating future research. We will launch and consistently maintain a leaderboard to provide a platform for the community to assess and investigate model capability.'), Document(metadata={'title': 'LISA: Reasoning Segmentation via Large Language Model'}, page_content='Although perception systems have made remarkable advancements in recent years, they still rely on explicit human instruction or pre-defined categories to identify the target objects before executing visual recognition tasks. Such systems cannot actively reason and comprehend implicit user intention. In this work, we propose a new segmentation task -- reasoning segmentation. The task is designed to output a segmentation mask given a complex and implicit query text. Furthermore, we establish a benchmark comprising over one thousand image-instruction-mask data samples, incorporating intricate reasoning and world knowledge for evaluation purposes. Finally, we present LISA: large Language Instructed Segmentation Assistant, which inherits the language generation capabilities of multimodal Large Language Models (LLMs) while also possessing the ability to produce segmentation masks. We expand the original vocabulary with a <SEG> token and propose the embedding-as-mask paradigm to unlock the segmentation capability. Remarkably, LISA can handle cases involving complex reasoning and world knowledge. Also, it demonstrates robust zero-shot capability when trained exclusively on reasoning-free datasets. In addition, fine-tuning the model with merely 239 reasoning segmentation data samples results in further performance enhancement. Both quantitative and qualitative experiments show our method effectively unlocks new reasoning segmentation capabilities for multimodal LLMs. Code, models, and data are available at https://github.com/dvlab-research/LISA.'), Document(metadata={'title': 'Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions'}, page_content=\"Recent advancements in Multimodal Large Language Models (MLLMs) have been utilizing Visual Prompt Generators (VPGs) to convert visual features into tokens that LLMs can recognize. This is achieved by training the VPGs on millions of image-caption pairs, where the VPG-generated tokens of images are fed into a frozen LLM to generate the corresponding captions. However, this image-captioning based training objective inherently biases the VPG to concentrate solely on the primary visual contents sufficient for caption generation, often neglecting other visual details. This shortcoming results in MLLMs' underperformance in comprehending demonstrative instructions consisting of multiple, interleaved, and multimodal instructions that demonstrate the required context to complete a task. To address this issue, we introduce a generic and lightweight Visual Prompt Generator Complete module (VPG-C), which can infer and complete the missing details essential for comprehending demonstrative instructions. Further, we propose a synthetic discriminative training strategy to fine-tune VPG-C, eliminating the need for supervised demonstrative instructions. As for evaluation, we build DEMON, a comprehensive benchmark for demonstrative instruction understanding. Synthetically trained with the proposed strategy, VPG-C achieves significantly stronger zero-shot performance across all tasks of DEMON. Further evaluation on the MME and OwlEval benchmarks also demonstrate the superiority of VPG-C. Our benchmark, code, and pre-trained models are available at https://github.com/DCDmllm/Cheetah.\"), Document(metadata={'title': 'Link-Context Learning for Multimodal LLMs'}, page_content='The ability to learn from context with novel concepts, and deliver appropriate responses are essential in human conversations. Despite current Multimodal Large Language Models (MLLMs) and Large Language Models (LLMs) being trained on mega-scale datasets, recognizing unseen images or understanding novel concepts in a training-free manner remains a challenge. In-Context Learning (ICL) explores training-free few-shot learning, where models are encouraged to ``learn to learn\" from limited tasks and generalize to unseen tasks. In this work, we propose link-context learning (LCL), which emphasizes \"reasoning from cause and effect\" to augment the learning capabilities of MLLMs. LCL goes beyond traditional ICL by explicitly strengthening the causal relationship between the support set and the query set. By providing demonstrations with causal links, LCL guides the model to discern not only the analogy but also the underlying causal associations between data points, which empowers MLLMs to recognize unseen images and understand novel concepts more effectively. To facilitate the evaluation of this novel approach, we introduce the ISEKAI dataset, comprising exclusively of unseen generated image-label pairs designed for link-context learning. Extensive experiments show that our LCL-MLLM exhibits strong link-context learning capabilities to novel concepts over vanilla MLLMs. Code and data will be released at https://github.com/isekai-portal/Link-Context-Learning.'), Document(metadata={'title': 'FashionLOGO: Prompting Multimodal Large Language Models for Fashion Logo Embeddings'}, page_content=\"Logo embedding models convert the product logos in images into vectors, enabling their utilization for logo recognition and detection within e-commerce platforms. This facilitates the enforcement of intellectual property rights and enhances product search capabilities. However, current methods treat logo embedding as a purely visual problem. A noteworthy issue is that visual models capture features more than logos. Instead, we view this as a multimodal task, using text as auxiliary information to facilitate the visual model's understanding of the logo. The emerging Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in both visual and textual understanding. Inspired by this, we propose an approach, \\\\textbf{FashionLOGO}, to explore how to prompt MLLMs to generate appropriate text for product images, which can help visual models achieve better logo embeddings. We adopt a cross-attention transformer block that enables visual embedding to automatically learn supplementary knowledge from textual embedding. Our extensive experiments on real-world datasets prove that FashionLOGO is capable of generating generic and robust logo embeddings, achieving state-of-the-art performance in all benchmarks.\"), Document(metadata={'title': 'WanJuan: A Comprehensive Multimodal Dataset for Advancing English and Chinese Large Models'}, page_content='The rise in popularity of ChatGPT and GPT-4 has significantly accelerated the development of large models, leading to the creation of numerous impressive large language models(LLMs) and multimodal large language models (MLLMs). These cutting-edge models owe their remarkable performance to high-quality data. However, the details of the training data used in leading paradigms are often kept confidential. This lack of transparency, coupled with the scarcity of open-source data, impedes further developments within the community. As a response, this paper presents \"Wan Juan\", a large-scale multimodal dataset composed of both Chinese and English data, collected from a wide range of web sources. The dataset incorporates text, image-text, and video modalities, with a total volume exceeding 2TB. It was utilized in the training of InternLM, a model that demonstrated significant advantages in multi-dimensional evaluations when compared to models of a similar scale. All data can be accessed at https://opendatalab.org.cn/WanJuan1.0.'), Document(metadata={'title': 'StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data'}, page_content=\"The remarkable multimodal capabilities demonstrated by OpenAI's GPT-4 have sparked significant interest in the development of multimodal Large Language Models (LLMs). A primary research objective of such models is to align visual and textual modalities effectively while comprehending human instructions. Current methodologies often rely on annotations derived from benchmark datasets to construct image-dialogue datasets for training purposes, akin to instruction tuning in LLMs. However, these datasets often exhibit domain bias, potentially constraining the generative capabilities of the models. In an effort to mitigate these limitations, we propose a novel data collection methodology that synchronously synthesizes images and dialogues for visual instruction tuning. This approach harnesses the power of generative models, marrying the abilities of ChatGPT and text-to-image generative models to yield a diverse and controllable dataset with varied image content. Additionally, datasets can be arbitrarily scaled. This not only provides greater flexibility compared to existing methodologies but also significantly enhances several model capabilities. Our research includes comprehensive experiments conducted on various datasets. The results emphasize substantial enhancements in more than ten commonly assessed capabilities. Additionally, our model achieves state-of-the-art results across multiple widely recognized multimodal benchmarks.\"), Document(metadata={'title': 'InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4'}, page_content='Multimodal large language models are typically trained in two stages: first pre-training on image-text pairs, and then fine-tuning using supervised vision-language instruction data. Recent studies have shown that large language models can achieve satisfactory results even with a limited amount of high-quality instruction-following data. In this paper, we introduce InstructionGPT-4, which is fine-tuned on a small dataset comprising only 200 examples, amounting to approximately 6\\\\% of the instruction-following data used in the alignment dataset for MiniGPT-4. To achieve this, we first propose several metrics to access the quality of multimodal instruction data. Based on these metrics, we present an effective and trainable data selector to automatically identify and filter low-quality vision-language data. By employing this method, InstructionGPT-4 outperforms the original MiniGPT-4 on various evaluations. Overall, our findings demonstrate that less but high-quality instruction tuning data is efficient in enabling multimodal large language models to generate better output. Our code is available at https://github.com/waltonfuture/InstructionGPT-4.'), Document(metadata={'title': 'VIGC: Visual Instruction Generation and Correction'}, page_content=\"The integration of visual encoders and large language models (LLMs) has driven recent progress in multimodal large language models (MLLMs). However, the scarcity of high-quality instruction-tuning data for vision-language tasks remains a challenge. The current leading paradigm, such as LLaVA, relies on language-only GPT-4 to generate data, which requires pre-annotated image captions and detection bounding boxes, suffering from understanding image details. A practical solution to this problem would be to utilize the available multimodal large language models (MLLMs) to generate instruction data for vision-language tasks. However, it's worth noting that the currently accessible MLLMs are not as powerful as their LLM counterparts, as they tend to produce inadequate responses and generate false information. As a solution for addressing the current issue, this paper proposes the Visual Instruction Generation and Correction (VIGC) framework that enables multimodal large language models to generate instruction-tuning data and progressively enhance its quality on-the-fly. Specifically, Visual Instruction Generation (VIG) guides the vision-language model to generate diverse instruction-tuning data. To ensure generation quality, Visual Instruction Correction (VIC) adopts an iterative update mechanism to correct any inaccuracies in data produced by VIG, effectively reducing the risk of hallucination. Leveraging the diverse, high-quality data generated by VIGC, we finetune mainstream models and validate data quality based on various evaluations. Experimental results demonstrate that VIGC not only compensates for the shortcomings of language-only data generation methods, but also effectively enhances the benchmark performance. The models, datasets, and code are available at https://opendatalab.github.io/VIGC.\"), Document(metadata={'title': 'Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models'}, page_content='Recently, Multimodal Large Language Models (MLLMs) that enable Large Language Models (LLMs) to interpret images through visual instruction tuning have achieved significant success. However, existing visual instruction tuning methods only utilize image-language instruction data to align the language and image modalities, lacking a more fine-grained cross-modal alignment. In this paper, we propose Position-enhanced Visual Instruction Tuning (PVIT), which extends the functionality of MLLMs by integrating an additional region-level vision encoder. This integration promotes a more detailed comprehension of images for the MLLM. In addition, to efficiently achieve a fine-grained alignment between the vision modules and the LLM, we design multiple data generation strategies to construct an image-region-language instruction dataset. Finally, we present both quantitative experiments and qualitative analysis that demonstrate the superiority of the proposed model. Code and data will be released at https://github.com/PVIT-official/PVIT.'), Document(metadata={'title': 'MLLM-DataEngine: An Iterative Refinement Approach for MLLM'}, page_content='Despite the great advance of Multimodal Large Language Models (MLLMs) in both instruction dataset building and benchmarking, the independence of training and evaluation makes current MLLMs hard to further improve their capability under the guidance of evaluation results with a relatively low human cost. In this paper, we propose MLLM-DataEngine, a novel closed-loop system that bridges data generation, model training, and evaluation. Within each loop iteration, the MLLM-DataEngine first analyze the weakness of the model based on the evaluation results, then generate a proper incremental dataset for the next training iteration and enhance the model capability iteratively. Compared with previous data collection methods which are separate from the benchmarking, the data generated by MLLM-DataEngine shows better targeting, quality, and correctness. For targeting, we propose an Adaptive Bad-case Sampling module, which adjusts the ratio of different types of data within each incremental dataset based on the benchmarking results. For quality, we resort to GPT-4 to generate high-quality data with each given data type. For correctness, prompt design is critical for the data generation results. Rather than previous hand-crafted prompt, we propose an Interactive Prompt Optimization strategy, which optimizes the prompt with the multi-round interaction between human and GPT, and improve the correctness of generated data greatly. Through extensive experiments, we find our MLLM-DataEngine could boost the MLLM capability in a targeted and automatic manner, with only a few human participation. We hope it could be a general solution for the following MLLMs building. The MLLM-DataEngine has been open-sourced and is now available at https://github.com/opendatalab/MLLM-DataEngine.'), Document(metadata={'title': 'Socratis: Are large multimodal models emotionally aware?'}, page_content='Existing emotion prediction benchmarks contain coarse emotion labels which do not consider the diversity of emotions that an image and text can elicit in humans due to various reasons. Learning diverse reactions to multimodal content is important as intelligent machines take a central role in generating and delivering content to society. To address this gap, we propose Socratis, a societal reactions benchmark, where each image-caption (IC) pair is annotated with multiple emotions and the reasons for feeling them. Socratis contains 18K free-form reactions for 980 emotions on 2075 image-caption pairs from 5 widely-read news and image-caption (IC) datasets. We benchmark the capability of state-of-the-art multimodal large language models to generate the reasons for feeling an emotion given an IC pair. Based on a preliminary human study, we observe that humans prefer human-written reasons over 2 times more often than machine-generated ones. This shows our task is harder than standard generation tasks because it starkly contrasts recent findings where humans cannot tell apart machine vs human-written news articles, for instance. We further see that current captioning metrics based on large vision-language models also fail to correlate with human preferences. We hope that these findings and our benchmark will inspire further research on training emotionally aware models.'), Document(metadata={'title': 'MultiWay-Adapater: Adapting large-scale multi-modal models for scalable image-text retrieval'}, page_content=\"As Multimodal Large Language Models (MLLMs) grow in size, adapting them to specialized tasks becomes increasingly challenging due to high computational and memory demands. Indeed, traditional fine-tuning methods are costly, due to the need for extensive, task-specific training. While efficient adaptation methods exist that aim to reduce these costs, in practice they suffer from shallow inter-modal alignment, which severely hurts model effectiveness. To tackle these computational challenges and improve inter-modal alignment, we introduce the MultiWay-Adapter (MWA), a novel framework featuring an 'Alignment Enhancer'. This enhancer deepens inter-modal alignment, enabling high transferability with minimal tuning effort. Our experiments show that unlike prior efficient tuning approaches, MWA maintains model effectiveness, while reducing training time by up-to 57%. MWA is also lightweight, increasing model size by only 2-3% (in terms of parameters) for state-of-the-art foundation models like BEiT-3 Large. These results demonstrate that MWA provides an efficient and effective adaptation method for MLLMs, significantly broadening their applicability.\"), Document(metadata={'title': 'NExT-GPT: Any-to-Any Multimodal LLM'}, page_content='While recently Multimodal Large Language Models (MM-LLMs) have made exciting strides, they mostly fall prey to the limitation of only input-side multimodal understanding, without the ability to produce content in multiple modalities. As we humans always perceive the world and communicate with people through various modalities, developing any-to-any MM-LLMs capable of accepting and delivering content in any modality becomes essential to human-level AI. To fill the gap, we present an end-to-end general-purpose any-to-any MM-LLM system, NExT-GPT. We connect an LLM with multimodal adaptors and different diffusion decoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio. By leveraging the existing well-trained highly-performing encoders and decoders, NExT-GPT is tuned with only a small amount of parameter (1%) of certain projection layers, which not only benefits low-cost training and also facilitates convenient expansion to more potential modalities. Moreover, we introduce a modality-switching instruction tuning (MosIT) and manually curate a high-quality dataset for MosIT, based on which NExT-GPT is empowered with complex cross-modal semantic understanding and content generation. Overall, our research showcases the promising possibility of building an AI agent capable of modeling universal modalities, paving the way for more human-like AI research in the community. Project page: https://next-gpt.github.io/'), Document(metadata={'title': 'HiLM-D: Towards High-Resolution Understanding in Multimodal Large Language Models for Autonomous Driving'}, page_content=\"Autonomous driving systems generally employ separate models for different tasks resulting in intricate designs. For the first time, we leverage singular multimodal large language models (MLLMs) to consolidate multiple autonomous driving tasks from videos, i.e., the Risk Object Localization and Intention and Suggestion Prediction (ROLISP) task. ROLISP uses natural language to simultaneously identify and interpret risk objects, understand ego-vehicle intentions, and provide motion suggestions, eliminating the necessity for task-specific architectures. However, lacking high-resolution (HR) information, existing MLLMs often miss small objects (e.g., traffic cones) and overly focus on salient ones (e.g., large trucks) when applied to ROLISP. We propose HiLM-D (Towards High-Resolution Understanding in MLLMs for Autonomous Driving), an efficient method to incorporate HR information into MLLMs for the ROLISP task. Especially, HiLM-D integrates two branches: (i) the low-resolution reasoning branch, can be any MLLMs, processes low-resolution videos to caption risk objects and discern ego-vehicle intentions/suggestions; (ii) the high-resolution perception branch (HR-PB), prominent to HiLM-D,, ingests HR images to enhance detection by capturing vision-specific HR feature maps and prioritizing all potential risks over merely salient objects. Our HR-PB serves as a plug-and-play module, seamlessly fitting into current MLLMs. Experiments on the ROLISP benchmark reveal HiLM-D's notable advantage over leading MLLMs, with improvements of 4.8% in BLEU-4 for captioning and 17.2% in mIoU for detection.\"), Document(metadata={'title': 'Investigating the Catastrophic Forgetting in Multimodal Large Language Models'}, page_content='Following the success of GPT4, there has been a surge in interest in multimodal large language model (MLLM) research. This line of research focuses on developing general-purpose LLMs through fine-tuning pre-trained LLMs and vision models. However, catastrophic forgetting, a notorious phenomenon where the fine-tuned model fails to retain similar performance compared to the pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM). In this paper, we introduce EMT: Evaluating MulTimodality for evaluating the catastrophic forgetting in MLLMs, by treating each MLLM as an image classifier. We first apply EMT to evaluate several open-source fine-tuned MLLMs and we discover that almost all evaluated MLLMs fail to retain the same performance levels as their vision encoders on standard image classification tasks. Moreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess performance throughout the fine-tuning. Interestingly, our results suggest that early-stage fine-tuning on an image dataset improves performance across other image datasets, by enhancing the alignment of text and visual features. However, as fine-tuning proceeds, the MLLMs begin to hallucinate, resulting in a significant loss of generalizability, even when the image encoder remains frozen. Our results suggest that MLLMs have yet to demonstrate performance on par with their vision models on standard image classification tasks and the current MLLM fine-tuning procedure still has room for improvement.'), Document(metadata={'title': 'DreamLLM: Synergistic Multimodal Comprehension and Creation'}, page_content=\"This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy. Project page: https://dreamllm.github.io.\"), Document(metadata={'title': \"How Robust is Google's Bard to Adversarial Image Attacks?\"}, page_content=\"Multimodal Large Language Models (MLLMs) that integrate text and other modalities (especially vision) have achieved unprecedented performance in various multimodal tasks. However, due to the unsolved adversarial robustness problem of vision models, MLLMs can have more severe safety and security risks by introducing the vision inputs. In this work, we study the adversarial robustness of Google's Bard, a competitive chatbot to ChatGPT that released its multimodal capability recently, to better understand the vulnerabilities of commercial MLLMs. By attacking white-box surrogate vision encoders or MLLMs, the generated adversarial examples can mislead Bard to output wrong image descriptions with a 22% success rate based solely on the transferability. We show that the adversarial examples can also attack other MLLMs, e.g., a 26% attack success rate against Bing Chat and a 86% attack success rate against ERNIE bot. Moreover, we identify two defense mechanisms of Bard, including face detection and toxicity detection of images. We design corresponding attacks to evade these defenses, demonstrating that the current defenses of Bard are also vulnerable. We hope this work can deepen our understanding on the robustness of MLLMs and facilitate future research on defenses. Our code is available at https://github.com/thu-ml/Attack-Bard. Update: GPT-4V is available at October 2023. We further evaluate its robustness under the same set of adversarial examples, achieving a 45% attack success rate.\"), Document(metadata={'title': 'VDC: Versatile Data Cleanser based on Visual-Linguistic Inconsistency by Multimodal Large Language Models'}, page_content='The role of data in building AI systems has recently been emphasized by the emerging concept of data-centric AI. Unfortunately, in the real-world, datasets may contain dirty samples, such as poisoned samples from backdoor attack, noisy labels in crowdsourcing, and even hybrids of them. The presence of such dirty samples makes the DNNs vunerable and unreliable.Hence, it is critical to detect dirty samples to improve the quality and realiability of dataset. Existing detectors only focus on detecting poisoned samples or noisy labels, that are often prone to weak generalization when dealing with dirty samples from other domains.In this paper, we find a commonality of various dirty samples is visual-linguistic inconsistency between images and associated labels. To capture the semantic inconsistency between modalities, we propose versatile data cleanser (VDC) leveraging the surpassing capabilities of multimodal large language models (MLLM) in cross-modal alignment and reasoning.It consists of three consecutive modules: the visual question generation module to generate insightful questions about the image; the visual question answering module to acquire the semantics of the visual content by answering the questions with MLLM; followed by the visual answer evaluation module to evaluate the inconsistency.Extensive experiments demonstrate its superior performance and generalization to various categories and types of dirty samples. The code is available at \\\\url{https://github.com/zihao-ai/vdc}.'), Document(metadata={'title': 'Guiding Instruction-based Image Editing via Multimodal Large Language Models'}, page_content='Instruction-based image editing improves the controllability and flexibility of image manipulation via natural commands without elaborate descriptions or regional masks. However, human instructions are sometimes too brief for current methods to capture and follow. Multimodal large language models (MLLMs) show promising capabilities in cross-modal understanding and visual-aware response generation via LMs. We investigate how MLLMs facilitate edit instructions and present MLLM-Guided Image Editing (MGIE). MGIE learns to derive expressive instructions and provides explicit guidance. The editing model jointly captures this visual imagination and performs manipulation through end-to-end training. We evaluate various aspects of Photoshop-style modification, global photo optimization, and local editing. Extensive experimental results demonstrate that expressive instructions are crucial to instruction-based image editing, and our MGIE can lead to a notable improvement in automatic metrics and human evaluation while maintaining competitive inference efficiency.'), Document(metadata={'title': 'Reformulating Vision-Language Foundation Models and Datasets Towards Universal Multimodal Assistants'}, page_content=\"Recent Multimodal Large Language Models (MLLMs) exhibit impressive abilities to perceive images and follow open-ended instructions. The capabilities of MLLMs depend on two crucial factors: the model architecture to facilitate the feature alignment of visual modules and large language models; the multimodal instruction tuning datasets for human instruction following. (i) For the model architecture, most existing models introduce an external bridge module to connect vision encoders with language models, which needs an additional feature-alignment pre-training. In this work, we discover that compact pre-trained vision language models can inherently serve as ``out-of-the-box'' bridges between vision and language. Based on this, we propose Muffin framework, which directly employs pre-trained vision-language models to act as providers of visual signals. (ii) For the multimodal instruction tuning datasets, existing methods omit the complementary relationship between different datasets and simply mix datasets from different tasks. Instead, we propose UniMM-Chat dataset which explores the complementarities of datasets to generate 1.1M high-quality and diverse multimodal instructions. We merge information describing the same image from diverse datasets and transforms it into more knowledge-intensive conversation data. Experimental results demonstrate the effectiveness of the Muffin framework and UniMM-Chat dataset. Muffin achieves state-of-the-art performance on a wide range of vision-language tasks, significantly surpassing state-of-the-art models like LLaVA and InstructBLIP. Our model and dataset are all accessible at https://github.com/thunlp/muffin.\"), Document(metadata={'title': 'DriveGPT4: Interpretable End-to-end Autonomous Driving via Large Language Model'}, page_content='Multimodal large language models (MLLMs) have emerged as a prominent area of interest within the research community, given their proficiency in handling and reasoning with non-textual data, including images and videos. This study seeks to extend the application of MLLMs to the realm of autonomous driving by introducing DriveGPT4, a novel interpretable end-to-end autonomous driving system based on LLMs. Capable of processing multi-frame video inputs and textual queries, DriveGPT4 facilitates the interpretation of vehicle actions, offers pertinent reasoning, and effectively addresses a diverse range of questions posed by users. Furthermore, DriveGPT4 predicts low-level vehicle control signals in an end-to-end fashion.These advanced capabilities are achieved through the utilization of a bespoke visual instruction tuning dataset, specifically tailored for autonomous driving applications, in conjunction with a mix-finetuning training strategy. DriveGPT4 represents the pioneering effort to leverage LLMs for the development of an interpretable end-to-end autonomous driving solution. Evaluations conducted on the BDD-X dataset showcase the superior qualitative and quantitative performance of DriveGPT4. Additionally, the fine-tuning of domain-specific data enables DriveGPT4 to yield close or even improved results in terms of autonomous driving grounding when contrasted with GPT4-V.'), Document(metadata={'title': 'MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens'}, page_content='The effectiveness of Multimodal Large Language Models (MLLMs) demonstrates a profound capability in multimodal understanding. However, the simultaneous generation of images with coherent texts is still underdeveloped. Addressing this, we introduce a novel interleaved vision-and-language generation method, centered around the concept of ``generative vokens\". These vokens serve as pivotal elements contributing to coherent image-text outputs. Our method is marked by a unique two-stage training strategy for description-free multimodal generation, which does not necessitate extensive descriptions of images. We integrate classifier-free guidance to enhance the alignment of generated images and texts, ensuring more seamless and contextually relevant multimodal interactions. Our model, MiniGPT-5, exhibits substantial improvement over the baseline models on multimodal generation datasets, including MMDialog and VIST. The human evaluation shows MiniGPT-5 is better than the baseline model on more than 56\\\\% cases for multimodal generation, highlighting its efficacy across diverse benchmarks.'), Document(metadata={'title': 'Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond'}, page_content='In this study, we explore the potential of Multimodal Large Language Models (MLLMs) in improving embodied decision-making processes for agents. While Large Language Models (LLMs) have been widely used due to their advanced reasoning skills and vast world knowledge, MLLMs like GPT4-Vision offer enhanced visual understanding and reasoning capabilities. We investigate whether state-of-the-art MLLMs can handle embodied decision-making in an end-to-end manner and whether collaborations between LLMs and MLLMs can enhance decision-making. To address these questions, we introduce a new benchmark called PCA-EVAL, which evaluates embodied decision-making from the perspectives of Perception, Cognition, and Action. Additionally, we propose HOLMES, a multi-agent cooperation framework that allows LLMs to leverage MLLMs and APIs to gather multimodal information for informed decision-making. We compare end-to-end embodied decision-making and HOLMES on our benchmark and find that the GPT4-Vision model demonstrates strong end-to-end embodied decision-making abilities, outperforming GPT4-HOLMES in terms of average decision accuracy (+3%). However, this performance is exclusive to the latest GPT4-Vision model, surpassing the open-source state-of-the-art MLLM by 26%. Our results indicate that powerful MLLMs like GPT4-Vision hold promise for decision-making in embodied agents, offering new avenues for MLLM research. Code and data are open at https://github.com/pkunlp-icler/PCA-EVAL/.'), Document(metadata={'title': 'Kosmos-G: Generating Images in Context with Multimodal Large Language Models'}, page_content='Recent advancements in subject-driven image generation have made significant strides. However, current methods still fall short in diverse application scenarios, as they require test-time tuning and cannot accept interleaved multi-image and text input. These limitations keep them far from the ultimate goal of \"image as a foreign language in image generation.\" This paper presents Kosmos-G, a model that leverages the advanced multimodal perception capabilities of Multimodal Large Language Models (MLLMs) to tackle the aforementioned challenge. Our approach aligns the output space of MLLM with CLIP using the textual modality as an anchor and performs compositional instruction tuning on curated data. Kosmos-G demonstrates an impressive capability of zero-shot subject-driven generation with interleaved multi-image and text input. Notably, the score distillation instruction tuning requires no modifications to the image decoder. This allows for a seamless substitution of CLIP and effortless integration with a myriad of U-Net techniques ranging from fine-grained controls to personalized image decoder variants. We posit Kosmos-G as an initial attempt towards the goal of \"image as a foreign language in image generation.\" The code can be found at https://aka.ms/Kosmos-G'), Document(metadata={'title': 'Fine-grained Audio-Visual Joint Representations for Multimodal Large Language Models'}, page_content='Audio-visual large language models (LLM) have drawn significant attention, yet the fine-grained combination of both input streams is rather under-explored, which is challenging but necessary for LLMs to understand general video inputs. To this end, a fine-grained audio-visual joint representation (FAVOR) learning framework for multimodal LLMs is proposed in this paper, which extends a text-based LLM to simultaneously perceive speech and audio events in the audio input stream and images or videos in the visual input stream, at the frame level. To fuse the audio and visual feature streams into joint representations and to align the joint space with the LLM input embedding space, we propose a causal Q-Former structure with a causal attention module to enhance the capture of causal relations of the audio-visual frames across time. An audio-visual evaluation benchmark (AVEB) is also proposed which comprises six representative single-modal tasks with five cross-modal tasks reflecting audio-visual co-reasoning abilities. While achieving competitive single-modal performance on audio, speech and image tasks in AVEB, FAVOR achieved over 20% accuracy improvements on the video question-answering task when fine-grained information or temporal causal reasoning is required. FAVOR, in addition, demonstrated remarkable video comprehension and reasoning abilities on tasks that are unprecedented by other multimodal LLMs. An interactive demo of FAVOR is available at https://github.com/BriansIDP/AudioVisualLLM.git, and the training code and model checkpoints will be released soon.'), Document(metadata={'title': 'Can We Edit Multimodal Large Language Models?'}, page_content='In this paper, we focus on editing Multimodal Large Language Models (MLLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights. Code and dataset are available in https://github.com/zjunlp/EasyEdit.'), Document(metadata={'title': 'AutoRepo: A general framework for multi-modal LLM-based automated construction reporting'}, page_content='Ensuring the safety, quality, and timely completion of construction projects is paramount, with construction inspections serving as a vital instrument towards these goals. Nevertheless, the predominantly manual approach of present-day inspections frequently results in inefficiencies and inadequate information management. Such methods often fall short of providing holistic, exhaustive assessments, consequently engendering regulatory oversights and potential safety hazards. To address this issue, this paper presents a novel framework named AutoRepo for automated generation of construction inspection reports. The unmanned vehicles efficiently perform construction inspections and collect scene information, while the multimodal large language models (LLMs) are leveraged to automatically generate the inspection reports. The framework was applied and tested on a real-world construction site, demonstrating its potential to expedite the inspection process, significantly reduce resource allocation, and produce high-quality, regulatory standard-compliant inspection reports. This research thus underscores the immense potential of multimodal large language models in revolutionizing construction inspection practices, signaling a significant leap forward towards a more efficient and safer construction management paradigm.'), Document(metadata={'title': 'MM-BigBench: Evaluating Multimodal Models on Multimodal Content Comprehension Tasks'}, page_content=\"The popularity of multimodal large language models (MLLMs) has triggered a recent surge in research efforts dedicated to evaluating these models. Nevertheless, existing evaluation studies of MLLMs primarily focus on the comprehension and reasoning of unimodal (vision) content, neglecting performance evaluations in the domain of multimodal (vision-language) content understanding. Beyond multimodal reasoning, tasks related to multimodal content comprehension necessitate a profound understanding of multimodal contexts, achieved through the multimodal interaction to obtain a final answer. In this paper, we introduce a comprehensive assessment framework called MM-BigBench, which incorporates a diverse range of metrics to offer an extensive evaluation of the performance of various models and instructions across a wide spectrum of diverse multimodal content comprehension tasks. Consequently, our work complements research on the performance of MLLMs in multimodal comprehension tasks, achieving a more comprehensive and holistic evaluation of MLLMs. To begin, we employ the Best Performance metric to ascertain each model's performance upper bound on different datasets. Subsequently, the Mean Relative Gain metric offers an assessment of the overall performance of various models and instructions, while the Stability metric measures their sensitivity. Furthermore, previous research centers on evaluating models independently or solely assessing instructions, neglecting the adaptability between models and instructions. We propose the Adaptability metric to quantify the adaptability between models and instructions. Our paper evaluates a total of 20 language models (14 MLLMs) on 14 multimodal datasets spanning 6 tasks, with 10 instructions for each task, and derives novel insights. Our code will be released at https://github.com/declare-lab/MM-BigBench.\"), Document(metadata={'title': 'RoboLLM: Robotic Vision Tasks Grounded on Multimodal Large Language Models'}, page_content='Robotic vision applications often necessitate a wide range of visual perception tasks, such as object detection, segmentation, and identification. While there have been substantial advances in these individual tasks, integrating specialized models into a unified vision pipeline presents significant engineering challenges and costs. Recently, Multimodal Large Language Models (MLLMs) have emerged as novel backbones for various downstream tasks. We argue that leveraging the pre-training capabilities of MLLMs enables the creation of a simplified framework, thus mitigating the need for task-specific encoders. Specifically, the large-scale pretrained knowledge in MLLMs allows for easier fine-tuning to downstream robotic vision tasks and yields superior performance. We introduce the RoboLLM framework, equipped with a BEiT-3 backbone, to address all visual perception tasks in the ARMBench challenge-a large-scale robotic manipulation dataset about real-world warehouse scenarios. RoboLLM not only outperforms existing baselines but also substantially reduces the engineering burden associated with model selection and tuning. The source code is publicly available at https://github.com/longkukuhi/armbench.'), Document(metadata={'title': 'Benchmarking Sequential Visual Input Reasoning and Prediction in Multimodal Large Language Models'}, page_content=\"Multimodal large language models (MLLMs) have shown great potential in perception and interpretation tasks, but their capabilities in predictive reasoning remain under-explored. To address this gap, we introduce a novel benchmark that assesses the predictive reasoning capabilities of MLLMs across diverse scenarios. Our benchmark targets three important domains: abstract pattern reasoning, human activity prediction, and physical interaction prediction. We further develop three evaluation methods powered by large language model to robustly quantify a model's performance in predicting and reasoning the future based on multi-visual context. Empirical experiments confirm the soundness of the proposed benchmark and evaluation methods via rigorous testing and reveal pros and cons of current popular MLLMs in the task of predictive reasoning. Lastly, our proposed benchmark provides a standardized evaluation framework for MLLMs and can facilitate the development of more advanced models that can reason and predict over complex long sequence of multimodal input.\"), Document(metadata={'title': 'Woodpecker: Hallucination Correction for Multimodal Large Language Models'}, page_content='Hallucination is a big shadow hanging over the rapidly evolving Multimodal Large Language Models (MLLMs), referring to the phenomenon that the generated text is inconsistent with the image content. In order to mitigate hallucinations, existing studies mainly resort to an instruction-tuning manner that requires retraining the models with specific data. In this paper, we pave a different way, introducing a training-free method named Woodpecker. Like a woodpecker heals trees, it picks out and corrects hallucinations from the generated text. Concretely, Woodpecker consists of five stages: key concept extraction, question formulation, visual knowledge validation, visual claim generation, and hallucination correction. Implemented in a post-remedy manner, Woodpecker can easily serve different MLLMs, while being interpretable by accessing intermediate outputs of the five stages. We evaluate Woodpecker both quantitatively and qualitatively and show the huge potential of this new paradigm. On the POPE benchmark, our method obtains a 30.66%/24.33% improvement in accuracy over the baseline MiniGPT-4/mPLUG-Owl. The source code is released at https://github.com/BradyFU/Woodpecker.'), Document(metadata={'title': 'Towards Perceiving Small Visual Details in Zero-shot Visual Question Answering with Multimodal LLMs'}, page_content=\"Multimodal Large Language Models (MLLMs) have recently achieved promising zero-shot accuracy on visual question answering (VQA) -- a fundamental task affecting various downstream applications and domains. Given the great potential for the broad use of these models, it is important to investigate their limitations in dealing with different image and question properties. In this work, we investigate whether MLLMs can perceive small details as well as large details in images. In particular, we show that their zero-shot accuracy in answering visual questions is very sensitive to the size of the visual subject of the question, declining up to 46% with size. Furthermore, we show that this effect is causal by observing that human visual cropping can significantly mitigate their sensitivity to size. Inspired by the usefulness of human cropping, we then propose five automatic visual cropping methods -- leveraging either external localization models or the decision process of the given MLLM itself -- as inference time mechanisms to improve the zero-shot performance of MLLMs. We study their effectiveness on four popular VQA datasets, and a subset of the VQAv2 dataset tailored towards fine visual details. Our findings suggest that MLLMs should be used with caution in detail-sensitive VQA applications, and that visual cropping is a promising direction to improve their zero-shot performance. To facilitate further investigation of MLLMs' behaviors, our code and data are publicly released.\"), Document(metadata={'title': 'ChEF: A Comprehensive Evaluation Framework for Standardized Assessment of Multimodal Large Language Models'}, page_content=\"Multimodal Large Language Models (MLLMs) have shown impressive abilities in interacting with visual content with myriad potential downstream tasks. However, even though a list of benchmarks has been proposed, the capabilities and limitations of MLLMs are still not comprehensively understood, due to a lack of a standardized and holistic evaluation framework. To this end, we present the first Comprehensive Evaluation Framework (ChEF) that can holistically profile each MLLM and fairly compare different MLLMs. First, we structure ChEF as four modular components, i.e., Scenario as scalable multimodal datasets, Instruction as flexible instruction retrieving formulae, Inferencer as reliable question answering strategies, and Metric as indicative task-specific score functions. Based on them, ChEF facilitates versatile evaluations in a standardized framework, and new evaluations can be built by designing new Recipes (systematic selection of these four components). Notably, current MLLM benchmarks can be readily summarized as recipes of ChEF. Second, we introduce 6 new recipes to quantify competent MLLMs' desired capabilities (or called desiderata, i.e., calibration, in-context learning, instruction following, language performance, hallucination, and robustness) as reliable agents that can perform real-world multimodal interactions. Third, we conduct a large-scale evaluation of 9 prominent MLLMs on 9 scenarios and 6 desiderata. Our evaluation summarized over 20 valuable observations concerning the generalizability of MLLMs across various scenarios and the composite capability of MLLMs required for multimodal interactions. We will publicly release all the detailed implementations for further analysis, as well as an easy-to-use modular toolkit for the integration of new recipes and models, so that ChEF can be a growing evaluation framework for the MLLM community.\"), Document(metadata={'title': 'Octavius: Mitigating Task Interference in MLLMs via LoRA-MoE'}, page_content='Recent studies have demonstrated Large Language Models (LLMs) can extend their zero-shot generalization capabilities to multimodal learning through instruction tuning. As more modalities and downstream tasks are introduced, negative conflicts and interference may have a worse impact on performance. While this phenomenon has been overlooked in previous work, we propose a novel and extensible framework, called Octavius, for comprehensive studies and experimentation on multimodal learning with Multimodal Large Language Models (MLLMs). Specifically, we combine the well-known Mixture-of-Experts (MoE) and one of the representative PEFT techniques, i.e., LoRA, designing a novel LLM-based decoder, called LoRA-MoE, for multimodal learning. To the best of our knowledge, we are one of the pioneering efforts to introduce MoE into MLLMs to address this problem. The experimental results (about 20% improvement) have shown the effectiveness and versatility of our design in various 2D and 3D downstream tasks. Code and datasets are available at https://openlamm.github.io/tutorial/.'), Document(metadata={'title': 'What Large Language Models Bring to Text-rich VQA?'}, page_content=\"Text-rich VQA, namely Visual Question Answering based on text recognition in the images, is a cross-modal task that requires both image comprehension and text recognition. In this work, we focus on investigating the advantages and bottlenecks of LLM-based approaches in addressing this problem. To address the above concern, we separate the vision and language modules, where we leverage external OCR models to recognize texts in the image and Large Language Models (LLMs) to answer the question given texts. The whole framework is training-free benefiting from the in-context ability of LLMs. This pipeline achieved superior performance compared to the majority of existing Multimodal Large Language Models (MLLM) on four text-rich VQA datasets. Besides, based on the ablation study, we find that LLM brings stronger comprehension ability and may introduce helpful knowledge for the VQA problem. The bottleneck for LLM to address text-rich VQA problems may primarily lie in visual part. We also combine the OCR module with MLLMs and pleasantly find that the combination of OCR module with MLLM also works. It's worth noting that not all MLLMs can comprehend the OCR information, which provides insights into how to train an MLLM that preserves the abilities of LLM.\")], 'results': [Document(id='9e9ac81d-692f-441b-8926-4b6ba73ad784', metadata={'title': 'A Survey on Multimodal Large Language Models'}, page_content='Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.'), Document(id='128c9e1b-adb3-49e0-a873-881354331b12', metadata={'title': 'A Survey on Multimodal Large Language Models'}, page_content='Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.'), Document(id='01a7f21a-97a8-45a2-a73c-74526485470f', metadata={'title': 'A Survey on Multimodal Large Language Models'}, page_content='Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.')], 'context': '[1] A Survey on Multimodal Large Language Models\\nRecently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.\\n\\n[2] A Survey on Multimodal Large Language Models\\nRecently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.\\n\\n[3] A Survey on Multimodal Large Language Models\\nRecently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.', 'expert_opinion': 'Multimodal Large Language Models (MLLMs) are a rising research area, with models like GPT-4V using LLMs to perform multimodal tasks [1, 2, 3]. These models exhibit emergent capabilities like image-based story writing and OCR-free math reasoning, which are uncommon in traditional multimodal methods, suggesting a possible route to artificial general intelligence [1, 2, 3]. Research is focused on developing MLLMs that can compete with or outperform GPT-4V [1, 2, 3]. The key aspects of MLLMs under investigation include architecture, training strategies, data, and evaluation methods [1, 2, 3]. Research also explores expanding MLLMs to support more granularity, modalities, languages, and scenarios, as well as addressing multimodal hallucination and incorporating techniques like Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR) [1, 2, 3].', 'messages': [AIMessage(content='Retrieving expert insights based on the papers and cite the papers...', additional_kwargs={}, response_metadata={}, id='22fe505c-90a0-4118-95eb-6045f2c066c7'), HumanMessage(content=\"Context: [1] A Survey on Multimodal Large Language Models\\nRecently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.\\n\\n[2] A Survey on Multimodal Large Language Models\\nRecently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.\\n\\n[3] A Survey on Multimodal Large Language Models\\nRecently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.\\n\\nQuery: latest advances in multimodal learning Consider User's response for the search query: I dont want to know about latest advancements, I want to know about multimodal llms\", additional_kwargs={}, response_metadata={}, id='c9f3c950-8322-4da3-a95b-1115429cd167'), AIMessage(content='Multimodal Large Language Models (MLLMs) are a rising research area, with models like GPT-4V using LLMs to perform multimodal tasks [1, 2, 3]. These models exhibit emergent capabilities like image-based story writing and OCR-free math reasoning, which are uncommon in traditional multimodal methods, suggesting a possible route to artificial general intelligence [1, 2, 3]. Research is focused on developing MLLMs that can compete with or outperform GPT-4V [1, 2, 3]. The key aspects of MLLMs under investigation include architecture, training strategies, data, and evaluation methods [1, 2, 3]. Research also explores expanding MLLMs to support more granularity, modalities, languages, and scenarios, as well as addressing multimodal hallucination and incorporating techniques like Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR) [1, 2, 3].', additional_kwargs={}, response_metadata={}, id='5d83e61e-b24a-4136-81d1-2c670290ed0f')], 'retry_count': 2}\n{'query': \"latest advances in multimodal learning Consider User's response for the search query: I dont want to know about latest advancements, I want to know about multimodal llms\", 'search_query': 'multimodal large language models', 'papers': [{'id': 'multimodal-large-language-models-for', 'arxiv_id': None, 'nips_id': None, 'url_abs': 'https://aclanthology.org/2022.naacl-srw.26', 'url_pdf': 'https://aclanthology.org/2022.naacl-srw.26.pdf', 'title': 'Multimodal large language models for inclusive collaboration learning tasks', 'abstract': 'This PhD project leverages advancements in multimodal large language models to build an inclusive collaboration feedback loop, in order to facilitate the automated detection, modeling, and feedback for participants developing general collaboration skills. This topic is important given the role of collaboration as an essential 21st century skill, the potential to ground large language models within learning theory and real-world practice, and the expressive potential of transformer models to support equity and inclusion. We address some concerns of integrating advances in natural language processing into downstream tasks such as the learning analytics feedback loop.', 'authors': ['Armanda Lewis'], 'published': None, 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': 'naacl-acl-2022-7'}, {'id': 'personality-aware-human-centric-multimodal', 'arxiv_id': '2304.02313', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2304.02313v2', 'url_pdf': 'https://arxiv.org/pdf/2304.02313v2.pdf', 'title': 'Personality-aware Human-centric Multimodal Reasoning: A New Task, Dataset and Baselines', 'abstract': \"Personality traits, emotions, and beliefs shape individuals' behavioral choices and decision-making processes. However, for one thing, the affective computing community normally focused on predicting personality traits but overlooks their application in behavior prediction. For another, the multimodal reasoning task emphasized the prediction of future states and behaviors but often neglected the incorporation of individual personality traits. In this work, we introduce a new task called Personality-aware Human-centric Multimodal Reasoning (PHMR) (T1), with the goal of forecasting the future behavior of a particular individual using multimodal information from past instances, while integrating personality factors. We accordingly construct a new dataset based on six television shows, encompassing 225 characters and 12k samples. To establish a benchmark for the task, we propose seven baseline methods: three adapted from related tasks, two pre-trained model, and two multimodal large language models. The experimental results demonstrate that incorporating personality traits enhances human-centric multimodal reasoning performance. To further solve the lack of personality annotation in real-life scenes, we introduce an extension task called Personality-predicted Human-centric Multimodal Reasoning task (T2) along with the corresponding dataset and method. We will make our dataset and code available on GitHub.\", 'authors': ['Rui Xia', 'Xiangqing Shen', 'Yaochen Zhu'], 'published': '2023-04-05', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'visual-transformation-telling', 'arxiv_id': '2305.01928', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2305.01928v2', 'url_pdf': 'https://arxiv.org/pdf/2305.01928v2.pdf', 'title': 'Visual Transformation Telling', 'abstract': 'Humans can naturally reason from superficial state differences (e.g. ground wetness) to transformations descriptions (e.g. raining) according to their life experience. In this paper, we propose a new visual reasoning task to test this transformation reasoning ability in real-world scenarios, called \\\\textbf{V}isual \\\\textbf{T}ransformation \\\\textbf{T}elling (VTT). Given a series of states (i.e. images), VTT requires to describe the transformation occurring between every two adjacent states. Different from existing visual reasoning tasks that focus on surface state reasoning, the advantage of VTT is that it captures the underlying causes, e.g. actions or events, behind the differences among states. We collect a novel dataset to support the study of transformation reasoning from two existing instructional video datasets, CrossTask and COIN, comprising 13,547 samples. Each sample involves the key state images along with their transformation descriptions. Our dataset covers diverse real-world activities, providing a rich resource for training and evaluation. To construct an initial benchmark for VTT, we test several models, including traditional visual storytelling methods (CST, GLACNet, Densecap) and advanced multimodal large language models (LLaVA v1.5-7B, Qwen-VL-chat, Gemini Pro Vision, GPT-4o, and GPT-4). Experimental results reveal that even state-of-the-art models still face challenges in VTT, highlighting substantial areas for improvement.', 'authors': ['Liang Pang', 'Xin Hong', 'Wanqing Cui', 'Xueqi Cheng', 'Jiafeng Guo', 'Yanyan Lan'], 'published': '2023-05-03', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'listen-think-and-understand', 'arxiv_id': '2305.10790', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2305.10790v3', 'url_pdf': 'https://arxiv.org/pdf/2305.10790v3.pdf', 'title': 'Listen, Think, and Understand', 'abstract': 'The ability of artificial intelligence (AI) systems to perceive and comprehend audio signals is crucial for many applications. Although significant progress has been made in this area since the development of AudioSet, most existing models are designed to map audio inputs to pre-defined, discrete sound label sets. In contrast, humans possess the ability to not only classify sounds into general categories, but also to listen to the finer details of the sounds, explain the reason for the predictions, think about what the sound infers, and understand the scene and what action needs to be taken, if any. Such capabilities beyond perception are not yet present in existing audio models. On the other hand, modern large language models (LLMs) exhibit emerging reasoning ability but they lack audio perception capabilities. Therefore, we ask the question: can we build a model that has both audio perception and a reasoning ability? In this paper, we propose a new audio foundation model, called LTU (Listen, Think, and Understand). To train LTU, we created a new OpenAQA-5M dataset consisting of 1.9 million closed-ended and 3.7 million open-ended, diverse (audio, question, answer) tuples, and have used an autoregressive training framework with a perception-to-understanding curriculum. LTU demonstrates strong performance and generalization ability on conventional audio tasks such as classification and captioning. More importantly, it exhibits emerging audio reasoning and comprehension abilities that are absent in existing audio models. To the best of our knowledge, LTU is one of the first multimodal large language models that focus on general audio (rather than just speech) understanding.', 'authors': ['James Glass', 'Leonid Karlinsky', 'Alexander H. Liu', 'Hongyin Luo', 'Yuan Gong'], 'published': '2023-05-18', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'what-makes-for-good-visual-tokenizers-for', 'arxiv_id': '2305.12223', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2305.12223v2', 'url_pdf': 'https://arxiv.org/pdf/2305.12223v2.pdf', 'title': 'What Makes for Good Visual Tokenizers for Large Language Models?', 'abstract': 'We empirically investigate proper pre-training methods to build good visual tokenizers, making Large Language Models (LLMs) powerful Multimodal Large Language Models (MLLMs). In our benchmark, which is curated to evaluate MLLMs visual semantic understanding and fine-grained perception capabilities, we discussed different visual tokenizers pre-trained with dominant methods (i.e., DeiT, CLIP, MAE, DINO), and observe that: i) Fully/weakly supervised models capture more semantics than self-supervised models, but the gap is narrowed by scaling up the pre-training dataset. ii) Self-supervised models are better at fine-grained perception, where patch-level supervision is particularly effective. iii) Tuning the visual tokenizer leads to the loss of semantics obtained from large-scale pretraining, which is unfavorable with relatively small-scale instruction-tuning dataset. Given the findings, we reviewed methods that attempted to unify semantics and fine-grained visual understanding, e.g., patch-level feature distillation with semantically-rich targets. We obtain an intriguing insight mask-based strategies that were once all the rage may not be applicable for obtaining good visual tokenizers. Based on this critical observation, we obtain a new MLLM equipped with a tailored Good Visual Tokenizer (GVT), which exhibits strong visual comprehension capability at multiple scales. In particular, without introducing extra parameters and task-specific fine-tuning, GVT achieves superior performance on visual question answering, image captioning, and other fine-grained visual understanding tasks such as object counting and multi-class identification.', 'authors': ['Ying Shan', 'Mohan Kankanhalli', 'Xiaohan Ding', 'Yixiao Ge', 'Guangzhi Wang'], 'published': '2023-05-20', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'interpretable-by-design-visual-question', 'arxiv_id': '2305.14882', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2305.14882v2', 'url_pdf': 'https://arxiv.org/pdf/2305.14882v2.pdf', 'title': 'Dynamic Clue Bottlenecks: Towards Interpretable-by-Design Visual Question Answering', 'abstract': 'Recent advances in multimodal large language models (LLMs) have shown extreme effectiveness in visual question answering (VQA). However, the design nature of these end-to-end models prevents them from being interpretable to humans, undermining trust and applicability in critical domains. While post-hoc rationales offer certain insight into understanding model behavior, these explanations are not guaranteed to be faithful to the model. In this paper, we address these shortcomings by introducing an interpretable by design model that factors model decisions into intermediate human-legible explanations, and allows people to easily understand why a model fails or succeeds. We propose the Dynamic Clue Bottleneck Model ( (DCLUB), a method that is designed towards an inherently interpretable VQA system. DCLUB provides an explainable intermediate space before the VQA decision and is faithful from the beginning, while maintaining comparable performance to black-box systems. Given a question, DCLUB first returns a set of visual clues: natural language statements of visually salient evidence from the image, and then generates the output based solely on the visual clues. To supervise and evaluate the generation of VQA explanations within DCLUB, we collect a dataset of 1.7k reasoning-focused questions with visual clues. Evaluations show that our inherently interpretable system can improve 4.64% over a comparable black-box system in reasoning-focused questions while preserving 99.43% of performance on VQA-v2.', 'authors': ['Dan Roth', 'Mark Yatskar', 'Sihao Chen', 'Ben Zhou', 'Xingyu Fu'], 'published': '2023-05-24', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'pathasst-redefining-pathology-through', 'arxiv_id': '2305.15072', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2305.15072v2', 'url_pdf': 'https://arxiv.org/pdf/2305.15072v2.pdf', 'title': 'PathAsst: A Generative Foundation AI Assistant Towards Artificial General Intelligence of Pathology', 'abstract': \"As advances in large language models (LLMs) and multimodal techniques continue to mature, the development of general-purpose multimodal large language models (MLLMs) has surged, offering significant applications in interpreting natural images. However, the field of pathology has largely remained untapped, particularly in gathering high-quality data and designing comprehensive model frameworks. To bridge the gap in pathology MLLMs, we present PathAsst, a multimodal generative foundation AI assistant to revolutionize diagnostic and predictive analytics in pathology. The development of PathAsst involves three pivotal steps: data acquisition, CLIP model adaptation, and the training of PathAsst's multimodal generative capabilities. Firstly, we collect over 207K high-quality pathology image-text pairs from authoritative sources. Leveraging the advanced power of ChatGPT, we generate over 180K instruction-following samples. Furthermore, we devise additional instruction-following data specifically tailored for invoking eight pathology-specific sub-models we prepared, allowing the PathAsst to effectively collaborate with these models, enhancing its diagnostic ability. Secondly, by leveraging the collected data, we construct PathCLIP, a pathology-dedicated CLIP, to enhance PathAsst's capabilities in interpreting pathology images. Finally, we integrate PathCLIP with the Vicuna-13b and utilize pathology-specific instruction-tuning data to enhance the multimodal generation capacity of PathAsst and bolster its synergistic interactions with sub-models. The experimental results of PathAsst show the potential of harnessing AI-powered generative foundation model to improve pathology diagnosis and treatment processes.\", 'authors': ['Yunlong Zhang', 'Zhongyi Shui', 'Lin Sun', 'Lin Yang', 'Honglin Li', 'Kai Zhang', 'Sunyi Zheng', 'Chenglu Zhu', 'Yuxuan Sun'], 'published': '2023-05-24', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'contextual-object-detection-with-multimodal', 'arxiv_id': '2305.18279', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2305.18279v2', 'url_pdf': 'https://arxiv.org/pdf/2305.18279v2.pdf', 'title': 'Contextual Object Detection with Multimodal Large Language Models', 'abstract': 'Recent Multimodal Large Language Models (MLLMs) are remarkable in vision-language tasks, such as image captioning and question answering, but lack the essential perception ability, i.e., object detection. In this work, we address this limitation by introducing a novel research problem of contextual object detection -- understanding visible objects within different human-AI interactive contexts. Three representative scenarios are investigated, including the language cloze test, visual captioning, and question answering. Moreover, we present ContextDET, a unified multimodal model that is capable of end-to-end differentiable modeling of visual-language contexts, so as to locate, identify, and associate visual objects with language inputs for human-AI interaction. Our ContextDET involves three key submodels: (i) a visual encoder for extracting visual representations, (ii) a pre-trained LLM for multimodal context decoding, and (iii) a visual decoder for predicting bounding boxes given contextual object words. The new generate-then-detect framework enables us to detect object words within human vocabulary. Extensive experiments show the advantages of ContextDET on our proposed CODE benchmark, open-vocabulary detection, and referring image segmentation. Github: https://github.com/yuhangzang/ContextDET.', 'authors': ['Chen Change Loy', 'Kaiyang Zhou', 'Jun Han', 'Wei Li', 'Yuhang Zang'], 'published': '2023-05-29', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'a-survey-on-multimodal-large-language-models', 'arxiv_id': '2306.13549', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2306.13549v4', 'url_pdf': 'https://arxiv.org/pdf/2306.13549v4.pdf', 'title': 'A Survey on Multimodal Large Language Models', 'abstract': 'Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.', 'authors': ['Enhong Chen', 'Tong Xu', 'Xing Sun', 'Ke Li', 'Sirui Zhao', 'Chaoyou Fu', 'Shukang Yin'], 'published': '2023-06-23', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'mme-a-comprehensive-evaluation-benchmark-for', 'arxiv_id': '2306.13394', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2306.13394v4', 'url_pdf': 'https://arxiv.org/pdf/2306.13394v4.pdf', 'title': 'MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models', 'abstract': 'Multimodal Large Language Model (MLLM) relies on the powerful LLM to perform multimodal tasks, showing amazing emergent abilities in recent studies, such as writing poems based on an image. However, it is difficult for these case studies to fully reflect the performance of MLLM, lacking a comprehensive evaluation. In this paper, we fill in this blank, presenting the first comprehensive MLLM Evaluation benchmark MME. It measures both perception and cognition abilities on a total of 14 subtasks. In order to avoid data leakage that may arise from direct use of public datasets for evaluation, the annotations of instruction-answer pairs are all manually designed. The concise instruction design allows us to fairly compare MLLMs, instead of struggling in prompt engineering. Besides, with such an instruction, we can also easily carry out quantitative statistics. A total of 30 advanced MLLMs are comprehensively evaluated on our MME, which not only suggests that existing MLLMs still have a large room for improvement, but also reveals the potential directions for the subsequent model optimization. The data application manner and online leaderboards are released at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation.', 'authors': ['Rongrong Ji', 'Yunsheng Wu', 'Jinrui Yang', 'Xing Sun', 'Ke Li', 'Xiawu Zheng', 'Xu Lin', 'Mengdan Zhang', 'Yulei Qin', 'Yunhang Shen', 'Peixian Chen', 'Chaoyou Fu'], 'published': '2023-06-23', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'kosmos-2-grounding-multimodal-large-language', 'arxiv_id': '2306.14824', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2306.14824v3', 'url_pdf': 'https://arxiv.org/pdf/2306.14824v3.pdf', 'title': 'Kosmos-2: Grounding Multimodal Large Language Models to the World', 'abstract': \"We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. Specifically, we represent refer expressions as links in Markdown, i.e., ``[text span](bounding boxes)'', where object descriptions are sequences of location tokens. Together with multimodal corpora, we construct large-scale data of grounded image-text pairs (called GrIT) to train the model. In addition to the existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), Kosmos-2 integrates the grounding capability into downstream applications. We evaluate Kosmos-2 on a wide range of tasks, including (i) multimodal grounding, such as referring expression comprehension, and phrase grounding, (ii) multimodal referring, such as referring expression generation, (iii) perception-language tasks, and (iv) language understanding and generation. This work lays out the foundation for the development of Embodiment AI and sheds light on the big convergence of language, multimodal perception, action, and world modeling, which is a key step toward artificial general intelligence. Code and pretrained models are available at https://aka.ms/kosmos-2.\", 'authors': ['Furu Wei', 'Shuming Ma', 'Shaohan Huang', 'Yaru Hao', 'Li Dong', 'Wenhui Wang', 'Zhiliang Peng'], 'published': '2023-06-26', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'shikra-unleashing-multimodal-llm-s', 'arxiv_id': '2306.15195', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2306.15195v2', 'url_pdf': 'https://arxiv.org/pdf/2306.15195v2.pdf', 'title': \"Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic\", 'abstract': \"In human conversations, individuals can indicate relevant regions within a scene while addressing others. In turn, the other person can then respond by referring to specific regions if necessary. This natural referential ability in dialogue remains absent in current Multimodal Large Language Models (MLLMs). To fill this gap, this paper proposes an MLLM called Shikra, which can handle spatial coordinate inputs and outputs in natural language. Its architecture consists of a vision encoder, an alignment layer, and a LLM. It is designed to be straightforward and simple, without the need for extra vocabularies, position encoder, pre-/post-detection modules, or external plug-in models. All inputs and outputs are in natural language form. Referential dialogue is a superset of various vision-language (VL) tasks. Shikra can naturally handle location-related tasks like REC and PointQA, as well as conventional VL tasks such as Image Captioning and VQA. Experimental results showcase Shikra's promising performance. Furthermore, it enables numerous exciting applications, like providing mentioned objects' coordinates in chains of thoughts and comparing user-pointed regions similarities. Our code, model and dataset are accessed at https://github.com/shikras/shikra.\", 'authors': ['Rui Zhao', 'Feng Zhu', 'Richong Zhang', 'Weili Zeng', 'Zhao Zhang', 'Keqin Chen'], 'published': '2023-06-27', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'svit-scaling-up-visual-instruction-tuning', 'arxiv_id': '2307.04087', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2307.04087v3', 'url_pdf': 'https://arxiv.org/pdf/2307.04087v3.pdf', 'title': 'SVIT: Scaling up Visual Instruction Tuning', 'abstract': \"Thanks to the emerging of foundation models, the large language and vision models are integrated to acquire the multimodal ability of visual captioning, question answering, etc. Although existing multimodal models present impressive performance of visual understanding and reasoning, their limits are still largely under-explored due to the scarcity of high-quality instruction tuning data. To push the limits of multimodal capability, we Scale up Visual Instruction Tuning (SVIT) by constructing a dataset of 4.2 million visual instruction tuning data including 1.6M conversation question-answer (QA) pairs, 1.6M complex reasoning QA pairs, 1.0M referring QA pairs and 106K detailed image descriptions. Besides the volume, the proposed dataset is also featured by the high quality and rich diversity, which is generated by prompting GPT-4 with the abundant manual annotations of images. We also propose a new data recipe to select subset with better diversity and balance, which evokes model's superior capabilities. Extensive experiments verify that SVIT-v1.5, trained on the proposed dataset, outperforms state-of-the-art Multimodal Large Language Models on popular benchmarks. The data and code are publicly available at https://github.com/BAAI-DCAI/Visual-Instruction-Tuning.\", 'authors': ['Tiejun Huang', 'Muyang He', 'Boya Wu', 'Bo Zhao'], 'published': '2023-07-09', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'chatspot-bootstrapping-multimodal-llms-via', 'arxiv_id': '2307.09474', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2307.09474v1', 'url_pdf': 'https://arxiv.org/pdf/2307.09474v1.pdf', 'title': 'ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning', 'abstract': \"Human-AI interactivity is a critical aspect that reflects the usability of multimodal large language models (MLLMs). However, existing end-to-end MLLMs only allow users to interact with them through language instructions, leading to the limitation of the interactive accuracy and efficiency. In this study, we present precise referring instructions that utilize diverse reference representations such as points and boxes as referring prompts to refer to the special region. This enables MLLMs to focus on the region of interest and achieve finer-grained interaction. Based on precise referring instruction, we propose ChatSpot, a unified end-to-end multimodal large language model that supports diverse forms of interactivity including mouse clicks, drag-and-drop, and drawing boxes, which provides a more flexible and seamless interactive experience. We also construct a multi-grained vision-language instruction-following dataset based on existing datasets and GPT-4 generating. Furthermore, we design a series of evaluation tasks to assess the effectiveness of region recognition and interaction. Experimental results showcase ChatSpot's promising performance.\", 'authors': ['Xiangyu Zhang', 'Chunrui Han', 'Runpei Dong', 'Yuang Peng', 'Jianjian Sun', 'HongYu Zhou', 'Haoran Wei', 'Jinrong Yang', 'Zheng Ge', 'En Yu', 'Liang Zhao'], 'published': '2023-07-18', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'seed-bench-benchmarking-multimodal-llms-with', 'arxiv_id': '2307.16125', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2307.16125v2', 'url_pdf': 'https://arxiv.org/pdf/2307.16125v2.pdf', 'title': 'SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension', 'abstract': 'Based on powerful Large Language Models (LLMs), recent generative Multimodal Large Language Models (MLLMs) have gained prominence as a pivotal research area, exhibiting remarkable capability for both comprehension and generation. In this work, we address the evaluation of generative comprehension in MLLMs as a preliminary step towards a comprehensive assessment of generative models, by introducing a benchmark named SEED-Bench. SEED-Bench consists of 19K multiple choice questions with accurate human annotations (x 6 larger than existing benchmarks), which spans 12 evaluation dimensions including the comprehension of both the image and video modality. We develop an advanced pipeline for generating multiple-choice questions that target specific evaluation dimensions, integrating both automatic filtering and manual verification processes. Multiple-choice questions with groundtruth options derived from human annotation enables an objective and efficient assessment of model performance, eliminating the need for human or GPT intervention during evaluation. We further evaluate the performance of 18 models across all 12 dimensions, covering both the spatial and temporal understanding. By revealing the limitations of existing MLLMs through evaluation results, we aim for SEED-Bench to provide insights for motivating future research. We will launch and consistently maintain a leaderboard to provide a platform for the community to assess and investigate model capability.', 'authors': ['Ying Shan', 'Yixiao Ge', 'Yuying Ge', 'Guangzhi Wang', 'Rui Wang', 'Bohao Li'], 'published': '2023-07-30', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'lisa-reasoning-segmentation-via-large', 'arxiv_id': '2308.00692', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.00692v3', 'url_pdf': 'https://arxiv.org/pdf/2308.00692v3.pdf', 'title': 'LISA: Reasoning Segmentation via Large Language Model', 'abstract': 'Although perception systems have made remarkable advancements in recent years, they still rely on explicit human instruction or pre-defined categories to identify the target objects before executing visual recognition tasks. Such systems cannot actively reason and comprehend implicit user intention. In this work, we propose a new segmentation task -- reasoning segmentation. The task is designed to output a segmentation mask given a complex and implicit query text. Furthermore, we establish a benchmark comprising over one thousand image-instruction-mask data samples, incorporating intricate reasoning and world knowledge for evaluation purposes. Finally, we present LISA: large Language Instructed Segmentation Assistant, which inherits the language generation capabilities of multimodal Large Language Models (LLMs) while also possessing the ability to produce segmentation masks. We expand the original vocabulary with a <SEG> token and propose the embedding-as-mask paradigm to unlock the segmentation capability. Remarkably, LISA can handle cases involving complex reasoning and world knowledge. Also, it demonstrates robust zero-shot capability when trained exclusively on reasoning-free datasets. In addition, fine-tuning the model with merely 239 reasoning segmentation data samples results in further performance enhancement. Both quantitative and qualitative experiments show our method effectively unlocks new reasoning segmentation capabilities for multimodal LLMs. Code, models, and data are available at https://github.com/dvlab-research/LISA.', 'authors': ['Jiaya Jia', 'Shu Liu', 'Yuhui Yuan', 'Yanwei Li', 'Yukang Chen', 'Zhuotao Tian', 'Xin Lai'], 'published': '2023-08-01', 'conference': None, 'conference_url_abs': 'http://openaccess.thecvf.com//content/CVPR2024/html/Lai_LISA_Reasoning_Segmentation_via_Large_Language_Model_CVPR_2024_paper.html', 'conference_url_pdf': 'http://openaccess.thecvf.com//content/CVPR2024/papers/Lai_LISA_Reasoning_Segmentation_via_Large_Language_Model_CVPR_2024_paper.pdf', 'proceeding': 'cvpr-2024-1'}, {'id': 'empowering-vision-language-models-to-follow', 'arxiv_id': '2308.04152', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.04152v4', 'url_pdf': 'https://arxiv.org/pdf/2308.04152v4.pdf', 'title': 'Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions', 'abstract': \"Recent advancements in Multimodal Large Language Models (MLLMs) have been utilizing Visual Prompt Generators (VPGs) to convert visual features into tokens that LLMs can recognize. This is achieved by training the VPGs on millions of image-caption pairs, where the VPG-generated tokens of images are fed into a frozen LLM to generate the corresponding captions. However, this image-captioning based training objective inherently biases the VPG to concentrate solely on the primary visual contents sufficient for caption generation, often neglecting other visual details. This shortcoming results in MLLMs' underperformance in comprehending demonstrative instructions consisting of multiple, interleaved, and multimodal instructions that demonstrate the required context to complete a task. To address this issue, we introduce a generic and lightweight Visual Prompt Generator Complete module (VPG-C), which can infer and complete the missing details essential for comprehending demonstrative instructions. Further, we propose a synthetic discriminative training strategy to fine-tune VPG-C, eliminating the need for supervised demonstrative instructions. As for evaluation, we build DEMON, a comprehensive benchmark for demonstrative instruction understanding. Synthetically trained with the proposed strategy, VPG-C achieves significantly stronger zero-shot performance across all tasks of DEMON. Further evaluation on the MME and OwlEval benchmarks also demonstrate the superiority of VPG-C. Our benchmark, code, and pre-trained models are available at https://github.com/DCDmllm/Cheetah.\", 'authors': ['Wenqiao Zhang', 'Wei Ji', 'Yueting Zhuang', 'Siliang Tang', 'Tat-Seng Chua', 'Hanwang Zhang', 'Minghe Gao', 'Zhiqi Ge', 'Kaihang Pan', 'Juncheng Li'], 'published': '2023-08-08', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'link-context-learning-for-multimodal-llms', 'arxiv_id': '2308.07891', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.07891v1', 'url_pdf': 'https://arxiv.org/pdf/2308.07891v1.pdf', 'title': 'Link-Context Learning for Multimodal LLMs', 'abstract': 'The ability to learn from context with novel concepts, and deliver appropriate responses are essential in human conversations. Despite current Multimodal Large Language Models (MLLMs) and Large Language Models (LLMs) being trained on mega-scale datasets, recognizing unseen images or understanding novel concepts in a training-free manner remains a challenge. In-Context Learning (ICL) explores training-free few-shot learning, where models are encouraged to ``learn to learn\" from limited tasks and generalize to unseen tasks. In this work, we propose link-context learning (LCL), which emphasizes \"reasoning from cause and effect\" to augment the learning capabilities of MLLMs. LCL goes beyond traditional ICL by explicitly strengthening the causal relationship between the support set and the query set. By providing demonstrations with causal links, LCL guides the model to discern not only the analogy but also the underlying causal associations between data points, which empowers MLLMs to recognize unseen images and understand novel concepts more effectively. To facilitate the evaluation of this novel approach, we introduce the ISEKAI dataset, comprising exclusively of unseen generated image-label pairs designed for link-context learning. Extensive experiments show that our LCL-MLLM exhibits strong link-context learning capabilities to novel concepts over vanilla MLLMs. Code and data will be released at https://github.com/isekai-portal/Link-Context-Learning.', 'authors': ['Ziwei Liu', 'Rui Zhao', 'Feng Zhu', 'Zhao Zhang', 'Weichen Fan', 'Yan Tai'], 'published': '2023-08-15', 'conference': None, 'conference_url_abs': 'http://openaccess.thecvf.com//content/CVPR2024/html/Tai_Link-Context_Learning_for_Multimodal_LLMs_CVPR_2024_paper.html', 'conference_url_pdf': 'http://openaccess.thecvf.com//content/CVPR2024/papers/Tai_Link-Context_Learning_for_Multimodal_LLMs_CVPR_2024_paper.pdf', 'proceeding': 'cvpr-2024-1'}, {'id': 'fashionlogo-prompting-multimodal-large', 'arxiv_id': '2308.09012', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.09012v2', 'url_pdf': 'https://arxiv.org/pdf/2308.09012v2.pdf', 'title': 'FashionLOGO: Prompting Multimodal Large Language Models for Fashion Logo Embeddings', 'abstract': \"Logo embedding models convert the product logos in images into vectors, enabling their utilization for logo recognition and detection within e-commerce platforms. This facilitates the enforcement of intellectual property rights and enhances product search capabilities. However, current methods treat logo embedding as a purely visual problem. A noteworthy issue is that visual models capture features more than logos. Instead, we view this as a multimodal task, using text as auxiliary information to facilitate the visual model's understanding of the logo. The emerging Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in both visual and textual understanding. Inspired by this, we propose an approach, \\\\textbf{FashionLOGO}, to explore how to prompt MLLMs to generate appropriate text for product images, which can help visual models achieve better logo embeddings. We adopt a cross-attention transformer block that enables visual embedding to automatically learn supplementary knowledge from textual embedding. Our extensive experiments on real-world datasets prove that FashionLOGO is capable of generating generic and robust logo embeddings, achieving state-of-the-art performance in all benchmarks.\", 'authors': ['Walton Wang', 'Minghui Qiu', 'Min Yang', 'Yulin Su', 'Da Li', 'Zhen Wang'], 'published': '2023-08-17', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'wanjuan-a-comprehensive-multimodal-dataset', 'arxiv_id': '2308.10755', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.10755v3', 'url_pdf': 'https://arxiv.org/pdf/2308.10755v3.pdf', 'title': 'WanJuan: A Comprehensive Multimodal Dataset for Advancing English and Chinese Large Models', 'abstract': 'The rise in popularity of ChatGPT and GPT-4 has significantly accelerated the development of large models, leading to the creation of numerous impressive large language models(LLMs) and multimodal large language models (MLLMs). These cutting-edge models owe their remarkable performance to high-quality data. However, the details of the training data used in leading paradigms are often kept confidential. This lack of transparency, coupled with the scarcity of open-source data, impedes further developments within the community. As a response, this paper presents \"Wan Juan\", a large-scale multimodal dataset composed of both Chinese and English data, collected from a wide range of web sources. The dataset incorporates text, image-text, and video modalities, with a total volume exceeding 2TB. It was utilized in the training of InternLM, a model that demonstrated significant advantages in multi-dimensional evaluations when compared to models of a similar scale. All data can be accessed at https://opendatalab.org.cn/WanJuan1.0.', 'authors': ['Dahua Lin', 'Jiaqi Wang', 'Hang Yan', 'Wei Li', 'Bin Wang', 'Jiantao Qiu', 'Chao Xu', 'Zhenjiang Jin', 'Conghui He'], 'published': '2023-08-21', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'stablellava-enhanced-visual-instruction', 'arxiv_id': '2308.10253', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.10253v2', 'url_pdf': 'https://arxiv.org/pdf/2308.10253v2.pdf', 'title': 'StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data', 'abstract': \"The remarkable multimodal capabilities demonstrated by OpenAI's GPT-4 have sparked significant interest in the development of multimodal Large Language Models (LLMs). A primary research objective of such models is to align visual and textual modalities effectively while comprehending human instructions. Current methodologies often rely on annotations derived from benchmark datasets to construct image-dialogue datasets for training purposes, akin to instruction tuning in LLMs. However, these datasets often exhibit domain bias, potentially constraining the generative capabilities of the models. In an effort to mitigate these limitations, we propose a novel data collection methodology that synchronously synthesizes images and dialogues for visual instruction tuning. This approach harnesses the power of generative models, marrying the abilities of ChatGPT and text-to-image generative models to yield a diverse and controllable dataset with varied image content. Additionally, datasets can be arbitrarily scaled. This not only provides greater flexibility compared to existing methodologies but also significantly enhances several model capabilities. Our research includes comprehensive experiments conducted on various datasets. The results emphasize substantial enhancements in more than ten commonly assessed capabilities. Additionally, our model achieves state-of-the-art results across multiple widely recognized multimodal benchmarks.\", 'authors': ['Yunchao Wei', 'Ling Chen', 'Chunhua Shen', 'Guosheng Lin', 'Bin Fu', 'Zhibin Wang', 'Gang Yu', 'Chi Zhang', 'Yanda Li'], 'published': '2023-08-20', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'instructiongpt-4-a-200-instruction-paradigm', 'arxiv_id': '2308.12067', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.12067v2', 'url_pdf': 'https://arxiv.org/pdf/2308.12067v2.pdf', 'title': 'InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4', 'abstract': 'Multimodal large language models are typically trained in two stages: first pre-training on image-text pairs, and then fine-tuning using supervised vision-language instruction data. Recent studies have shown that large language models can achieve satisfactory results even with a limited amount of high-quality instruction-following data. In this paper, we introduce InstructionGPT-4, which is fine-tuned on a small dataset comprising only 200 examples, amounting to approximately 6\\\\% of the instruction-following data used in the alignment dataset for MiniGPT-4. To achieve this, we first propose several metrics to access the quality of multimodal instruction data. Based on these metrics, we present an effective and trainable data selector to automatically identify and filter low-quality vision-language data. By employing this method, InstructionGPT-4 outperforms the original MiniGPT-4 on various evaluations. Overall, our findings demonstrate that less but high-quality instruction tuning data is efficient in enabling multimodal large language models to generate better output. Our code is available at https://github.com/waltonfuture/InstructionGPT-4.', 'authors': ['Lichao Sun', 'Weiran Huang', 'Zihao Jiang', 'Lai Wei'], 'published': '2023-08-23', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'vigc-visual-instruction-generation-and', 'arxiv_id': '2308.12714', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.12714v3', 'url_pdf': 'https://arxiv.org/pdf/2308.12714v3.pdf', 'title': 'VIGC: Visual Instruction Generation and Correction', 'abstract': \"The integration of visual encoders and large language models (LLMs) has driven recent progress in multimodal large language models (MLLMs). However, the scarcity of high-quality instruction-tuning data for vision-language tasks remains a challenge. The current leading paradigm, such as LLaVA, relies on language-only GPT-4 to generate data, which requires pre-annotated image captions and detection bounding boxes, suffering from understanding image details. A practical solution to this problem would be to utilize the available multimodal large language models (MLLMs) to generate instruction data for vision-language tasks. However, it's worth noting that the currently accessible MLLMs are not as powerful as their LLM counterparts, as they tend to produce inadequate responses and generate false information. As a solution for addressing the current issue, this paper proposes the Visual Instruction Generation and Correction (VIGC) framework that enables multimodal large language models to generate instruction-tuning data and progressively enhance its quality on-the-fly. Specifically, Visual Instruction Generation (VIG) guides the vision-language model to generate diverse instruction-tuning data. To ensure generation quality, Visual Instruction Correction (VIC) adopts an iterative update mechanism to correct any inaccuracies in data produced by VIG, effectively reducing the risk of hallucination. Leveraging the diverse, high-quality data generated by VIGC, we finetune mainstream models and validate data quality based on various evaluations. Experimental results demonstrate that VIGC not only compensates for the shortcomings of language-only data generation methods, but also effectively enhances the benchmark performance. The models, datasets, and code are available at https://opendatalab.github.io/VIGC.\", 'authors': ['Conghui He', 'Jiaqi Wang', 'Wei Li', 'Weijia Li', 'Xiaoyi Dong', 'Pan Zhang', 'Huaping Zhong', 'Jiahui Peng', 'Xiao Han', 'Fan Wu', 'Bin Wang'], 'published': '2023-08-24', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'position-enhanced-visual-instruction-tuning', 'arxiv_id': '2308.13437', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.13437v2', 'url_pdf': 'https://arxiv.org/pdf/2308.13437v2.pdf', 'title': 'Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models', 'abstract': 'Recently, Multimodal Large Language Models (MLLMs) that enable Large Language Models (LLMs) to interpret images through visual instruction tuning have achieved significant success. However, existing visual instruction tuning methods only utilize image-language instruction data to align the language and image modalities, lacking a more fine-grained cross-modal alignment. In this paper, we propose Position-enhanced Visual Instruction Tuning (PVIT), which extends the functionality of MLLMs by integrating an additional region-level vision encoder. This integration promotes a more detailed comprehension of images for the MLLM. In addition, to efficiently achieve a fine-grained alignment between the vision modules and the LLM, we design multiple data generation strategies to construct an image-region-language instruction dataset. Finally, we present both quantitative experiments and qualitative analysis that demonstrate the superiority of the proposed model. Code and data will be released at https://github.com/PVIT-official/PVIT.', 'authors': ['Yang Liu', 'Maosong Sun', 'Peng Li', 'Xiaoyue Mi', 'Fuwen Luo', 'Ruoyu Qin', 'Chi Chen'], 'published': '2023-08-25', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'mllm-dataengine-an-iterative-refinement', 'arxiv_id': '2308.13566', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.13566v2', 'url_pdf': 'https://arxiv.org/pdf/2308.13566v2.pdf', 'title': 'MLLM-DataEngine: An Iterative Refinement Approach for MLLM', 'abstract': 'Despite the great advance of Multimodal Large Language Models (MLLMs) in both instruction dataset building and benchmarking, the independence of training and evaluation makes current MLLMs hard to further improve their capability under the guidance of evaluation results with a relatively low human cost. In this paper, we propose MLLM-DataEngine, a novel closed-loop system that bridges data generation, model training, and evaluation. Within each loop iteration, the MLLM-DataEngine first analyze the weakness of the model based on the evaluation results, then generate a proper incremental dataset for the next training iteration and enhance the model capability iteratively. Compared with previous data collection methods which are separate from the benchmarking, the data generated by MLLM-DataEngine shows better targeting, quality, and correctness. For targeting, we propose an Adaptive Bad-case Sampling module, which adjusts the ratio of different types of data within each incremental dataset based on the benchmarking results. For quality, we resort to GPT-4 to generate high-quality data with each given data type. For correctness, prompt design is critical for the data generation results. Rather than previous hand-crafted prompt, we propose an Interactive Prompt Optimization strategy, which optimizes the prompt with the multi-round interaction between human and GPT, and improve the correctness of generated data greatly. Through extensive experiments, we find our MLLM-DataEngine could boost the MLLM capability in a targeted and automatic manner, with only a few human participation. We hope it could be a general solution for the following MLLMs building. The MLLM-DataEngine has been open-sourced and is now available at https://github.com/opendatalab/MLLM-DataEngine.', 'authors': ['Conghui He', 'Jiaqi Wang', 'Xiaoyi Dong', 'Pan Zhang', 'Siyuan Huang', 'Bin Wang', 'Linke Ouyang', 'Zhiyuan Zhao'], 'published': '2023-08-25', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'socratis-are-large-multimodal-models', 'arxiv_id': '2308.16741', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.16741v3', 'url_pdf': 'https://arxiv.org/pdf/2308.16741v3.pdf', 'title': 'Socratis: Are large multimodal models emotionally aware?', 'abstract': 'Existing emotion prediction benchmarks contain coarse emotion labels which do not consider the diversity of emotions that an image and text can elicit in humans due to various reasons. Learning diverse reactions to multimodal content is important as intelligent machines take a central role in generating and delivering content to society. To address this gap, we propose Socratis, a societal reactions benchmark, where each image-caption (IC) pair is annotated with multiple emotions and the reasons for feeling them. Socratis contains 18K free-form reactions for 980 emotions on 2075 image-caption pairs from 5 widely-read news and image-caption (IC) datasets. We benchmark the capability of state-of-the-art multimodal large language models to generate the reasons for feeling an emotion given an IC pair. Based on a preliminary human study, we observe that humans prefer human-written reasons over 2 times more often than machine-generated ones. This shows our task is harder than standard generation tasks because it starkly contrasts recent findings where humans cannot tell apart machine vs human-written news articles, for instance. We further see that current captioning metrics based on large vision-language models also fail to correlate with human preferences. We hope that these findings and our benchmark will inspire further research on training emotionally aware models.', 'authors': ['Kate Saenko', 'Bryan A. Plummer', 'Saadia Gabriel', 'Reuben Tan', 'Arijit Ray', 'Katherine Deng'], 'published': '2023-08-31', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'multiway-adapater-adapting-large-scale-multi', 'arxiv_id': '2309.01516', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.01516v3', 'url_pdf': 'https://arxiv.org/pdf/2309.01516v3.pdf', 'title': 'MultiWay-Adapater: Adapting large-scale multi-modal models for scalable image-text retrieval', 'abstract': \"As Multimodal Large Language Models (MLLMs) grow in size, adapting them to specialized tasks becomes increasingly challenging due to high computational and memory demands. Indeed, traditional fine-tuning methods are costly, due to the need for extensive, task-specific training. While efficient adaptation methods exist that aim to reduce these costs, in practice they suffer from shallow inter-modal alignment, which severely hurts model effectiveness. To tackle these computational challenges and improve inter-modal alignment, we introduce the MultiWay-Adapter (MWA), a novel framework featuring an 'Alignment Enhancer'. This enhancer deepens inter-modal alignment, enabling high transferability with minimal tuning effort. Our experiments show that unlike prior efficient tuning approaches, MWA maintains model effectiveness, while reducing training time by up-to 57%. MWA is also lightweight, increasing model size by only 2-3% (in terms of parameters) for state-of-the-art foundation models like BEiT-3 Large. These results demonstrate that MWA provides an efficient and effective adaptation method for MLLMs, significantly broadening their applicability.\", 'authors': ['Gerardo Aragon Camarasa', 'Richard McCreadie', 'George Killick', 'Zijun Long'], 'published': '2023-09-04', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'next-gpt-any-to-any-multimodal-llm', 'arxiv_id': '2309.05519', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.05519v3', 'url_pdf': 'https://arxiv.org/pdf/2309.05519v3.pdf', 'title': 'NExT-GPT: Any-to-Any Multimodal LLM', 'abstract': 'While recently Multimodal Large Language Models (MM-LLMs) have made exciting strides, they mostly fall prey to the limitation of only input-side multimodal understanding, without the ability to produce content in multiple modalities. As we humans always perceive the world and communicate with people through various modalities, developing any-to-any MM-LLMs capable of accepting and delivering content in any modality becomes essential to human-level AI. To fill the gap, we present an end-to-end general-purpose any-to-any MM-LLM system, NExT-GPT. We connect an LLM with multimodal adaptors and different diffusion decoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio. By leveraging the existing well-trained highly-performing encoders and decoders, NExT-GPT is tuned with only a small amount of parameter (1%) of certain projection layers, which not only benefits low-cost training and also facilitates convenient expansion to more potential modalities. Moreover, we introduce a modality-switching instruction tuning (MosIT) and manually curate a high-quality dataset for MosIT, based on which NExT-GPT is empowered with complex cross-modal semantic understanding and content generation. Overall, our research showcases the promising possibility of building an AI agent capable of modeling universal modalities, paving the way for more human-like AI research in the community. Project page: https://next-gpt.github.io/', 'authors': ['Tat-Seng Chua', 'Wei Ji', 'Leigang Qu', 'Hao Fei', 'Shengqiong Wu'], 'published': '2023-09-11', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'hilm-d-towards-high-resolution-understanding', 'arxiv_id': '2309.05186', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.05186v1', 'url_pdf': 'https://arxiv.org/pdf/2309.05186v1.pdf', 'title': 'HiLM-D: Towards High-Resolution Understanding in Multimodal Large Language Models for Autonomous Driving', 'abstract': \"Autonomous driving systems generally employ separate models for different tasks resulting in intricate designs. For the first time, we leverage singular multimodal large language models (MLLMs) to consolidate multiple autonomous driving tasks from videos, i.e., the Risk Object Localization and Intention and Suggestion Prediction (ROLISP) task. ROLISP uses natural language to simultaneously identify and interpret risk objects, understand ego-vehicle intentions, and provide motion suggestions, eliminating the necessity for task-specific architectures. However, lacking high-resolution (HR) information, existing MLLMs often miss small objects (e.g., traffic cones) and overly focus on salient ones (e.g., large trucks) when applied to ROLISP. We propose HiLM-D (Towards High-Resolution Understanding in MLLMs for Autonomous Driving), an efficient method to incorporate HR information into MLLMs for the ROLISP task. Especially, HiLM-D integrates two branches: (i) the low-resolution reasoning branch, can be any MLLMs, processes low-resolution videos to caption risk objects and discern ego-vehicle intentions/suggestions; (ii) the high-resolution perception branch (HR-PB), prominent to HiLM-D,, ingests HR images to enhance detection by capturing vision-specific HR feature maps and prioritizing all potential risks over merely salient objects. Our HR-PB serves as a plug-and-play module, seamlessly fitting into current MLLMs. Experiments on the ROLISP benchmark reveal HiLM-D's notable advantage over leading MLLMs, with improvements of 4.8% in BLEU-4 for captioning and 17.2% in mIoU for detection.\", 'authors': ['Xiaomeng Li', 'Wei zhang', 'Hang Xu', 'Jianhua Han', 'Xinpeng Ding'], 'published': '2023-09-11', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'investigating-the-catastrophic-forgetting-in', 'arxiv_id': '2309.10313', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.10313v4', 'url_pdf': 'https://arxiv.org/pdf/2309.10313v4.pdf', 'title': 'Investigating the Catastrophic Forgetting in Multimodal Large Language Models', 'abstract': 'Following the success of GPT4, there has been a surge in interest in multimodal large language model (MLLM) research. This line of research focuses on developing general-purpose LLMs through fine-tuning pre-trained LLMs and vision models. However, catastrophic forgetting, a notorious phenomenon where the fine-tuned model fails to retain similar performance compared to the pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM). In this paper, we introduce EMT: Evaluating MulTimodality for evaluating the catastrophic forgetting in MLLMs, by treating each MLLM as an image classifier. We first apply EMT to evaluate several open-source fine-tuned MLLMs and we discover that almost all evaluated MLLMs fail to retain the same performance levels as their vision encoders on standard image classification tasks. Moreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess performance throughout the fine-tuning. Interestingly, our results suggest that early-stage fine-tuning on an image dataset improves performance across other image datasets, by enhancing the alignment of text and visual features. However, as fine-tuning proceeds, the MLLMs begin to hallucinate, resulting in a significant loss of generalizability, even when the image encoder remains frozen. Our results suggest that MLLMs have yet to demonstrate performance on par with their vision models on standard image classification tasks and the current MLLM fine-tuning procedure still has room for improvement.', 'authors': ['Yi Ma', 'Yong Jae Lee', 'Qing Qu', 'Mu Cai', 'Xiao Li', 'Shengbang Tong', 'Yuexiang Zhai'], 'published': '2023-09-19', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'dreamllm-synergistic-multimodal-comprehension', 'arxiv_id': '2309.11499', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.11499v2', 'url_pdf': 'https://arxiv.org/pdf/2309.11499v2.pdf', 'title': 'DreamLLM: Synergistic Multimodal Comprehension and Creation', 'abstract': \"This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy. Project page: https://dreamllm.github.io.\", 'authors': ['Li Yi', 'Kaisheng Ma', 'Xiangyu Zhang', 'Xiangwen Kong', 'Haoran Wei', 'HongYu Zhou', 'Jianjian Sun', 'Liang Zhao', 'Jinrong Yang', 'Zheng Ge', 'Zekun Qi', 'Yuang Peng', 'Chunrui Han', 'Runpei Dong'], 'published': '2023-09-20', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'how-robust-is-google-s-bard-to-adversarial', 'arxiv_id': '2309.11751', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.11751v2', 'url_pdf': 'https://arxiv.org/pdf/2309.11751v2.pdf', 'title': \"How Robust is Google's Bard to Adversarial Image Attacks?\", 'abstract': \"Multimodal Large Language Models (MLLMs) that integrate text and other modalities (especially vision) have achieved unprecedented performance in various multimodal tasks. However, due to the unsolved adversarial robustness problem of vision models, MLLMs can have more severe safety and security risks by introducing the vision inputs. In this work, we study the adversarial robustness of Google's Bard, a competitive chatbot to ChatGPT that released its multimodal capability recently, to better understand the vulnerabilities of commercial MLLMs. By attacking white-box surrogate vision encoders or MLLMs, the generated adversarial examples can mislead Bard to output wrong image descriptions with a 22% success rate based solely on the transferability. We show that the adversarial examples can also attack other MLLMs, e.g., a 26% attack success rate against Bing Chat and a 86% attack success rate against ERNIE bot. Moreover, we identify two defense mechanisms of Bard, including face detection and toxicity detection of images. We design corresponding attacks to evade these defenses, demonstrating that the current defenses of Bard are also vulnerable. We hope this work can deepen our understanding on the robustness of MLLMs and facilitate future research on defenses. Our code is available at https://github.com/thu-ml/Attack-Bard. Update: GPT-4V is available at October 2023. We further evaluate its robustness under the same set of adversarial examples, achieving a 45% attack success rate.\", 'authors': ['Jun Zhu', 'Hang Su', 'Yu Tian', 'Yichi Zhang', 'Xiao Yang', 'Zhengwei Fang', 'Jiawei Chen', 'Huanran Chen', 'Yinpeng Dong'], 'published': '2023-09-21', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'vdc-versatile-data-cleanser-for-detecting', 'arxiv_id': '2309.16211', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.16211v2', 'url_pdf': 'https://arxiv.org/pdf/2309.16211v2.pdf', 'title': 'VDC: Versatile Data Cleanser based on Visual-Linguistic Inconsistency by Multimodal Large Language Models', 'abstract': 'The role of data in building AI systems has recently been emphasized by the emerging concept of data-centric AI. Unfortunately, in the real-world, datasets may contain dirty samples, such as poisoned samples from backdoor attack, noisy labels in crowdsourcing, and even hybrids of them. The presence of such dirty samples makes the DNNs vunerable and unreliable.Hence, it is critical to detect dirty samples to improve the quality and realiability of dataset. Existing detectors only focus on detecting poisoned samples or noisy labels, that are often prone to weak generalization when dealing with dirty samples from other domains.In this paper, we find a commonality of various dirty samples is visual-linguistic inconsistency between images and associated labels. To capture the semantic inconsistency between modalities, we propose versatile data cleanser (VDC) leveraging the surpassing capabilities of multimodal large language models (MLLM) in cross-modal alignment and reasoning.It consists of three consecutive modules: the visual question generation module to generate insightful questions about the image; the visual question answering module to acquire the semantics of the visual content by answering the questions with MLLM; followed by the visual answer evaluation module to evaluate the inconsistency.Extensive experiments demonstrate its superior performance and generalization to various categories and types of dirty samples. The code is available at \\\\url{https://github.com/zihao-ai/vdc}.', 'authors': ['Baoyuan Wu', 'Bingzhe Wu', 'Shaokui Wei', 'Mingda Zhang', 'Zihao Zhu'], 'published': '2023-09-28', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'guiding-instruction-based-image-editing-via', 'arxiv_id': '2309.17102', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.17102v2', 'url_pdf': 'https://arxiv.org/pdf/2309.17102v2.pdf', 'title': 'Guiding Instruction-based Image Editing via Multimodal Large Language Models', 'abstract': 'Instruction-based image editing improves the controllability and flexibility of image manipulation via natural commands without elaborate descriptions or regional masks. However, human instructions are sometimes too brief for current methods to capture and follow. Multimodal large language models (MLLMs) show promising capabilities in cross-modal understanding and visual-aware response generation via LMs. We investigate how MLLMs facilitate edit instructions and present MLLM-Guided Image Editing (MGIE). MGIE learns to derive expressive instructions and provides explicit guidance. The editing model jointly captures this visual imagination and performs manipulation through end-to-end training. We evaluate various aspects of Photoshop-style modification, global photo optimization, and local editing. Extensive experimental results demonstrate that expressive instructions are crucial to instruction-based image editing, and our MGIE can lead to a notable improvement in automatic metrics and human evaluation while maintaining competitive inference efficiency.', 'authors': ['Zhe Gan', 'Yinfei Yang', 'William Yang Wang', 'Xianzhi Du', 'Wenze Hu', 'Tsu-Jui Fu'], 'published': '2023-09-29', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'reformulating-vision-language-foundation', 'arxiv_id': '2310.00653', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.00653v1', 'url_pdf': 'https://arxiv.org/pdf/2310.00653v1.pdf', 'title': 'Reformulating Vision-Language Foundation Models and Datasets Towards Universal Multimodal Assistants', 'abstract': \"Recent Multimodal Large Language Models (MLLMs) exhibit impressive abilities to perceive images and follow open-ended instructions. The capabilities of MLLMs depend on two crucial factors: the model architecture to facilitate the feature alignment of visual modules and large language models; the multimodal instruction tuning datasets for human instruction following. (i) For the model architecture, most existing models introduce an external bridge module to connect vision encoders with language models, which needs an additional feature-alignment pre-training. In this work, we discover that compact pre-trained vision language models can inherently serve as ``out-of-the-box'' bridges between vision and language. Based on this, we propose Muffin framework, which directly employs pre-trained vision-language models to act as providers of visual signals. (ii) For the multimodal instruction tuning datasets, existing methods omit the complementary relationship between different datasets and simply mix datasets from different tasks. Instead, we propose UniMM-Chat dataset which explores the complementarities of datasets to generate 1.1M high-quality and diverse multimodal instructions. We merge information describing the same image from diverse datasets and transforms it into more knowledge-intensive conversation data. Experimental results demonstrate the effectiveness of the Muffin framework and UniMM-Chat dataset. Muffin achieves state-of-the-art performance on a wide range of vision-language tasks, significantly surpassing state-of-the-art models like LLaVA and InstructBLIP. Our model and dataset are all accessible at https://github.com/thunlp/muffin.\", 'authors': ['Maosong Sun', 'Hai-Tao Zheng', 'Zhiyuan Liu', 'Dahai Li', 'Jiao Xue', 'Yinxv Pan', 'Shan Wang', 'Chongyi Wang', 'Yue Zhao', 'Haoye Zhang', 'Yuan YAO', 'Jinyi Hu', 'Tianyu Yu'], 'published': '2023-10-01', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'drivegpt4-interpretable-end-to-end-autonomous', 'arxiv_id': '2310.01412', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.01412v5', 'url_pdf': 'https://arxiv.org/pdf/2310.01412v5.pdf', 'title': 'DriveGPT4: Interpretable End-to-end Autonomous Driving via Large Language Model', 'abstract': 'Multimodal large language models (MLLMs) have emerged as a prominent area of interest within the research community, given their proficiency in handling and reasoning with non-textual data, including images and videos. This study seeks to extend the application of MLLMs to the realm of autonomous driving by introducing DriveGPT4, a novel interpretable end-to-end autonomous driving system based on LLMs. Capable of processing multi-frame video inputs and textual queries, DriveGPT4 facilitates the interpretation of vehicle actions, offers pertinent reasoning, and effectively addresses a diverse range of questions posed by users. Furthermore, DriveGPT4 predicts low-level vehicle control signals in an end-to-end fashion.These advanced capabilities are achieved through the utilization of a bespoke visual instruction tuning dataset, specifically tailored for autonomous driving applications, in conjunction with a mix-finetuning training strategy. DriveGPT4 represents the pioneering effort to leverage LLMs for the development of an interpretable end-to-end autonomous driving solution. Evaluations conducted on the BDD-X dataset showcase the superior qualitative and quantitative performance of DriveGPT4. Additionally, the fine-tuning of domain-specific data enables DriveGPT4 to yield close or even improved results in terms of autonomous driving grounding when contrasted with GPT4-V.', 'authors': ['Kwan-Yee. K. Wong', 'Hengshuang Zhao', 'Zhenguo Li', 'Yong Guo', 'Zhen Zhao', 'Enze Xie', 'Yujia Zhang', 'Zhenhua Xu'], 'published': '2023-10-02', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'minigpt-5-interleaved-vision-and-language', 'arxiv_id': '2310.02239', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.02239v3', 'url_pdf': 'https://arxiv.org/pdf/2310.02239v3.pdf', 'title': 'MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens', 'abstract': 'The effectiveness of Multimodal Large Language Models (MLLMs) demonstrates a profound capability in multimodal understanding. However, the simultaneous generation of images with coherent texts is still underdeveloped. Addressing this, we introduce a novel interleaved vision-and-language generation method, centered around the concept of ``generative vokens\". These vokens serve as pivotal elements contributing to coherent image-text outputs. Our method is marked by a unique two-stage training strategy for description-free multimodal generation, which does not necessitate extensive descriptions of images. We integrate classifier-free guidance to enhance the alignment of generated images and texts, ensuring more seamless and contextually relevant multimodal interactions. Our model, MiniGPT-5, exhibits substantial improvement over the baseline models on multimodal generation datasets, including MMDialog and VIST. The human evaluation shows MiniGPT-5 is better than the baseline model on more than 56\\\\% cases for multimodal generation, highlighting its efficacy across diverse benchmarks.', 'authors': ['Xin Eric Wang', 'Xuehai He', 'Kaizhi Zheng'], 'published': '2023-10-03', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'towards-end-to-end-embodied-decision-making', 'arxiv_id': '2310.02071', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.02071v4', 'url_pdf': 'https://arxiv.org/pdf/2310.02071v4.pdf', 'title': 'Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond', 'abstract': 'In this study, we explore the potential of Multimodal Large Language Models (MLLMs) in improving embodied decision-making processes for agents. While Large Language Models (LLMs) have been widely used due to their advanced reasoning skills and vast world knowledge, MLLMs like GPT4-Vision offer enhanced visual understanding and reasoning capabilities. We investigate whether state-of-the-art MLLMs can handle embodied decision-making in an end-to-end manner and whether collaborations between LLMs and MLLMs can enhance decision-making. To address these questions, we introduce a new benchmark called PCA-EVAL, which evaluates embodied decision-making from the perspectives of Perception, Cognition, and Action. Additionally, we propose HOLMES, a multi-agent cooperation framework that allows LLMs to leverage MLLMs and APIs to gather multimodal information for informed decision-making. We compare end-to-end embodied decision-making and HOLMES on our benchmark and find that the GPT4-Vision model demonstrates strong end-to-end embodied decision-making abilities, outperforming GPT4-HOLMES in terms of average decision accuracy (+3%). However, this performance is exclusive to the latest GPT4-Vision model, surpassing the open-source state-of-the-art MLLM by 26%. Our results indicate that powerful MLLMs like GPT4-Vision hold promise for decision-making in embodied agents, offering new avenues for MLLM research. Code and data are open at https://github.com/pkunlp-icler/PCA-EVAL/.', 'authors': ['Peiyi Wang', 'Baobao Chang', 'Tianyu Liu', 'Yuchi Wang', 'Zefan Cai', 'Haozhe Zhao', 'Shuhuai Ren', 'Yichi Zhang', 'Liang Chen'], 'published': '2023-10-03', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'kosmos-g-generating-images-in-context-with', 'arxiv_id': '2310.02992', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.02992v3', 'url_pdf': 'https://arxiv.org/pdf/2310.02992v3.pdf', 'title': 'Kosmos-G: Generating Images in Context with Multimodal Large Language Models', 'abstract': 'Recent advancements in subject-driven image generation have made significant strides. However, current methods still fall short in diverse application scenarios, as they require test-time tuning and cannot accept interleaved multi-image and text input. These limitations keep them far from the ultimate goal of \"image as a foreign language in image generation.\" This paper presents Kosmos-G, a model that leverages the advanced multimodal perception capabilities of Multimodal Large Language Models (MLLMs) to tackle the aforementioned challenge. Our approach aligns the output space of MLLM with CLIP using the textual modality as an anchor and performs compositional instruction tuning on curated data. Kosmos-G demonstrates an impressive capability of zero-shot subject-driven generation with interleaved multi-image and text input. Notably, the score distillation instruction tuning requires no modifications to the image decoder. This allows for a seamless substitution of CLIP and effortless integration with a myriad of U-Net techniques ranging from fine-grained controls to personalized image decoder variants. We posit Kosmos-G as an initial attempt towards the goal of \"image as a foreign language in image generation.\" The code can be found at https://aka.ms/Kosmos-G', 'authors': ['Furu Wei', 'Wenhu Chen', 'Zhiliang Peng', 'Shaohan Huang', 'Li Dong', 'Xichen Pan'], 'published': '2023-10-04', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'fine-grained-audio-visual-joint', 'arxiv_id': '2310.05863', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.05863v2', 'url_pdf': 'https://arxiv.org/pdf/2310.05863v2.pdf', 'title': 'Fine-grained Audio-Visual Joint Representations for Multimodal Large Language Models', 'abstract': 'Audio-visual large language models (LLM) have drawn significant attention, yet the fine-grained combination of both input streams is rather under-explored, which is challenging but necessary for LLMs to understand general video inputs. To this end, a fine-grained audio-visual joint representation (FAVOR) learning framework for multimodal LLMs is proposed in this paper, which extends a text-based LLM to simultaneously perceive speech and audio events in the audio input stream and images or videos in the visual input stream, at the frame level. To fuse the audio and visual feature streams into joint representations and to align the joint space with the LLM input embedding space, we propose a causal Q-Former structure with a causal attention module to enhance the capture of causal relations of the audio-visual frames across time. An audio-visual evaluation benchmark (AVEB) is also proposed which comprises six representative single-modal tasks with five cross-modal tasks reflecting audio-visual co-reasoning abilities. While achieving competitive single-modal performance on audio, speech and image tasks in AVEB, FAVOR achieved over 20% accuracy improvements on the video question-answering task when fine-grained information or temporal causal reasoning is required. FAVOR, in addition, demonstrated remarkable video comprehension and reasoning abilities on tasks that are unprecedented by other multimodal LLMs. An interactive demo of FAVOR is available at https://github.com/BriansIDP/AudioVisualLLM.git, and the training code and model checkpoints will be released soon.', 'authors': ['Chao Zhang', 'Zejun Ma', 'Lu Lu', 'Wei Li', 'Tian Tan', 'Xianzhao Chen', 'Changli Tang', 'Wenyi Yu', 'Guangzhi Sun'], 'published': '2023-10-09', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'can-we-edit-multimodal-large-language-models', 'arxiv_id': '2310.08475', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.08475v5', 'url_pdf': 'https://arxiv.org/pdf/2310.08475v5.pdf', 'title': 'Can We Edit Multimodal Large Language Models?', 'abstract': 'In this paper, we focus on editing Multimodal Large Language Models (MLLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights. Code and dataset are available in https://github.com/zjunlp/EasyEdit.', 'authors': ['Ningyu Zhang', 'Huajun Chen', 'Yongheng Wang', 'Xi Chen', 'Qingbin Liu', 'Bozhong Tian', 'Siyuan Cheng'], 'published': '2023-10-12', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'autorepo-a-general-framework-for-multi-modal', 'arxiv_id': '2310.07944', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.07944v2', 'url_pdf': 'https://arxiv.org/pdf/2310.07944v2.pdf', 'title': 'AutoRepo: A general framework for multi-modal LLM-based automated construction reporting', 'abstract': 'Ensuring the safety, quality, and timely completion of construction projects is paramount, with construction inspections serving as a vital instrument towards these goals. Nevertheless, the predominantly manual approach of present-day inspections frequently results in inefficiencies and inadequate information management. Such methods often fall short of providing holistic, exhaustive assessments, consequently engendering regulatory oversights and potential safety hazards. To address this issue, this paper presents a novel framework named AutoRepo for automated generation of construction inspection reports. The unmanned vehicles efficiently perform construction inspections and collect scene information, while the multimodal large language models (LLMs) are leveraged to automatically generate the inspection reports. The framework was applied and tested on a real-world construction site, demonstrating its potential to expedite the inspection process, significantly reduce resource allocation, and produce high-quality, regulatory standard-compliant inspection reports. This research thus underscores the immense potential of multimodal large language models in revolutionizing construction inspection practices, signaling a significant leap forward towards a more efficient and safer construction management paradigm.', 'authors': ['Heng Li', 'Runhao Guo', 'Jing Li', 'Xincong Yang', 'Hongxu Pu'], 'published': '2023-10-11', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'mm-bigbench-evaluating-multimodal-models-on', 'arxiv_id': '2310.09036', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.09036v1', 'url_pdf': 'https://arxiv.org/pdf/2310.09036v1.pdf', 'title': 'MM-BigBench: Evaluating Multimodal Models on Multimodal Content Comprehension Tasks', 'abstract': \"The popularity of multimodal large language models (MLLMs) has triggered a recent surge in research efforts dedicated to evaluating these models. Nevertheless, existing evaluation studies of MLLMs primarily focus on the comprehension and reasoning of unimodal (vision) content, neglecting performance evaluations in the domain of multimodal (vision-language) content understanding. Beyond multimodal reasoning, tasks related to multimodal content comprehension necessitate a profound understanding of multimodal contexts, achieved through the multimodal interaction to obtain a final answer. In this paper, we introduce a comprehensive assessment framework called MM-BigBench, which incorporates a diverse range of metrics to offer an extensive evaluation of the performance of various models and instructions across a wide spectrum of diverse multimodal content comprehension tasks. Consequently, our work complements research on the performance of MLLMs in multimodal comprehension tasks, achieving a more comprehensive and holistic evaluation of MLLMs. To begin, we employ the Best Performance metric to ascertain each model's performance upper bound on different datasets. Subsequently, the Mean Relative Gain metric offers an assessment of the overall performance of various models and instructions, while the Stability metric measures their sensitivity. Furthermore, previous research centers on evaluating models independently or solely assessing instructions, neglecting the adaptability between models and instructions. We propose the Adaptability metric to quantify the adaptability between models and instructions. Our paper evaluates a total of 20 language models (14 MLLMs) on 14 multimodal datasets spanning 6 tasks, with 10 instructions for each task, and derives novel insights. Our code will be released at https://github.com/declare-lab/MM-BigBench.\", 'authors': ['Soujanya Poria', 'XiaoMing Fu', 'Yifei Zhang', 'Qi Sun', 'Yang Li', 'Daling Wang', 'Ming Wang', 'Shi Feng', 'Wenfang Wu', 'Xiaocui Yang'], 'published': '2023-10-13', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'robollm-robotic-vision-tasks-grounded-on', 'arxiv_id': '2310.10221', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.10221v2', 'url_pdf': 'https://arxiv.org/pdf/2310.10221v2.pdf', 'title': 'RoboLLM: Robotic Vision Tasks Grounded on Multimodal Large Language Models', 'abstract': 'Robotic vision applications often necessitate a wide range of visual perception tasks, such as object detection, segmentation, and identification. While there have been substantial advances in these individual tasks, integrating specialized models into a unified vision pipeline presents significant engineering challenges and costs. Recently, Multimodal Large Language Models (MLLMs) have emerged as novel backbones for various downstream tasks. We argue that leveraging the pre-training capabilities of MLLMs enables the creation of a simplified framework, thus mitigating the need for task-specific encoders. Specifically, the large-scale pretrained knowledge in MLLMs allows for easier fine-tuning to downstream robotic vision tasks and yields superior performance. We introduce the RoboLLM framework, equipped with a BEiT-3 backbone, to address all visual perception tasks in the ARMBench challenge-a large-scale robotic manipulation dataset about real-world warehouse scenarios. RoboLLM not only outperforms existing baselines but also substantially reduces the engineering burden associated with model selection and tuning. The source code is publicly available at https://github.com/longkukuhi/armbench.', 'authors': ['Gerardo Aragon Camarasa', 'Richard McCreadie', 'George Killick', 'Zijun Long'], 'published': '2023-10-16', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'benchmarking-sequential-visual-input', 'arxiv_id': '2310.13473', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.13473v1', 'url_pdf': 'https://arxiv.org/pdf/2310.13473v1.pdf', 'title': 'Benchmarking Sequential Visual Input Reasoning and Prediction in Multimodal Large Language Models', 'abstract': \"Multimodal large language models (MLLMs) have shown great potential in perception and interpretation tasks, but their capabilities in predictive reasoning remain under-explored. To address this gap, we introduce a novel benchmark that assesses the predictive reasoning capabilities of MLLMs across diverse scenarios. Our benchmark targets three important domains: abstract pattern reasoning, human activity prediction, and physical interaction prediction. We further develop three evaluation methods powered by large language model to robustly quantify a model's performance in predicting and reasoning the future based on multi-visual context. Empirical experiments confirm the soundness of the proposed benchmark and evaluation methods via rigorous testing and reveal pros and cons of current popular MLLMs in the task of predictive reasoning. Lastly, our proposed benchmark provides a standardized evaluation framework for MLLMs and can facilitate the development of more advanced models that can reason and predict over complex long sequence of multimodal input.\", 'authors': ['Jianwei Yin', 'Tiancheng Zhao', 'Kangjia Zhao', 'Yu Shu', 'Leigang Sha', 'Mingwei Zhu'], 'published': '2023-10-20', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'woodpecker-hallucination-correction-for', 'arxiv_id': '2310.16045', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.16045v2', 'url_pdf': 'https://arxiv.org/pdf/2310.16045v2.pdf', 'title': 'Woodpecker: Hallucination Correction for Multimodal Large Language Models', 'abstract': 'Hallucination is a big shadow hanging over the rapidly evolving Multimodal Large Language Models (MLLMs), referring to the phenomenon that the generated text is inconsistent with the image content. In order to mitigate hallucinations, existing studies mainly resort to an instruction-tuning manner that requires retraining the models with specific data. In this paper, we pave a different way, introducing a training-free method named Woodpecker. Like a woodpecker heals trees, it picks out and corrects hallucinations from the generated text. Concretely, Woodpecker consists of five stages: key concept extraction, question formulation, visual knowledge validation, visual claim generation, and hallucination correction. Implemented in a post-remedy manner, Woodpecker can easily serve different MLLMs, while being interpretable by accessing intermediate outputs of the five stages. We evaluate Woodpecker both quantitatively and qualitatively and show the huge potential of this new paradigm. On the POPE benchmark, our method obtains a 30.66%/24.33% improvement in accuracy over the baseline MiniGPT-4/mPLUG-Owl. The source code is released at https://github.com/BradyFU/Woodpecker.', 'authors': ['Enhong Chen', 'Xing Sun', 'Ke Li', 'Yunhang Shen', 'Dianbo Sui', 'Hao Wang', 'Tong Xu', 'Sirui Zhao', 'Chaoyou Fu', 'Shukang Yin'], 'published': '2023-10-24', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'visual-cropping-improves-zero-shot-question', 'arxiv_id': '2310.16033', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.16033v3', 'url_pdf': 'https://arxiv.org/pdf/2310.16033v3.pdf', 'title': 'Towards Perceiving Small Visual Details in Zero-shot Visual Question Answering with Multimodal LLMs', 'abstract': \"Multimodal Large Language Models (MLLMs) have recently achieved promising zero-shot accuracy on visual question answering (VQA) -- a fundamental task affecting various downstream applications and domains. Given the great potential for the broad use of these models, it is important to investigate their limitations in dealing with different image and question properties. In this work, we investigate whether MLLMs can perceive small details as well as large details in images. In particular, we show that their zero-shot accuracy in answering visual questions is very sensitive to the size of the visual subject of the question, declining up to 46% with size. Furthermore, we show that this effect is causal by observing that human visual cropping can significantly mitigate their sensitivity to size. Inspired by the usefulness of human cropping, we then propose five automatic visual cropping methods -- leveraging either external localization models or the decision process of the given MLLM itself -- as inference time mechanisms to improve the zero-shot performance of MLLMs. We study their effectiveness on four popular VQA datasets, and a subset of the VQAv2 dataset tailored towards fine visual details. Our findings suggest that MLLMs should be used with caution in detail-sensitive VQA applications, and that visual cropping is a promising direction to improve their zero-shot performance. To facilitate further investigation of MLLMs' behaviors, our code and data are publicly released.\", 'authors': ['Filip Ilievski', 'Prateek Chhikara', 'Mahyar Khayatkhoei', 'Jiarui Zhang'], 'published': '2023-10-24', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'chef-a-comprehensive-evaluation-framework-for', 'arxiv_id': '2311.02692', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2311.02692v1', 'url_pdf': 'https://arxiv.org/pdf/2311.02692v1.pdf', 'title': 'ChEF: A Comprehensive Evaluation Framework for Standardized Assessment of Multimodal Large Language Models', 'abstract': \"Multimodal Large Language Models (MLLMs) have shown impressive abilities in interacting with visual content with myriad potential downstream tasks. However, even though a list of benchmarks has been proposed, the capabilities and limitations of MLLMs are still not comprehensively understood, due to a lack of a standardized and holistic evaluation framework. To this end, we present the first Comprehensive Evaluation Framework (ChEF) that can holistically profile each MLLM and fairly compare different MLLMs. First, we structure ChEF as four modular components, i.e., Scenario as scalable multimodal datasets, Instruction as flexible instruction retrieving formulae, Inferencer as reliable question answering strategies, and Metric as indicative task-specific score functions. Based on them, ChEF facilitates versatile evaluations in a standardized framework, and new evaluations can be built by designing new Recipes (systematic selection of these four components). Notably, current MLLM benchmarks can be readily summarized as recipes of ChEF. Second, we introduce 6 new recipes to quantify competent MLLMs' desired capabilities (or called desiderata, i.e., calibration, in-context learning, instruction following, language performance, hallucination, and robustness) as reliable agents that can perform real-world multimodal interactions. Third, we conduct a large-scale evaluation of 9 prominent MLLMs on 9 scenarios and 6 desiderata. Our evaluation summarized over 20 valuable observations concerning the generalizability of MLLMs across various scenarios and the composite capability of MLLMs required for multimodal interactions. We will publicly release all the detailed implementations for further analysis, as well as an easy-to-use modular toolkit for the integration of new recipes and models, so that ChEF can be a growing evaluation framework for the MLLM community.\", 'authors': ['Jing Shao', 'Yu Qiao', 'Lu Sheng', 'Zhenfei Yin', 'Hongxing Fan', 'Zhipin Wang', 'Zhelun Shi'], 'published': '2023-11-05', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'octavius-mitigating-task-interference-in', 'arxiv_id': '2311.02684', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2311.02684v3', 'url_pdf': 'https://arxiv.org/pdf/2311.02684v3.pdf', 'title': 'Octavius: Mitigating Task Interference in MLLMs via LoRA-MoE', 'abstract': 'Recent studies have demonstrated Large Language Models (LLMs) can extend their zero-shot generalization capabilities to multimodal learning through instruction tuning. As more modalities and downstream tasks are introduced, negative conflicts and interference may have a worse impact on performance. While this phenomenon has been overlooked in previous work, we propose a novel and extensible framework, called Octavius, for comprehensive studies and experimentation on multimodal learning with Multimodal Large Language Models (MLLMs). Specifically, we combine the well-known Mixture-of-Experts (MoE) and one of the representative PEFT techniques, i.e., LoRA, designing a novel LLM-based decoder, called LoRA-MoE, for multimodal learning. To the best of our knowledge, we are one of the pioneering efforts to introduce MoE into MLLMs to address this problem. The experimental results (about 20% improvement) have shown the effectiveness and versatility of our design in various 2D and 3D downstream tasks. Code and datasets are available at https://openlamm.github.io/tutorial/.', 'authors': ['Jing Shao', 'Yu Qiao', 'Wanli Ouyang', 'Lu Sheng', 'Si Liu', 'Zhenfei Yin', 'Huayang Liu', 'Zhen Wang', 'Ziqin Wang', 'Zeren Chen'], 'published': '2023-11-05', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'what-large-language-models-bring-to-text-rich', 'arxiv_id': '2311.07306', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2311.07306v1', 'url_pdf': 'https://arxiv.org/pdf/2311.07306v1.pdf', 'title': 'What Large Language Models Bring to Text-rich VQA?', 'abstract': \"Text-rich VQA, namely Visual Question Answering based on text recognition in the images, is a cross-modal task that requires both image comprehension and text recognition. In this work, we focus on investigating the advantages and bottlenecks of LLM-based approaches in addressing this problem. To address the above concern, we separate the vision and language modules, where we leverage external OCR models to recognize texts in the image and Large Language Models (LLMs) to answer the question given texts. The whole framework is training-free benefiting from the in-context ability of LLMs. This pipeline achieved superior performance compared to the majority of existing Multimodal Large Language Models (MLLM) on four text-rich VQA datasets. Besides, based on the ablation study, we find that LLM brings stronger comprehension ability and may introduce helpful knowledge for the VQA problem. The bottleneck for LLM to address text-rich VQA problems may primarily lie in visual part. We also combine the OCR module with MLLMs and pleasantly find that the combination of OCR module with MLLM also works. It's worth noting that not all MLLMs can comprehend the OCR information, which provides insights into how to train an MLLM that preserves the abilities of LLM.\", 'authors': ['Fei Tan', 'Zechao Li', 'Rui Zhao', 'Jinghui Lu', 'Xinzhe Ni', 'Wei Tang', 'Xuejing Liu'], 'published': '2023-11-13', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}], 'docs': [Document(metadata={'title': 'Multimodal large language models for inclusive collaboration learning tasks'}, page_content='This PhD project leverages advancements in multimodal large language models to build an inclusive collaboration feedback loop, in order to facilitate the automated detection, modeling, and feedback for participants developing general collaboration skills. This topic is important given the role of collaboration as an essential 21st century skill, the potential to ground large language models within learning theory and real-world practice, and the expressive potential of transformer models to support equity and inclusion. We address some concerns of integrating advances in natural language processing into downstream tasks such as the learning analytics feedback loop.'), Document(metadata={'title': 'Personality-aware Human-centric Multimodal Reasoning: A New Task, Dataset and Baselines'}, page_content=\"Personality traits, emotions, and beliefs shape individuals' behavioral choices and decision-making processes. However, for one thing, the affective computing community normally focused on predicting personality traits but overlooks their application in behavior prediction. For another, the multimodal reasoning task emphasized the prediction of future states and behaviors but often neglected the incorporation of individual personality traits. In this work, we introduce a new task called Personality-aware Human-centric Multimodal Reasoning (PHMR) (T1), with the goal of forecasting the future behavior of a particular individual using multimodal information from past instances, while integrating personality factors. We accordingly construct a new dataset based on six television shows, encompassing 225 characters and 12k samples. To establish a benchmark for the task, we propose seven baseline methods: three adapted from related tasks, two pre-trained model, and two multimodal large language models. The experimental results demonstrate that incorporating personality traits enhances human-centric multimodal reasoning performance. To further solve the lack of personality annotation in real-life scenes, we introduce an extension task called Personality-predicted Human-centric Multimodal Reasoning task (T2) along with the corresponding dataset and method. We will make our dataset and code available on GitHub.\"), Document(metadata={'title': 'Visual Transformation Telling'}, page_content='Humans can naturally reason from superficial state differences (e.g. ground wetness) to transformations descriptions (e.g. raining) according to their life experience. In this paper, we propose a new visual reasoning task to test this transformation reasoning ability in real-world scenarios, called \\\\textbf{V}isual \\\\textbf{T}ransformation \\\\textbf{T}elling (VTT). Given a series of states (i.e. images), VTT requires to describe the transformation occurring between every two adjacent states. Different from existing visual reasoning tasks that focus on surface state reasoning, the advantage of VTT is that it captures the underlying causes, e.g. actions or events, behind the differences among states. We collect a novel dataset to support the study of transformation reasoning from two existing instructional video datasets, CrossTask and COIN, comprising 13,547 samples. Each sample involves the key state images along with their transformation descriptions. Our dataset covers diverse real-world activities, providing a rich resource for training and evaluation. To construct an initial benchmark for VTT, we test several models, including traditional visual storytelling methods (CST, GLACNet, Densecap) and advanced multimodal large language models (LLaVA v1.5-7B, Qwen-VL-chat, Gemini Pro Vision, GPT-4o, and GPT-4). Experimental results reveal that even state-of-the-art models still face challenges in VTT, highlighting substantial areas for improvement.'), Document(metadata={'title': 'Listen, Think, and Understand'}, page_content='The ability of artificial intelligence (AI) systems to perceive and comprehend audio signals is crucial for many applications. Although significant progress has been made in this area since the development of AudioSet, most existing models are designed to map audio inputs to pre-defined, discrete sound label sets. In contrast, humans possess the ability to not only classify sounds into general categories, but also to listen to the finer details of the sounds, explain the reason for the predictions, think about what the sound infers, and understand the scene and what action needs to be taken, if any. Such capabilities beyond perception are not yet present in existing audio models. On the other hand, modern large language models (LLMs) exhibit emerging reasoning ability but they lack audio perception capabilities. Therefore, we ask the question: can we build a model that has both audio perception and a reasoning ability? In this paper, we propose a new audio foundation model, called LTU (Listen, Think, and Understand). To train LTU, we created a new OpenAQA-5M dataset consisting of 1.9 million closed-ended and 3.7 million open-ended, diverse (audio, question, answer) tuples, and have used an autoregressive training framework with a perception-to-understanding curriculum. LTU demonstrates strong performance and generalization ability on conventional audio tasks such as classification and captioning. More importantly, it exhibits emerging audio reasoning and comprehension abilities that are absent in existing audio models. To the best of our knowledge, LTU is one of the first multimodal large language models that focus on general audio (rather than just speech) understanding.'), Document(metadata={'title': 'What Makes for Good Visual Tokenizers for Large Language Models?'}, page_content='We empirically investigate proper pre-training methods to build good visual tokenizers, making Large Language Models (LLMs) powerful Multimodal Large Language Models (MLLMs). In our benchmark, which is curated to evaluate MLLMs visual semantic understanding and fine-grained perception capabilities, we discussed different visual tokenizers pre-trained with dominant methods (i.e., DeiT, CLIP, MAE, DINO), and observe that: i) Fully/weakly supervised models capture more semantics than self-supervised models, but the gap is narrowed by scaling up the pre-training dataset. ii) Self-supervised models are better at fine-grained perception, where patch-level supervision is particularly effective. iii) Tuning the visual tokenizer leads to the loss of semantics obtained from large-scale pretraining, which is unfavorable with relatively small-scale instruction-tuning dataset. Given the findings, we reviewed methods that attempted to unify semantics and fine-grained visual understanding, e.g., patch-level feature distillation with semantically-rich targets. We obtain an intriguing insight mask-based strategies that were once all the rage may not be applicable for obtaining good visual tokenizers. Based on this critical observation, we obtain a new MLLM equipped with a tailored Good Visual Tokenizer (GVT), which exhibits strong visual comprehension capability at multiple scales. In particular, without introducing extra parameters and task-specific fine-tuning, GVT achieves superior performance on visual question answering, image captioning, and other fine-grained visual understanding tasks such as object counting and multi-class identification.'), Document(metadata={'title': 'Dynamic Clue Bottlenecks: Towards Interpretable-by-Design Visual Question Answering'}, page_content='Recent advances in multimodal large language models (LLMs) have shown extreme effectiveness in visual question answering (VQA). However, the design nature of these end-to-end models prevents them from being interpretable to humans, undermining trust and applicability in critical domains. While post-hoc rationales offer certain insight into understanding model behavior, these explanations are not guaranteed to be faithful to the model. In this paper, we address these shortcomings by introducing an interpretable by design model that factors model decisions into intermediate human-legible explanations, and allows people to easily understand why a model fails or succeeds. We propose the Dynamic Clue Bottleneck Model ( (DCLUB), a method that is designed towards an inherently interpretable VQA system. DCLUB provides an explainable intermediate space before the VQA decision and is faithful from the beginning, while maintaining comparable performance to black-box systems. Given a question, DCLUB first returns a set of visual clues: natural language statements of visually salient evidence from the image, and then generates the output based solely on the visual clues. To supervise and evaluate the generation of VQA explanations within DCLUB, we collect a dataset of 1.7k reasoning-focused questions with visual clues. Evaluations show that our inherently interpretable system can improve 4.64% over a comparable black-box system in reasoning-focused questions while preserving 99.43% of performance on VQA-v2.'), Document(metadata={'title': 'PathAsst: A Generative Foundation AI Assistant Towards Artificial General Intelligence of Pathology'}, page_content=\"As advances in large language models (LLMs) and multimodal techniques continue to mature, the development of general-purpose multimodal large language models (MLLMs) has surged, offering significant applications in interpreting natural images. However, the field of pathology has largely remained untapped, particularly in gathering high-quality data and designing comprehensive model frameworks. To bridge the gap in pathology MLLMs, we present PathAsst, a multimodal generative foundation AI assistant to revolutionize diagnostic and predictive analytics in pathology. The development of PathAsst involves three pivotal steps: data acquisition, CLIP model adaptation, and the training of PathAsst's multimodal generative capabilities. Firstly, we collect over 207K high-quality pathology image-text pairs from authoritative sources. Leveraging the advanced power of ChatGPT, we generate over 180K instruction-following samples. Furthermore, we devise additional instruction-following data specifically tailored for invoking eight pathology-specific sub-models we prepared, allowing the PathAsst to effectively collaborate with these models, enhancing its diagnostic ability. Secondly, by leveraging the collected data, we construct PathCLIP, a pathology-dedicated CLIP, to enhance PathAsst's capabilities in interpreting pathology images. Finally, we integrate PathCLIP with the Vicuna-13b and utilize pathology-specific instruction-tuning data to enhance the multimodal generation capacity of PathAsst and bolster its synergistic interactions with sub-models. The experimental results of PathAsst show the potential of harnessing AI-powered generative foundation model to improve pathology diagnosis and treatment processes.\"), Document(metadata={'title': 'Contextual Object Detection with Multimodal Large Language Models'}, page_content='Recent Multimodal Large Language Models (MLLMs) are remarkable in vision-language tasks, such as image captioning and question answering, but lack the essential perception ability, i.e., object detection. In this work, we address this limitation by introducing a novel research problem of contextual object detection -- understanding visible objects within different human-AI interactive contexts. Three representative scenarios are investigated, including the language cloze test, visual captioning, and question answering. Moreover, we present ContextDET, a unified multimodal model that is capable of end-to-end differentiable modeling of visual-language contexts, so as to locate, identify, and associate visual objects with language inputs for human-AI interaction. Our ContextDET involves three key submodels: (i) a visual encoder for extracting visual representations, (ii) a pre-trained LLM for multimodal context decoding, and (iii) a visual decoder for predicting bounding boxes given contextual object words. The new generate-then-detect framework enables us to detect object words within human vocabulary. Extensive experiments show the advantages of ContextDET on our proposed CODE benchmark, open-vocabulary detection, and referring image segmentation. Github: https://github.com/yuhangzang/ContextDET.'), Document(metadata={'title': 'A Survey on Multimodal Large Language Models'}, page_content='Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.'), Document(metadata={'title': 'MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models'}, page_content='Multimodal Large Language Model (MLLM) relies on the powerful LLM to perform multimodal tasks, showing amazing emergent abilities in recent studies, such as writing poems based on an image. However, it is difficult for these case studies to fully reflect the performance of MLLM, lacking a comprehensive evaluation. In this paper, we fill in this blank, presenting the first comprehensive MLLM Evaluation benchmark MME. It measures both perception and cognition abilities on a total of 14 subtasks. In order to avoid data leakage that may arise from direct use of public datasets for evaluation, the annotations of instruction-answer pairs are all manually designed. The concise instruction design allows us to fairly compare MLLMs, instead of struggling in prompt engineering. Besides, with such an instruction, we can also easily carry out quantitative statistics. A total of 30 advanced MLLMs are comprehensively evaluated on our MME, which not only suggests that existing MLLMs still have a large room for improvement, but also reveals the potential directions for the subsequent model optimization. The data application manner and online leaderboards are released at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation.'), Document(metadata={'title': 'Kosmos-2: Grounding Multimodal Large Language Models to the World'}, page_content=\"We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. Specifically, we represent refer expressions as links in Markdown, i.e., ``[text span](bounding boxes)'', where object descriptions are sequences of location tokens. Together with multimodal corpora, we construct large-scale data of grounded image-text pairs (called GrIT) to train the model. In addition to the existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), Kosmos-2 integrates the grounding capability into downstream applications. We evaluate Kosmos-2 on a wide range of tasks, including (i) multimodal grounding, such as referring expression comprehension, and phrase grounding, (ii) multimodal referring, such as referring expression generation, (iii) perception-language tasks, and (iv) language understanding and generation. This work lays out the foundation for the development of Embodiment AI and sheds light on the big convergence of language, multimodal perception, action, and world modeling, which is a key step toward artificial general intelligence. Code and pretrained models are available at https://aka.ms/kosmos-2.\"), Document(metadata={'title': \"Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic\"}, page_content=\"In human conversations, individuals can indicate relevant regions within a scene while addressing others. In turn, the other person can then respond by referring to specific regions if necessary. This natural referential ability in dialogue remains absent in current Multimodal Large Language Models (MLLMs). To fill this gap, this paper proposes an MLLM called Shikra, which can handle spatial coordinate inputs and outputs in natural language. Its architecture consists of a vision encoder, an alignment layer, and a LLM. It is designed to be straightforward and simple, without the need for extra vocabularies, position encoder, pre-/post-detection modules, or external plug-in models. All inputs and outputs are in natural language form. Referential dialogue is a superset of various vision-language (VL) tasks. Shikra can naturally handle location-related tasks like REC and PointQA, as well as conventional VL tasks such as Image Captioning and VQA. Experimental results showcase Shikra's promising performance. Furthermore, it enables numerous exciting applications, like providing mentioned objects' coordinates in chains of thoughts and comparing user-pointed regions similarities. Our code, model and dataset are accessed at https://github.com/shikras/shikra.\"), Document(metadata={'title': 'SVIT: Scaling up Visual Instruction Tuning'}, page_content=\"Thanks to the emerging of foundation models, the large language and vision models are integrated to acquire the multimodal ability of visual captioning, question answering, etc. Although existing multimodal models present impressive performance of visual understanding and reasoning, their limits are still largely under-explored due to the scarcity of high-quality instruction tuning data. To push the limits of multimodal capability, we Scale up Visual Instruction Tuning (SVIT) by constructing a dataset of 4.2 million visual instruction tuning data including 1.6M conversation question-answer (QA) pairs, 1.6M complex reasoning QA pairs, 1.0M referring QA pairs and 106K detailed image descriptions. Besides the volume, the proposed dataset is also featured by the high quality and rich diversity, which is generated by prompting GPT-4 with the abundant manual annotations of images. We also propose a new data recipe to select subset with better diversity and balance, which evokes model's superior capabilities. Extensive experiments verify that SVIT-v1.5, trained on the proposed dataset, outperforms state-of-the-art Multimodal Large Language Models on popular benchmarks. The data and code are publicly available at https://github.com/BAAI-DCAI/Visual-Instruction-Tuning.\"), Document(metadata={'title': 'ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning'}, page_content=\"Human-AI interactivity is a critical aspect that reflects the usability of multimodal large language models (MLLMs). However, existing end-to-end MLLMs only allow users to interact with them through language instructions, leading to the limitation of the interactive accuracy and efficiency. In this study, we present precise referring instructions that utilize diverse reference representations such as points and boxes as referring prompts to refer to the special region. This enables MLLMs to focus on the region of interest and achieve finer-grained interaction. Based on precise referring instruction, we propose ChatSpot, a unified end-to-end multimodal large language model that supports diverse forms of interactivity including mouse clicks, drag-and-drop, and drawing boxes, which provides a more flexible and seamless interactive experience. We also construct a multi-grained vision-language instruction-following dataset based on existing datasets and GPT-4 generating. Furthermore, we design a series of evaluation tasks to assess the effectiveness of region recognition and interaction. Experimental results showcase ChatSpot's promising performance.\"), Document(metadata={'title': 'SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension'}, page_content='Based on powerful Large Language Models (LLMs), recent generative Multimodal Large Language Models (MLLMs) have gained prominence as a pivotal research area, exhibiting remarkable capability for both comprehension and generation. In this work, we address the evaluation of generative comprehension in MLLMs as a preliminary step towards a comprehensive assessment of generative models, by introducing a benchmark named SEED-Bench. SEED-Bench consists of 19K multiple choice questions with accurate human annotations (x 6 larger than existing benchmarks), which spans 12 evaluation dimensions including the comprehension of both the image and video modality. We develop an advanced pipeline for generating multiple-choice questions that target specific evaluation dimensions, integrating both automatic filtering and manual verification processes. Multiple-choice questions with groundtruth options derived from human annotation enables an objective and efficient assessment of model performance, eliminating the need for human or GPT intervention during evaluation. We further evaluate the performance of 18 models across all 12 dimensions, covering both the spatial and temporal understanding. By revealing the limitations of existing MLLMs through evaluation results, we aim for SEED-Bench to provide insights for motivating future research. We will launch and consistently maintain a leaderboard to provide a platform for the community to assess and investigate model capability.'), Document(metadata={'title': 'LISA: Reasoning Segmentation via Large Language Model'}, page_content='Although perception systems have made remarkable advancements in recent years, they still rely on explicit human instruction or pre-defined categories to identify the target objects before executing visual recognition tasks. Such systems cannot actively reason and comprehend implicit user intention. In this work, we propose a new segmentation task -- reasoning segmentation. The task is designed to output a segmentation mask given a complex and implicit query text. Furthermore, we establish a benchmark comprising over one thousand image-instruction-mask data samples, incorporating intricate reasoning and world knowledge for evaluation purposes. Finally, we present LISA: large Language Instructed Segmentation Assistant, which inherits the language generation capabilities of multimodal Large Language Models (LLMs) while also possessing the ability to produce segmentation masks. We expand the original vocabulary with a <SEG> token and propose the embedding-as-mask paradigm to unlock the segmentation capability. Remarkably, LISA can handle cases involving complex reasoning and world knowledge. Also, it demonstrates robust zero-shot capability when trained exclusively on reasoning-free datasets. In addition, fine-tuning the model with merely 239 reasoning segmentation data samples results in further performance enhancement. Both quantitative and qualitative experiments show our method effectively unlocks new reasoning segmentation capabilities for multimodal LLMs. Code, models, and data are available at https://github.com/dvlab-research/LISA.'), Document(metadata={'title': 'Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions'}, page_content=\"Recent advancements in Multimodal Large Language Models (MLLMs) have been utilizing Visual Prompt Generators (VPGs) to convert visual features into tokens that LLMs can recognize. This is achieved by training the VPGs on millions of image-caption pairs, where the VPG-generated tokens of images are fed into a frozen LLM to generate the corresponding captions. However, this image-captioning based training objective inherently biases the VPG to concentrate solely on the primary visual contents sufficient for caption generation, often neglecting other visual details. This shortcoming results in MLLMs' underperformance in comprehending demonstrative instructions consisting of multiple, interleaved, and multimodal instructions that demonstrate the required context to complete a task. To address this issue, we introduce a generic and lightweight Visual Prompt Generator Complete module (VPG-C), which can infer and complete the missing details essential for comprehending demonstrative instructions. Further, we propose a synthetic discriminative training strategy to fine-tune VPG-C, eliminating the need for supervised demonstrative instructions. As for evaluation, we build DEMON, a comprehensive benchmark for demonstrative instruction understanding. Synthetically trained with the proposed strategy, VPG-C achieves significantly stronger zero-shot performance across all tasks of DEMON. Further evaluation on the MME and OwlEval benchmarks also demonstrate the superiority of VPG-C. Our benchmark, code, and pre-trained models are available at https://github.com/DCDmllm/Cheetah.\"), Document(metadata={'title': 'Link-Context Learning for Multimodal LLMs'}, page_content='The ability to learn from context with novel concepts, and deliver appropriate responses are essential in human conversations. Despite current Multimodal Large Language Models (MLLMs) and Large Language Models (LLMs) being trained on mega-scale datasets, recognizing unseen images or understanding novel concepts in a training-free manner remains a challenge. In-Context Learning (ICL) explores training-free few-shot learning, where models are encouraged to ``learn to learn\" from limited tasks and generalize to unseen tasks. In this work, we propose link-context learning (LCL), which emphasizes \"reasoning from cause and effect\" to augment the learning capabilities of MLLMs. LCL goes beyond traditional ICL by explicitly strengthening the causal relationship between the support set and the query set. By providing demonstrations with causal links, LCL guides the model to discern not only the analogy but also the underlying causal associations between data points, which empowers MLLMs to recognize unseen images and understand novel concepts more effectively. To facilitate the evaluation of this novel approach, we introduce the ISEKAI dataset, comprising exclusively of unseen generated image-label pairs designed for link-context learning. Extensive experiments show that our LCL-MLLM exhibits strong link-context learning capabilities to novel concepts over vanilla MLLMs. Code and data will be released at https://github.com/isekai-portal/Link-Context-Learning.'), Document(metadata={'title': 'FashionLOGO: Prompting Multimodal Large Language Models for Fashion Logo Embeddings'}, page_content=\"Logo embedding models convert the product logos in images into vectors, enabling their utilization for logo recognition and detection within e-commerce platforms. This facilitates the enforcement of intellectual property rights and enhances product search capabilities. However, current methods treat logo embedding as a purely visual problem. A noteworthy issue is that visual models capture features more than logos. Instead, we view this as a multimodal task, using text as auxiliary information to facilitate the visual model's understanding of the logo. The emerging Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in both visual and textual understanding. Inspired by this, we propose an approach, \\\\textbf{FashionLOGO}, to explore how to prompt MLLMs to generate appropriate text for product images, which can help visual models achieve better logo embeddings. We adopt a cross-attention transformer block that enables visual embedding to automatically learn supplementary knowledge from textual embedding. Our extensive experiments on real-world datasets prove that FashionLOGO is capable of generating generic and robust logo embeddings, achieving state-of-the-art performance in all benchmarks.\"), Document(metadata={'title': 'WanJuan: A Comprehensive Multimodal Dataset for Advancing English and Chinese Large Models'}, page_content='The rise in popularity of ChatGPT and GPT-4 has significantly accelerated the development of large models, leading to the creation of numerous impressive large language models(LLMs) and multimodal large language models (MLLMs). These cutting-edge models owe their remarkable performance to high-quality data. However, the details of the training data used in leading paradigms are often kept confidential. This lack of transparency, coupled with the scarcity of open-source data, impedes further developments within the community. As a response, this paper presents \"Wan Juan\", a large-scale multimodal dataset composed of both Chinese and English data, collected from a wide range of web sources. The dataset incorporates text, image-text, and video modalities, with a total volume exceeding 2TB. It was utilized in the training of InternLM, a model that demonstrated significant advantages in multi-dimensional evaluations when compared to models of a similar scale. All data can be accessed at https://opendatalab.org.cn/WanJuan1.0.'), Document(metadata={'title': 'StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data'}, page_content=\"The remarkable multimodal capabilities demonstrated by OpenAI's GPT-4 have sparked significant interest in the development of multimodal Large Language Models (LLMs). A primary research objective of such models is to align visual and textual modalities effectively while comprehending human instructions. Current methodologies often rely on annotations derived from benchmark datasets to construct image-dialogue datasets for training purposes, akin to instruction tuning in LLMs. However, these datasets often exhibit domain bias, potentially constraining the generative capabilities of the models. In an effort to mitigate these limitations, we propose a novel data collection methodology that synchronously synthesizes images and dialogues for visual instruction tuning. This approach harnesses the power of generative models, marrying the abilities of ChatGPT and text-to-image generative models to yield a diverse and controllable dataset with varied image content. Additionally, datasets can be arbitrarily scaled. This not only provides greater flexibility compared to existing methodologies but also significantly enhances several model capabilities. Our research includes comprehensive experiments conducted on various datasets. The results emphasize substantial enhancements in more than ten commonly assessed capabilities. Additionally, our model achieves state-of-the-art results across multiple widely recognized multimodal benchmarks.\"), Document(metadata={'title': 'InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4'}, page_content='Multimodal large language models are typically trained in two stages: first pre-training on image-text pairs, and then fine-tuning using supervised vision-language instruction data. Recent studies have shown that large language models can achieve satisfactory results even with a limited amount of high-quality instruction-following data. In this paper, we introduce InstructionGPT-4, which is fine-tuned on a small dataset comprising only 200 examples, amounting to approximately 6\\\\% of the instruction-following data used in the alignment dataset for MiniGPT-4. To achieve this, we first propose several metrics to access the quality of multimodal instruction data. Based on these metrics, we present an effective and trainable data selector to automatically identify and filter low-quality vision-language data. By employing this method, InstructionGPT-4 outperforms the original MiniGPT-4 on various evaluations. Overall, our findings demonstrate that less but high-quality instruction tuning data is efficient in enabling multimodal large language models to generate better output. Our code is available at https://github.com/waltonfuture/InstructionGPT-4.'), Document(metadata={'title': 'VIGC: Visual Instruction Generation and Correction'}, page_content=\"The integration of visual encoders and large language models (LLMs) has driven recent progress in multimodal large language models (MLLMs). However, the scarcity of high-quality instruction-tuning data for vision-language tasks remains a challenge. The current leading paradigm, such as LLaVA, relies on language-only GPT-4 to generate data, which requires pre-annotated image captions and detection bounding boxes, suffering from understanding image details. A practical solution to this problem would be to utilize the available multimodal large language models (MLLMs) to generate instruction data for vision-language tasks. However, it's worth noting that the currently accessible MLLMs are not as powerful as their LLM counterparts, as they tend to produce inadequate responses and generate false information. As a solution for addressing the current issue, this paper proposes the Visual Instruction Generation and Correction (VIGC) framework that enables multimodal large language models to generate instruction-tuning data and progressively enhance its quality on-the-fly. Specifically, Visual Instruction Generation (VIG) guides the vision-language model to generate diverse instruction-tuning data. To ensure generation quality, Visual Instruction Correction (VIC) adopts an iterative update mechanism to correct any inaccuracies in data produced by VIG, effectively reducing the risk of hallucination. Leveraging the diverse, high-quality data generated by VIGC, we finetune mainstream models and validate data quality based on various evaluations. Experimental results demonstrate that VIGC not only compensates for the shortcomings of language-only data generation methods, but also effectively enhances the benchmark performance. The models, datasets, and code are available at https://opendatalab.github.io/VIGC.\"), Document(metadata={'title': 'Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models'}, page_content='Recently, Multimodal Large Language Models (MLLMs) that enable Large Language Models (LLMs) to interpret images through visual instruction tuning have achieved significant success. However, existing visual instruction tuning methods only utilize image-language instruction data to align the language and image modalities, lacking a more fine-grained cross-modal alignment. In this paper, we propose Position-enhanced Visual Instruction Tuning (PVIT), which extends the functionality of MLLMs by integrating an additional region-level vision encoder. This integration promotes a more detailed comprehension of images for the MLLM. In addition, to efficiently achieve a fine-grained alignment between the vision modules and the LLM, we design multiple data generation strategies to construct an image-region-language instruction dataset. Finally, we present both quantitative experiments and qualitative analysis that demonstrate the superiority of the proposed model. Code and data will be released at https://github.com/PVIT-official/PVIT.'), Document(metadata={'title': 'MLLM-DataEngine: An Iterative Refinement Approach for MLLM'}, page_content='Despite the great advance of Multimodal Large Language Models (MLLMs) in both instruction dataset building and benchmarking, the independence of training and evaluation makes current MLLMs hard to further improve their capability under the guidance of evaluation results with a relatively low human cost. In this paper, we propose MLLM-DataEngine, a novel closed-loop system that bridges data generation, model training, and evaluation. Within each loop iteration, the MLLM-DataEngine first analyze the weakness of the model based on the evaluation results, then generate a proper incremental dataset for the next training iteration and enhance the model capability iteratively. Compared with previous data collection methods which are separate from the benchmarking, the data generated by MLLM-DataEngine shows better targeting, quality, and correctness. For targeting, we propose an Adaptive Bad-case Sampling module, which adjusts the ratio of different types of data within each incremental dataset based on the benchmarking results. For quality, we resort to GPT-4 to generate high-quality data with each given data type. For correctness, prompt design is critical for the data generation results. Rather than previous hand-crafted prompt, we propose an Interactive Prompt Optimization strategy, which optimizes the prompt with the multi-round interaction between human and GPT, and improve the correctness of generated data greatly. Through extensive experiments, we find our MLLM-DataEngine could boost the MLLM capability in a targeted and automatic manner, with only a few human participation. We hope it could be a general solution for the following MLLMs building. The MLLM-DataEngine has been open-sourced and is now available at https://github.com/opendatalab/MLLM-DataEngine.'), Document(metadata={'title': 'Socratis: Are large multimodal models emotionally aware?'}, page_content='Existing emotion prediction benchmarks contain coarse emotion labels which do not consider the diversity of emotions that an image and text can elicit in humans due to various reasons. Learning diverse reactions to multimodal content is important as intelligent machines take a central role in generating and delivering content to society. To address this gap, we propose Socratis, a societal reactions benchmark, where each image-caption (IC) pair is annotated with multiple emotions and the reasons for feeling them. Socratis contains 18K free-form reactions for 980 emotions on 2075 image-caption pairs from 5 widely-read news and image-caption (IC) datasets. We benchmark the capability of state-of-the-art multimodal large language models to generate the reasons for feeling an emotion given an IC pair. Based on a preliminary human study, we observe that humans prefer human-written reasons over 2 times more often than machine-generated ones. This shows our task is harder than standard generation tasks because it starkly contrasts recent findings where humans cannot tell apart machine vs human-written news articles, for instance. We further see that current captioning metrics based on large vision-language models also fail to correlate with human preferences. We hope that these findings and our benchmark will inspire further research on training emotionally aware models.'), Document(metadata={'title': 'MultiWay-Adapater: Adapting large-scale multi-modal models for scalable image-text retrieval'}, page_content=\"As Multimodal Large Language Models (MLLMs) grow in size, adapting them to specialized tasks becomes increasingly challenging due to high computational and memory demands. Indeed, traditional fine-tuning methods are costly, due to the need for extensive, task-specific training. While efficient adaptation methods exist that aim to reduce these costs, in practice they suffer from shallow inter-modal alignment, which severely hurts model effectiveness. To tackle these computational challenges and improve inter-modal alignment, we introduce the MultiWay-Adapter (MWA), a novel framework featuring an 'Alignment Enhancer'. This enhancer deepens inter-modal alignment, enabling high transferability with minimal tuning effort. Our experiments show that unlike prior efficient tuning approaches, MWA maintains model effectiveness, while reducing training time by up-to 57%. MWA is also lightweight, increasing model size by only 2-3% (in terms of parameters) for state-of-the-art foundation models like BEiT-3 Large. These results demonstrate that MWA provides an efficient and effective adaptation method for MLLMs, significantly broadening their applicability.\"), Document(metadata={'title': 'NExT-GPT: Any-to-Any Multimodal LLM'}, page_content='While recently Multimodal Large Language Models (MM-LLMs) have made exciting strides, they mostly fall prey to the limitation of only input-side multimodal understanding, without the ability to produce content in multiple modalities. As we humans always perceive the world and communicate with people through various modalities, developing any-to-any MM-LLMs capable of accepting and delivering content in any modality becomes essential to human-level AI. To fill the gap, we present an end-to-end general-purpose any-to-any MM-LLM system, NExT-GPT. We connect an LLM with multimodal adaptors and different diffusion decoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio. By leveraging the existing well-trained highly-performing encoders and decoders, NExT-GPT is tuned with only a small amount of parameter (1%) of certain projection layers, which not only benefits low-cost training and also facilitates convenient expansion to more potential modalities. Moreover, we introduce a modality-switching instruction tuning (MosIT) and manually curate a high-quality dataset for MosIT, based on which NExT-GPT is empowered with complex cross-modal semantic understanding and content generation. Overall, our research showcases the promising possibility of building an AI agent capable of modeling universal modalities, paving the way for more human-like AI research in the community. Project page: https://next-gpt.github.io/'), Document(metadata={'title': 'HiLM-D: Towards High-Resolution Understanding in Multimodal Large Language Models for Autonomous Driving'}, page_content=\"Autonomous driving systems generally employ separate models for different tasks resulting in intricate designs. For the first time, we leverage singular multimodal large language models (MLLMs) to consolidate multiple autonomous driving tasks from videos, i.e., the Risk Object Localization and Intention and Suggestion Prediction (ROLISP) task. ROLISP uses natural language to simultaneously identify and interpret risk objects, understand ego-vehicle intentions, and provide motion suggestions, eliminating the necessity for task-specific architectures. However, lacking high-resolution (HR) information, existing MLLMs often miss small objects (e.g., traffic cones) and overly focus on salient ones (e.g., large trucks) when applied to ROLISP. We propose HiLM-D (Towards High-Resolution Understanding in MLLMs for Autonomous Driving), an efficient method to incorporate HR information into MLLMs for the ROLISP task. Especially, HiLM-D integrates two branches: (i) the low-resolution reasoning branch, can be any MLLMs, processes low-resolution videos to caption risk objects and discern ego-vehicle intentions/suggestions; (ii) the high-resolution perception branch (HR-PB), prominent to HiLM-D,, ingests HR images to enhance detection by capturing vision-specific HR feature maps and prioritizing all potential risks over merely salient objects. Our HR-PB serves as a plug-and-play module, seamlessly fitting into current MLLMs. Experiments on the ROLISP benchmark reveal HiLM-D's notable advantage over leading MLLMs, with improvements of 4.8% in BLEU-4 for captioning and 17.2% in mIoU for detection.\"), Document(metadata={'title': 'Investigating the Catastrophic Forgetting in Multimodal Large Language Models'}, page_content='Following the success of GPT4, there has been a surge in interest in multimodal large language model (MLLM) research. This line of research focuses on developing general-purpose LLMs through fine-tuning pre-trained LLMs and vision models. However, catastrophic forgetting, a notorious phenomenon where the fine-tuned model fails to retain similar performance compared to the pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM). In this paper, we introduce EMT: Evaluating MulTimodality for evaluating the catastrophic forgetting in MLLMs, by treating each MLLM as an image classifier. We first apply EMT to evaluate several open-source fine-tuned MLLMs and we discover that almost all evaluated MLLMs fail to retain the same performance levels as their vision encoders on standard image classification tasks. Moreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess performance throughout the fine-tuning. Interestingly, our results suggest that early-stage fine-tuning on an image dataset improves performance across other image datasets, by enhancing the alignment of text and visual features. However, as fine-tuning proceeds, the MLLMs begin to hallucinate, resulting in a significant loss of generalizability, even when the image encoder remains frozen. Our results suggest that MLLMs have yet to demonstrate performance on par with their vision models on standard image classification tasks and the current MLLM fine-tuning procedure still has room for improvement.'), Document(metadata={'title': 'DreamLLM: Synergistic Multimodal Comprehension and Creation'}, page_content=\"This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy. Project page: https://dreamllm.github.io.\"), Document(metadata={'title': \"How Robust is Google's Bard to Adversarial Image Attacks?\"}, page_content=\"Multimodal Large Language Models (MLLMs) that integrate text and other modalities (especially vision) have achieved unprecedented performance in various multimodal tasks. However, due to the unsolved adversarial robustness problem of vision models, MLLMs can have more severe safety and security risks by introducing the vision inputs. In this work, we study the adversarial robustness of Google's Bard, a competitive chatbot to ChatGPT that released its multimodal capability recently, to better understand the vulnerabilities of commercial MLLMs. By attacking white-box surrogate vision encoders or MLLMs, the generated adversarial examples can mislead Bard to output wrong image descriptions with a 22% success rate based solely on the transferability. We show that the adversarial examples can also attack other MLLMs, e.g., a 26% attack success rate against Bing Chat and a 86% attack success rate against ERNIE bot. Moreover, we identify two defense mechanisms of Bard, including face detection and toxicity detection of images. We design corresponding attacks to evade these defenses, demonstrating that the current defenses of Bard are also vulnerable. We hope this work can deepen our understanding on the robustness of MLLMs and facilitate future research on defenses. Our code is available at https://github.com/thu-ml/Attack-Bard. Update: GPT-4V is available at October 2023. We further evaluate its robustness under the same set of adversarial examples, achieving a 45% attack success rate.\"), Document(metadata={'title': 'VDC: Versatile Data Cleanser based on Visual-Linguistic Inconsistency by Multimodal Large Language Models'}, page_content='The role of data in building AI systems has recently been emphasized by the emerging concept of data-centric AI. Unfortunately, in the real-world, datasets may contain dirty samples, such as poisoned samples from backdoor attack, noisy labels in crowdsourcing, and even hybrids of them. The presence of such dirty samples makes the DNNs vunerable and unreliable.Hence, it is critical to detect dirty samples to improve the quality and realiability of dataset. Existing detectors only focus on detecting poisoned samples or noisy labels, that are often prone to weak generalization when dealing with dirty samples from other domains.In this paper, we find a commonality of various dirty samples is visual-linguistic inconsistency between images and associated labels. To capture the semantic inconsistency between modalities, we propose versatile data cleanser (VDC) leveraging the surpassing capabilities of multimodal large language models (MLLM) in cross-modal alignment and reasoning.It consists of three consecutive modules: the visual question generation module to generate insightful questions about the image; the visual question answering module to acquire the semantics of the visual content by answering the questions with MLLM; followed by the visual answer evaluation module to evaluate the inconsistency.Extensive experiments demonstrate its superior performance and generalization to various categories and types of dirty samples. The code is available at \\\\url{https://github.com/zihao-ai/vdc}.'), Document(metadata={'title': 'Guiding Instruction-based Image Editing via Multimodal Large Language Models'}, page_content='Instruction-based image editing improves the controllability and flexibility of image manipulation via natural commands without elaborate descriptions or regional masks. However, human instructions are sometimes too brief for current methods to capture and follow. Multimodal large language models (MLLMs) show promising capabilities in cross-modal understanding and visual-aware response generation via LMs. We investigate how MLLMs facilitate edit instructions and present MLLM-Guided Image Editing (MGIE). MGIE learns to derive expressive instructions and provides explicit guidance. The editing model jointly captures this visual imagination and performs manipulation through end-to-end training. We evaluate various aspects of Photoshop-style modification, global photo optimization, and local editing. Extensive experimental results demonstrate that expressive instructions are crucial to instruction-based image editing, and our MGIE can lead to a notable improvement in automatic metrics and human evaluation while maintaining competitive inference efficiency.'), Document(metadata={'title': 'Reformulating Vision-Language Foundation Models and Datasets Towards Universal Multimodal Assistants'}, page_content=\"Recent Multimodal Large Language Models (MLLMs) exhibit impressive abilities to perceive images and follow open-ended instructions. The capabilities of MLLMs depend on two crucial factors: the model architecture to facilitate the feature alignment of visual modules and large language models; the multimodal instruction tuning datasets for human instruction following. (i) For the model architecture, most existing models introduce an external bridge module to connect vision encoders with language models, which needs an additional feature-alignment pre-training. In this work, we discover that compact pre-trained vision language models can inherently serve as ``out-of-the-box'' bridges between vision and language. Based on this, we propose Muffin framework, which directly employs pre-trained vision-language models to act as providers of visual signals. (ii) For the multimodal instruction tuning datasets, existing methods omit the complementary relationship between different datasets and simply mix datasets from different tasks. Instead, we propose UniMM-Chat dataset which explores the complementarities of datasets to generate 1.1M high-quality and diverse multimodal instructions. We merge information describing the same image from diverse datasets and transforms it into more knowledge-intensive conversation data. Experimental results demonstrate the effectiveness of the Muffin framework and UniMM-Chat dataset. Muffin achieves state-of-the-art performance on a wide range of vision-language tasks, significantly surpassing state-of-the-art models like LLaVA and InstructBLIP. Our model and dataset are all accessible at https://github.com/thunlp/muffin.\"), Document(metadata={'title': 'DriveGPT4: Interpretable End-to-end Autonomous Driving via Large Language Model'}, page_content='Multimodal large language models (MLLMs) have emerged as a prominent area of interest within the research community, given their proficiency in handling and reasoning with non-textual data, including images and videos. This study seeks to extend the application of MLLMs to the realm of autonomous driving by introducing DriveGPT4, a novel interpretable end-to-end autonomous driving system based on LLMs. Capable of processing multi-frame video inputs and textual queries, DriveGPT4 facilitates the interpretation of vehicle actions, offers pertinent reasoning, and effectively addresses a diverse range of questions posed by users. Furthermore, DriveGPT4 predicts low-level vehicle control signals in an end-to-end fashion.These advanced capabilities are achieved through the utilization of a bespoke visual instruction tuning dataset, specifically tailored for autonomous driving applications, in conjunction with a mix-finetuning training strategy. DriveGPT4 represents the pioneering effort to leverage LLMs for the development of an interpretable end-to-end autonomous driving solution. Evaluations conducted on the BDD-X dataset showcase the superior qualitative and quantitative performance of DriveGPT4. Additionally, the fine-tuning of domain-specific data enables DriveGPT4 to yield close or even improved results in terms of autonomous driving grounding when contrasted with GPT4-V.'), Document(metadata={'title': 'MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens'}, page_content='The effectiveness of Multimodal Large Language Models (MLLMs) demonstrates a profound capability in multimodal understanding. However, the simultaneous generation of images with coherent texts is still underdeveloped. Addressing this, we introduce a novel interleaved vision-and-language generation method, centered around the concept of ``generative vokens\". These vokens serve as pivotal elements contributing to coherent image-text outputs. Our method is marked by a unique two-stage training strategy for description-free multimodal generation, which does not necessitate extensive descriptions of images. We integrate classifier-free guidance to enhance the alignment of generated images and texts, ensuring more seamless and contextually relevant multimodal interactions. Our model, MiniGPT-5, exhibits substantial improvement over the baseline models on multimodal generation datasets, including MMDialog and VIST. The human evaluation shows MiniGPT-5 is better than the baseline model on more than 56\\\\% cases for multimodal generation, highlighting its efficacy across diverse benchmarks.'), Document(metadata={'title': 'Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond'}, page_content='In this study, we explore the potential of Multimodal Large Language Models (MLLMs) in improving embodied decision-making processes for agents. While Large Language Models (LLMs) have been widely used due to their advanced reasoning skills and vast world knowledge, MLLMs like GPT4-Vision offer enhanced visual understanding and reasoning capabilities. We investigate whether state-of-the-art MLLMs can handle embodied decision-making in an end-to-end manner and whether collaborations between LLMs and MLLMs can enhance decision-making. To address these questions, we introduce a new benchmark called PCA-EVAL, which evaluates embodied decision-making from the perspectives of Perception, Cognition, and Action. Additionally, we propose HOLMES, a multi-agent cooperation framework that allows LLMs to leverage MLLMs and APIs to gather multimodal information for informed decision-making. We compare end-to-end embodied decision-making and HOLMES on our benchmark and find that the GPT4-Vision model demonstrates strong end-to-end embodied decision-making abilities, outperforming GPT4-HOLMES in terms of average decision accuracy (+3%). However, this performance is exclusive to the latest GPT4-Vision model, surpassing the open-source state-of-the-art MLLM by 26%. Our results indicate that powerful MLLMs like GPT4-Vision hold promise for decision-making in embodied agents, offering new avenues for MLLM research. Code and data are open at https://github.com/pkunlp-icler/PCA-EVAL/.'), Document(metadata={'title': 'Kosmos-G: Generating Images in Context with Multimodal Large Language Models'}, page_content='Recent advancements in subject-driven image generation have made significant strides. However, current methods still fall short in diverse application scenarios, as they require test-time tuning and cannot accept interleaved multi-image and text input. These limitations keep them far from the ultimate goal of \"image as a foreign language in image generation.\" This paper presents Kosmos-G, a model that leverages the advanced multimodal perception capabilities of Multimodal Large Language Models (MLLMs) to tackle the aforementioned challenge. Our approach aligns the output space of MLLM with CLIP using the textual modality as an anchor and performs compositional instruction tuning on curated data. Kosmos-G demonstrates an impressive capability of zero-shot subject-driven generation with interleaved multi-image and text input. Notably, the score distillation instruction tuning requires no modifications to the image decoder. This allows for a seamless substitution of CLIP and effortless integration with a myriad of U-Net techniques ranging from fine-grained controls to personalized image decoder variants. We posit Kosmos-G as an initial attempt towards the goal of \"image as a foreign language in image generation.\" The code can be found at https://aka.ms/Kosmos-G'), Document(metadata={'title': 'Fine-grained Audio-Visual Joint Representations for Multimodal Large Language Models'}, page_content='Audio-visual large language models (LLM) have drawn significant attention, yet the fine-grained combination of both input streams is rather under-explored, which is challenging but necessary for LLMs to understand general video inputs. To this end, a fine-grained audio-visual joint representation (FAVOR) learning framework for multimodal LLMs is proposed in this paper, which extends a text-based LLM to simultaneously perceive speech and audio events in the audio input stream and images or videos in the visual input stream, at the frame level. To fuse the audio and visual feature streams into joint representations and to align the joint space with the LLM input embedding space, we propose a causal Q-Former structure with a causal attention module to enhance the capture of causal relations of the audio-visual frames across time. An audio-visual evaluation benchmark (AVEB) is also proposed which comprises six representative single-modal tasks with five cross-modal tasks reflecting audio-visual co-reasoning abilities. While achieving competitive single-modal performance on audio, speech and image tasks in AVEB, FAVOR achieved over 20% accuracy improvements on the video question-answering task when fine-grained information or temporal causal reasoning is required. FAVOR, in addition, demonstrated remarkable video comprehension and reasoning abilities on tasks that are unprecedented by other multimodal LLMs. An interactive demo of FAVOR is available at https://github.com/BriansIDP/AudioVisualLLM.git, and the training code and model checkpoints will be released soon.'), Document(metadata={'title': 'Can We Edit Multimodal Large Language Models?'}, page_content='In this paper, we focus on editing Multimodal Large Language Models (MLLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights. Code and dataset are available in https://github.com/zjunlp/EasyEdit.'), Document(metadata={'title': 'AutoRepo: A general framework for multi-modal LLM-based automated construction reporting'}, page_content='Ensuring the safety, quality, and timely completion of construction projects is paramount, with construction inspections serving as a vital instrument towards these goals. Nevertheless, the predominantly manual approach of present-day inspections frequently results in inefficiencies and inadequate information management. Such methods often fall short of providing holistic, exhaustive assessments, consequently engendering regulatory oversights and potential safety hazards. To address this issue, this paper presents a novel framework named AutoRepo for automated generation of construction inspection reports. The unmanned vehicles efficiently perform construction inspections and collect scene information, while the multimodal large language models (LLMs) are leveraged to automatically generate the inspection reports. The framework was applied and tested on a real-world construction site, demonstrating its potential to expedite the inspection process, significantly reduce resource allocation, and produce high-quality, regulatory standard-compliant inspection reports. This research thus underscores the immense potential of multimodal large language models in revolutionizing construction inspection practices, signaling a significant leap forward towards a more efficient and safer construction management paradigm.'), Document(metadata={'title': 'MM-BigBench: Evaluating Multimodal Models on Multimodal Content Comprehension Tasks'}, page_content=\"The popularity of multimodal large language models (MLLMs) has triggered a recent surge in research efforts dedicated to evaluating these models. Nevertheless, existing evaluation studies of MLLMs primarily focus on the comprehension and reasoning of unimodal (vision) content, neglecting performance evaluations in the domain of multimodal (vision-language) content understanding. Beyond multimodal reasoning, tasks related to multimodal content comprehension necessitate a profound understanding of multimodal contexts, achieved through the multimodal interaction to obtain a final answer. In this paper, we introduce a comprehensive assessment framework called MM-BigBench, which incorporates a diverse range of metrics to offer an extensive evaluation of the performance of various models and instructions across a wide spectrum of diverse multimodal content comprehension tasks. Consequently, our work complements research on the performance of MLLMs in multimodal comprehension tasks, achieving a more comprehensive and holistic evaluation of MLLMs. To begin, we employ the Best Performance metric to ascertain each model's performance upper bound on different datasets. Subsequently, the Mean Relative Gain metric offers an assessment of the overall performance of various models and instructions, while the Stability metric measures their sensitivity. Furthermore, previous research centers on evaluating models independently or solely assessing instructions, neglecting the adaptability between models and instructions. We propose the Adaptability metric to quantify the adaptability between models and instructions. Our paper evaluates a total of 20 language models (14 MLLMs) on 14 multimodal datasets spanning 6 tasks, with 10 instructions for each task, and derives novel insights. Our code will be released at https://github.com/declare-lab/MM-BigBench.\"), Document(metadata={'title': 'RoboLLM: Robotic Vision Tasks Grounded on Multimodal Large Language Models'}, page_content='Robotic vision applications often necessitate a wide range of visual perception tasks, such as object detection, segmentation, and identification. While there have been substantial advances in these individual tasks, integrating specialized models into a unified vision pipeline presents significant engineering challenges and costs. Recently, Multimodal Large Language Models (MLLMs) have emerged as novel backbones for various downstream tasks. We argue that leveraging the pre-training capabilities of MLLMs enables the creation of a simplified framework, thus mitigating the need for task-specific encoders. Specifically, the large-scale pretrained knowledge in MLLMs allows for easier fine-tuning to downstream robotic vision tasks and yields superior performance. We introduce the RoboLLM framework, equipped with a BEiT-3 backbone, to address all visual perception tasks in the ARMBench challenge-a large-scale robotic manipulation dataset about real-world warehouse scenarios. RoboLLM not only outperforms existing baselines but also substantially reduces the engineering burden associated with model selection and tuning. The source code is publicly available at https://github.com/longkukuhi/armbench.'), Document(metadata={'title': 'Benchmarking Sequential Visual Input Reasoning and Prediction in Multimodal Large Language Models'}, page_content=\"Multimodal large language models (MLLMs) have shown great potential in perception and interpretation tasks, but their capabilities in predictive reasoning remain under-explored. To address this gap, we introduce a novel benchmark that assesses the predictive reasoning capabilities of MLLMs across diverse scenarios. Our benchmark targets three important domains: abstract pattern reasoning, human activity prediction, and physical interaction prediction. We further develop three evaluation methods powered by large language model to robustly quantify a model's performance in predicting and reasoning the future based on multi-visual context. Empirical experiments confirm the soundness of the proposed benchmark and evaluation methods via rigorous testing and reveal pros and cons of current popular MLLMs in the task of predictive reasoning. Lastly, our proposed benchmark provides a standardized evaluation framework for MLLMs and can facilitate the development of more advanced models that can reason and predict over complex long sequence of multimodal input.\"), Document(metadata={'title': 'Woodpecker: Hallucination Correction for Multimodal Large Language Models'}, page_content='Hallucination is a big shadow hanging over the rapidly evolving Multimodal Large Language Models (MLLMs), referring to the phenomenon that the generated text is inconsistent with the image content. In order to mitigate hallucinations, existing studies mainly resort to an instruction-tuning manner that requires retraining the models with specific data. In this paper, we pave a different way, introducing a training-free method named Woodpecker. Like a woodpecker heals trees, it picks out and corrects hallucinations from the generated text. Concretely, Woodpecker consists of five stages: key concept extraction, question formulation, visual knowledge validation, visual claim generation, and hallucination correction. Implemented in a post-remedy manner, Woodpecker can easily serve different MLLMs, while being interpretable by accessing intermediate outputs of the five stages. We evaluate Woodpecker both quantitatively and qualitatively and show the huge potential of this new paradigm. On the POPE benchmark, our method obtains a 30.66%/24.33% improvement in accuracy over the baseline MiniGPT-4/mPLUG-Owl. The source code is released at https://github.com/BradyFU/Woodpecker.'), Document(metadata={'title': 'Towards Perceiving Small Visual Details in Zero-shot Visual Question Answering with Multimodal LLMs'}, page_content=\"Multimodal Large Language Models (MLLMs) have recently achieved promising zero-shot accuracy on visual question answering (VQA) -- a fundamental task affecting various downstream applications and domains. Given the great potential for the broad use of these models, it is important to investigate their limitations in dealing with different image and question properties. In this work, we investigate whether MLLMs can perceive small details as well as large details in images. In particular, we show that their zero-shot accuracy in answering visual questions is very sensitive to the size of the visual subject of the question, declining up to 46% with size. Furthermore, we show that this effect is causal by observing that human visual cropping can significantly mitigate their sensitivity to size. Inspired by the usefulness of human cropping, we then propose five automatic visual cropping methods -- leveraging either external localization models or the decision process of the given MLLM itself -- as inference time mechanisms to improve the zero-shot performance of MLLMs. We study their effectiveness on four popular VQA datasets, and a subset of the VQAv2 dataset tailored towards fine visual details. Our findings suggest that MLLMs should be used with caution in detail-sensitive VQA applications, and that visual cropping is a promising direction to improve their zero-shot performance. To facilitate further investigation of MLLMs' behaviors, our code and data are publicly released.\"), Document(metadata={'title': 'ChEF: A Comprehensive Evaluation Framework for Standardized Assessment of Multimodal Large Language Models'}, page_content=\"Multimodal Large Language Models (MLLMs) have shown impressive abilities in interacting with visual content with myriad potential downstream tasks. However, even though a list of benchmarks has been proposed, the capabilities and limitations of MLLMs are still not comprehensively understood, due to a lack of a standardized and holistic evaluation framework. To this end, we present the first Comprehensive Evaluation Framework (ChEF) that can holistically profile each MLLM and fairly compare different MLLMs. First, we structure ChEF as four modular components, i.e., Scenario as scalable multimodal datasets, Instruction as flexible instruction retrieving formulae, Inferencer as reliable question answering strategies, and Metric as indicative task-specific score functions. Based on them, ChEF facilitates versatile evaluations in a standardized framework, and new evaluations can be built by designing new Recipes (systematic selection of these four components). Notably, current MLLM benchmarks can be readily summarized as recipes of ChEF. Second, we introduce 6 new recipes to quantify competent MLLMs' desired capabilities (or called desiderata, i.e., calibration, in-context learning, instruction following, language performance, hallucination, and robustness) as reliable agents that can perform real-world multimodal interactions. Third, we conduct a large-scale evaluation of 9 prominent MLLMs on 9 scenarios and 6 desiderata. Our evaluation summarized over 20 valuable observations concerning the generalizability of MLLMs across various scenarios and the composite capability of MLLMs required for multimodal interactions. We will publicly release all the detailed implementations for further analysis, as well as an easy-to-use modular toolkit for the integration of new recipes and models, so that ChEF can be a growing evaluation framework for the MLLM community.\"), Document(metadata={'title': 'Octavius: Mitigating Task Interference in MLLMs via LoRA-MoE'}, page_content='Recent studies have demonstrated Large Language Models (LLMs) can extend their zero-shot generalization capabilities to multimodal learning through instruction tuning. As more modalities and downstream tasks are introduced, negative conflicts and interference may have a worse impact on performance. While this phenomenon has been overlooked in previous work, we propose a novel and extensible framework, called Octavius, for comprehensive studies and experimentation on multimodal learning with Multimodal Large Language Models (MLLMs). Specifically, we combine the well-known Mixture-of-Experts (MoE) and one of the representative PEFT techniques, i.e., LoRA, designing a novel LLM-based decoder, called LoRA-MoE, for multimodal learning. To the best of our knowledge, we are one of the pioneering efforts to introduce MoE into MLLMs to address this problem. The experimental results (about 20% improvement) have shown the effectiveness and versatility of our design in various 2D and 3D downstream tasks. Code and datasets are available at https://openlamm.github.io/tutorial/.'), Document(metadata={'title': 'What Large Language Models Bring to Text-rich VQA?'}, page_content=\"Text-rich VQA, namely Visual Question Answering based on text recognition in the images, is a cross-modal task that requires both image comprehension and text recognition. In this work, we focus on investigating the advantages and bottlenecks of LLM-based approaches in addressing this problem. To address the above concern, we separate the vision and language modules, where we leverage external OCR models to recognize texts in the image and Large Language Models (LLMs) to answer the question given texts. The whole framework is training-free benefiting from the in-context ability of LLMs. This pipeline achieved superior performance compared to the majority of existing Multimodal Large Language Models (MLLM) on four text-rich VQA datasets. Besides, based on the ablation study, we find that LLM brings stronger comprehension ability and may introduce helpful knowledge for the VQA problem. The bottleneck for LLM to address text-rich VQA problems may primarily lie in visual part. We also combine the OCR module with MLLMs and pleasantly find that the combination of OCR module with MLLM also works. It's worth noting that not all MLLMs can comprehend the OCR information, which provides insights into how to train an MLLM that preserves the abilities of LLM.\")], 'results': [Document(id='9e9ac81d-692f-441b-8926-4b6ba73ad784', metadata={'title': 'A Survey on Multimodal Large Language Models'}, page_content='Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.'), Document(id='128c9e1b-adb3-49e0-a873-881354331b12', metadata={'title': 'A Survey on Multimodal Large Language Models'}, page_content='Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.'), Document(id='01a7f21a-97a8-45a2-a73c-74526485470f', metadata={'title': 'A Survey on Multimodal Large Language Models'}, page_content='Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.')], 'context': '[1] A Survey on Multimodal Large Language Models\\nRecently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.\\n\\n[2] A Survey on Multimodal Large Language Models\\nRecently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.\\n\\n[3] A Survey on Multimodal Large Language Models\\nRecently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.', 'expert_opinion': 'Multimodal Large Language Models (MLLMs) are a rising research area, with models like GPT-4V using LLMs to perform multimodal tasks [1, 2, 3]. These models exhibit emergent capabilities like image-based story writing and OCR-free math reasoning, which are uncommon in traditional multimodal methods, suggesting a possible route to artificial general intelligence [1, 2, 3]. Research is focused on developing MLLMs that can compete with or outperform GPT-4V [1, 2, 3]. The key aspects of MLLMs under investigation include architecture, training strategies, data, and evaluation methods [1, 2, 3]. Research also explores expanding MLLMs to support more granularity, modalities, languages, and scenarios, as well as addressing multimodal hallucination and incorporating techniques like Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR) [1, 2, 3].', 'reviewer_opinion': \"The expert's response accurately summarizes the information provided in the context documents. All claims are supported by citations to the provided documents. The response directly addresses the user's query about multimodal LLMs, avoiding the information the user specified they were not interested in.\", 'messages': [AIMessage(content='Retrieving expert insights based on the papers and cite the papers...', additional_kwargs={}, response_metadata={}, id='22fe505c-90a0-4118-95eb-6045f2c066c7'), HumanMessage(content=\"Context: [1] A Survey on Multimodal Large Language Models\\nRecently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.\\n\\n[2] A Survey on Multimodal Large Language Models\\nRecently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.\\n\\n[3] A Survey on Multimodal Large Language Models\\nRecently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.\\n\\nQuery: latest advances in multimodal learning Consider User's response for the search query: I dont want to know about latest advancements, I want to know about multimodal llms\", additional_kwargs={}, response_metadata={}, id='c9f3c950-8322-4da3-a95b-1115429cd167'), AIMessage(content='Multimodal Large Language Models (MLLMs) are a rising research area, with models like GPT-4V using LLMs to perform multimodal tasks [1, 2, 3]. These models exhibit emergent capabilities like image-based story writing and OCR-free math reasoning, which are uncommon in traditional multimodal methods, suggesting a possible route to artificial general intelligence [1, 2, 3]. Research is focused on developing MLLMs that can compete with or outperform GPT-4V [1, 2, 3]. The key aspects of MLLMs under investigation include architecture, training strategies, data, and evaluation methods [1, 2, 3]. Research also explores expanding MLLMs to support more granularity, modalities, languages, and scenarios, as well as addressing multimodal hallucination and incorporating techniques like Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR) [1, 2, 3].', additional_kwargs={}, response_metadata={}, id='5d83e61e-b24a-4136-81d1-2c670290ed0f'), AIMessage(content=\"🧪 Reviewing the expert's findings for validity...\", additional_kwargs={}, response_metadata={}, id='53d69017-f1f4-47e6-bb32-dd68bf9bc464'), HumanMessage(content='Expert said:\\nMultimodal Large Language Models (MLLMs) are a rising research area, with models like GPT-4V using LLMs to perform multimodal tasks [1, 2, 3]. These models exhibit emergent capabilities like image-based story writing and OCR-free math reasoning, which are uncommon in traditional multimodal methods, suggesting a possible route to artificial general intelligence [1, 2, 3]. Research is focused on developing MLLMs that can compete with or outperform GPT-4V [1, 2, 3]. The key aspects of MLLMs under investigation include architecture, training strategies, data, and evaluation methods [1, 2, 3]. Research also explores expanding MLLMs to support more granularity, modalities, languages, and scenarios, as well as addressing multimodal hallucination and incorporating techniques like Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR) [1, 2, 3].', additional_kwargs={}, response_metadata={}, id='f5d7f20d-0d50-4ae5-98b5-9d750c4eaa60'), AIMessage(content=\"The expert's response accurately summarizes the information provided in the context documents. All claims are supported by citations to the provided documents. The response directly addresses the user's query about multimodal LLMs, avoiding the information the user specified they were not interested in.\", additional_kwargs={}, response_metadata={}, id='da43df89-5565-45f7-a7ca-2cadfe5fe841')], 'retry_count': 2}\n{'query': \"latest advances in multimodal learning Consider User's response for the search query: I dont want to know about latest advancements, I want to know about multimodal llms\", 'search_query': 'multimodal large language models', 'papers': [{'id': 'multimodal-large-language-models-for', 'arxiv_id': None, 'nips_id': None, 'url_abs': 'https://aclanthology.org/2022.naacl-srw.26', 'url_pdf': 'https://aclanthology.org/2022.naacl-srw.26.pdf', 'title': 'Multimodal large language models for inclusive collaboration learning tasks', 'abstract': 'This PhD project leverages advancements in multimodal large language models to build an inclusive collaboration feedback loop, in order to facilitate the automated detection, modeling, and feedback for participants developing general collaboration skills. This topic is important given the role of collaboration as an essential 21st century skill, the potential to ground large language models within learning theory and real-world practice, and the expressive potential of transformer models to support equity and inclusion. We address some concerns of integrating advances in natural language processing into downstream tasks such as the learning analytics feedback loop.', 'authors': ['Armanda Lewis'], 'published': None, 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': 'naacl-acl-2022-7'}, {'id': 'personality-aware-human-centric-multimodal', 'arxiv_id': '2304.02313', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2304.02313v2', 'url_pdf': 'https://arxiv.org/pdf/2304.02313v2.pdf', 'title': 'Personality-aware Human-centric Multimodal Reasoning: A New Task, Dataset and Baselines', 'abstract': \"Personality traits, emotions, and beliefs shape individuals' behavioral choices and decision-making processes. However, for one thing, the affective computing community normally focused on predicting personality traits but overlooks their application in behavior prediction. For another, the multimodal reasoning task emphasized the prediction of future states and behaviors but often neglected the incorporation of individual personality traits. In this work, we introduce a new task called Personality-aware Human-centric Multimodal Reasoning (PHMR) (T1), with the goal of forecasting the future behavior of a particular individual using multimodal information from past instances, while integrating personality factors. We accordingly construct a new dataset based on six television shows, encompassing 225 characters and 12k samples. To establish a benchmark for the task, we propose seven baseline methods: three adapted from related tasks, two pre-trained model, and two multimodal large language models. The experimental results demonstrate that incorporating personality traits enhances human-centric multimodal reasoning performance. To further solve the lack of personality annotation in real-life scenes, we introduce an extension task called Personality-predicted Human-centric Multimodal Reasoning task (T2) along with the corresponding dataset and method. We will make our dataset and code available on GitHub.\", 'authors': ['Rui Xia', 'Xiangqing Shen', 'Yaochen Zhu'], 'published': '2023-04-05', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'visual-transformation-telling', 'arxiv_id': '2305.01928', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2305.01928v2', 'url_pdf': 'https://arxiv.org/pdf/2305.01928v2.pdf', 'title': 'Visual Transformation Telling', 'abstract': 'Humans can naturally reason from superficial state differences (e.g. ground wetness) to transformations descriptions (e.g. raining) according to their life experience. In this paper, we propose a new visual reasoning task to test this transformation reasoning ability in real-world scenarios, called \\\\textbf{V}isual \\\\textbf{T}ransformation \\\\textbf{T}elling (VTT). Given a series of states (i.e. images), VTT requires to describe the transformation occurring between every two adjacent states. Different from existing visual reasoning tasks that focus on surface state reasoning, the advantage of VTT is that it captures the underlying causes, e.g. actions or events, behind the differences among states. We collect a novel dataset to support the study of transformation reasoning from two existing instructional video datasets, CrossTask and COIN, comprising 13,547 samples. Each sample involves the key state images along with their transformation descriptions. Our dataset covers diverse real-world activities, providing a rich resource for training and evaluation. To construct an initial benchmark for VTT, we test several models, including traditional visual storytelling methods (CST, GLACNet, Densecap) and advanced multimodal large language models (LLaVA v1.5-7B, Qwen-VL-chat, Gemini Pro Vision, GPT-4o, and GPT-4). Experimental results reveal that even state-of-the-art models still face challenges in VTT, highlighting substantial areas for improvement.', 'authors': ['Liang Pang', 'Xin Hong', 'Wanqing Cui', 'Xueqi Cheng', 'Jiafeng Guo', 'Yanyan Lan'], 'published': '2023-05-03', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'listen-think-and-understand', 'arxiv_id': '2305.10790', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2305.10790v3', 'url_pdf': 'https://arxiv.org/pdf/2305.10790v3.pdf', 'title': 'Listen, Think, and Understand', 'abstract': 'The ability of artificial intelligence (AI) systems to perceive and comprehend audio signals is crucial for many applications. Although significant progress has been made in this area since the development of AudioSet, most existing models are designed to map audio inputs to pre-defined, discrete sound label sets. In contrast, humans possess the ability to not only classify sounds into general categories, but also to listen to the finer details of the sounds, explain the reason for the predictions, think about what the sound infers, and understand the scene and what action needs to be taken, if any. Such capabilities beyond perception are not yet present in existing audio models. On the other hand, modern large language models (LLMs) exhibit emerging reasoning ability but they lack audio perception capabilities. Therefore, we ask the question: can we build a model that has both audio perception and a reasoning ability? In this paper, we propose a new audio foundation model, called LTU (Listen, Think, and Understand). To train LTU, we created a new OpenAQA-5M dataset consisting of 1.9 million closed-ended and 3.7 million open-ended, diverse (audio, question, answer) tuples, and have used an autoregressive training framework with a perception-to-understanding curriculum. LTU demonstrates strong performance and generalization ability on conventional audio tasks such as classification and captioning. More importantly, it exhibits emerging audio reasoning and comprehension abilities that are absent in existing audio models. To the best of our knowledge, LTU is one of the first multimodal large language models that focus on general audio (rather than just speech) understanding.', 'authors': ['James Glass', 'Leonid Karlinsky', 'Alexander H. Liu', 'Hongyin Luo', 'Yuan Gong'], 'published': '2023-05-18', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'what-makes-for-good-visual-tokenizers-for', 'arxiv_id': '2305.12223', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2305.12223v2', 'url_pdf': 'https://arxiv.org/pdf/2305.12223v2.pdf', 'title': 'What Makes for Good Visual Tokenizers for Large Language Models?', 'abstract': 'We empirically investigate proper pre-training methods to build good visual tokenizers, making Large Language Models (LLMs) powerful Multimodal Large Language Models (MLLMs). In our benchmark, which is curated to evaluate MLLMs visual semantic understanding and fine-grained perception capabilities, we discussed different visual tokenizers pre-trained with dominant methods (i.e., DeiT, CLIP, MAE, DINO), and observe that: i) Fully/weakly supervised models capture more semantics than self-supervised models, but the gap is narrowed by scaling up the pre-training dataset. ii) Self-supervised models are better at fine-grained perception, where patch-level supervision is particularly effective. iii) Tuning the visual tokenizer leads to the loss of semantics obtained from large-scale pretraining, which is unfavorable with relatively small-scale instruction-tuning dataset. Given the findings, we reviewed methods that attempted to unify semantics and fine-grained visual understanding, e.g., patch-level feature distillation with semantically-rich targets. We obtain an intriguing insight mask-based strategies that were once all the rage may not be applicable for obtaining good visual tokenizers. Based on this critical observation, we obtain a new MLLM equipped with a tailored Good Visual Tokenizer (GVT), which exhibits strong visual comprehension capability at multiple scales. In particular, without introducing extra parameters and task-specific fine-tuning, GVT achieves superior performance on visual question answering, image captioning, and other fine-grained visual understanding tasks such as object counting and multi-class identification.', 'authors': ['Ying Shan', 'Mohan Kankanhalli', 'Xiaohan Ding', 'Yixiao Ge', 'Guangzhi Wang'], 'published': '2023-05-20', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'interpretable-by-design-visual-question', 'arxiv_id': '2305.14882', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2305.14882v2', 'url_pdf': 'https://arxiv.org/pdf/2305.14882v2.pdf', 'title': 'Dynamic Clue Bottlenecks: Towards Interpretable-by-Design Visual Question Answering', 'abstract': 'Recent advances in multimodal large language models (LLMs) have shown extreme effectiveness in visual question answering (VQA). However, the design nature of these end-to-end models prevents them from being interpretable to humans, undermining trust and applicability in critical domains. While post-hoc rationales offer certain insight into understanding model behavior, these explanations are not guaranteed to be faithful to the model. In this paper, we address these shortcomings by introducing an interpretable by design model that factors model decisions into intermediate human-legible explanations, and allows people to easily understand why a model fails or succeeds. We propose the Dynamic Clue Bottleneck Model ( (DCLUB), a method that is designed towards an inherently interpretable VQA system. DCLUB provides an explainable intermediate space before the VQA decision and is faithful from the beginning, while maintaining comparable performance to black-box systems. Given a question, DCLUB first returns a set of visual clues: natural language statements of visually salient evidence from the image, and then generates the output based solely on the visual clues. To supervise and evaluate the generation of VQA explanations within DCLUB, we collect a dataset of 1.7k reasoning-focused questions with visual clues. Evaluations show that our inherently interpretable system can improve 4.64% over a comparable black-box system in reasoning-focused questions while preserving 99.43% of performance on VQA-v2.', 'authors': ['Dan Roth', 'Mark Yatskar', 'Sihao Chen', 'Ben Zhou', 'Xingyu Fu'], 'published': '2023-05-24', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'pathasst-redefining-pathology-through', 'arxiv_id': '2305.15072', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2305.15072v2', 'url_pdf': 'https://arxiv.org/pdf/2305.15072v2.pdf', 'title': 'PathAsst: A Generative Foundation AI Assistant Towards Artificial General Intelligence of Pathology', 'abstract': \"As advances in large language models (LLMs) and multimodal techniques continue to mature, the development of general-purpose multimodal large language models (MLLMs) has surged, offering significant applications in interpreting natural images. However, the field of pathology has largely remained untapped, particularly in gathering high-quality data and designing comprehensive model frameworks. To bridge the gap in pathology MLLMs, we present PathAsst, a multimodal generative foundation AI assistant to revolutionize diagnostic and predictive analytics in pathology. The development of PathAsst involves three pivotal steps: data acquisition, CLIP model adaptation, and the training of PathAsst's multimodal generative capabilities. Firstly, we collect over 207K high-quality pathology image-text pairs from authoritative sources. Leveraging the advanced power of ChatGPT, we generate over 180K instruction-following samples. Furthermore, we devise additional instruction-following data specifically tailored for invoking eight pathology-specific sub-models we prepared, allowing the PathAsst to effectively collaborate with these models, enhancing its diagnostic ability. Secondly, by leveraging the collected data, we construct PathCLIP, a pathology-dedicated CLIP, to enhance PathAsst's capabilities in interpreting pathology images. Finally, we integrate PathCLIP with the Vicuna-13b and utilize pathology-specific instruction-tuning data to enhance the multimodal generation capacity of PathAsst and bolster its synergistic interactions with sub-models. The experimental results of PathAsst show the potential of harnessing AI-powered generative foundation model to improve pathology diagnosis and treatment processes.\", 'authors': ['Yunlong Zhang', 'Zhongyi Shui', 'Lin Sun', 'Lin Yang', 'Honglin Li', 'Kai Zhang', 'Sunyi Zheng', 'Chenglu Zhu', 'Yuxuan Sun'], 'published': '2023-05-24', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'contextual-object-detection-with-multimodal', 'arxiv_id': '2305.18279', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2305.18279v2', 'url_pdf': 'https://arxiv.org/pdf/2305.18279v2.pdf', 'title': 'Contextual Object Detection with Multimodal Large Language Models', 'abstract': 'Recent Multimodal Large Language Models (MLLMs) are remarkable in vision-language tasks, such as image captioning and question answering, but lack the essential perception ability, i.e., object detection. In this work, we address this limitation by introducing a novel research problem of contextual object detection -- understanding visible objects within different human-AI interactive contexts. Three representative scenarios are investigated, including the language cloze test, visual captioning, and question answering. Moreover, we present ContextDET, a unified multimodal model that is capable of end-to-end differentiable modeling of visual-language contexts, so as to locate, identify, and associate visual objects with language inputs for human-AI interaction. Our ContextDET involves three key submodels: (i) a visual encoder for extracting visual representations, (ii) a pre-trained LLM for multimodal context decoding, and (iii) a visual decoder for predicting bounding boxes given contextual object words. The new generate-then-detect framework enables us to detect object words within human vocabulary. Extensive experiments show the advantages of ContextDET on our proposed CODE benchmark, open-vocabulary detection, and referring image segmentation. Github: https://github.com/yuhangzang/ContextDET.', 'authors': ['Chen Change Loy', 'Kaiyang Zhou', 'Jun Han', 'Wei Li', 'Yuhang Zang'], 'published': '2023-05-29', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'a-survey-on-multimodal-large-language-models', 'arxiv_id': '2306.13549', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2306.13549v4', 'url_pdf': 'https://arxiv.org/pdf/2306.13549v4.pdf', 'title': 'A Survey on Multimodal Large Language Models', 'abstract': 'Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.', 'authors': ['Enhong Chen', 'Tong Xu', 'Xing Sun', 'Ke Li', 'Sirui Zhao', 'Chaoyou Fu', 'Shukang Yin'], 'published': '2023-06-23', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'mme-a-comprehensive-evaluation-benchmark-for', 'arxiv_id': '2306.13394', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2306.13394v4', 'url_pdf': 'https://arxiv.org/pdf/2306.13394v4.pdf', 'title': 'MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models', 'abstract': 'Multimodal Large Language Model (MLLM) relies on the powerful LLM to perform multimodal tasks, showing amazing emergent abilities in recent studies, such as writing poems based on an image. However, it is difficult for these case studies to fully reflect the performance of MLLM, lacking a comprehensive evaluation. In this paper, we fill in this blank, presenting the first comprehensive MLLM Evaluation benchmark MME. It measures both perception and cognition abilities on a total of 14 subtasks. In order to avoid data leakage that may arise from direct use of public datasets for evaluation, the annotations of instruction-answer pairs are all manually designed. The concise instruction design allows us to fairly compare MLLMs, instead of struggling in prompt engineering. Besides, with such an instruction, we can also easily carry out quantitative statistics. A total of 30 advanced MLLMs are comprehensively evaluated on our MME, which not only suggests that existing MLLMs still have a large room for improvement, but also reveals the potential directions for the subsequent model optimization. The data application manner and online leaderboards are released at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation.', 'authors': ['Rongrong Ji', 'Yunsheng Wu', 'Jinrui Yang', 'Xing Sun', 'Ke Li', 'Xiawu Zheng', 'Xu Lin', 'Mengdan Zhang', 'Yulei Qin', 'Yunhang Shen', 'Peixian Chen', 'Chaoyou Fu'], 'published': '2023-06-23', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'kosmos-2-grounding-multimodal-large-language', 'arxiv_id': '2306.14824', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2306.14824v3', 'url_pdf': 'https://arxiv.org/pdf/2306.14824v3.pdf', 'title': 'Kosmos-2: Grounding Multimodal Large Language Models to the World', 'abstract': \"We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. Specifically, we represent refer expressions as links in Markdown, i.e., ``[text span](bounding boxes)'', where object descriptions are sequences of location tokens. Together with multimodal corpora, we construct large-scale data of grounded image-text pairs (called GrIT) to train the model. In addition to the existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), Kosmos-2 integrates the grounding capability into downstream applications. We evaluate Kosmos-2 on a wide range of tasks, including (i) multimodal grounding, such as referring expression comprehension, and phrase grounding, (ii) multimodal referring, such as referring expression generation, (iii) perception-language tasks, and (iv) language understanding and generation. This work lays out the foundation for the development of Embodiment AI and sheds light on the big convergence of language, multimodal perception, action, and world modeling, which is a key step toward artificial general intelligence. Code and pretrained models are available at https://aka.ms/kosmos-2.\", 'authors': ['Furu Wei', 'Shuming Ma', 'Shaohan Huang', 'Yaru Hao', 'Li Dong', 'Wenhui Wang', 'Zhiliang Peng'], 'published': '2023-06-26', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'shikra-unleashing-multimodal-llm-s', 'arxiv_id': '2306.15195', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2306.15195v2', 'url_pdf': 'https://arxiv.org/pdf/2306.15195v2.pdf', 'title': \"Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic\", 'abstract': \"In human conversations, individuals can indicate relevant regions within a scene while addressing others. In turn, the other person can then respond by referring to specific regions if necessary. This natural referential ability in dialogue remains absent in current Multimodal Large Language Models (MLLMs). To fill this gap, this paper proposes an MLLM called Shikra, which can handle spatial coordinate inputs and outputs in natural language. Its architecture consists of a vision encoder, an alignment layer, and a LLM. It is designed to be straightforward and simple, without the need for extra vocabularies, position encoder, pre-/post-detection modules, or external plug-in models. All inputs and outputs are in natural language form. Referential dialogue is a superset of various vision-language (VL) tasks. Shikra can naturally handle location-related tasks like REC and PointQA, as well as conventional VL tasks such as Image Captioning and VQA. Experimental results showcase Shikra's promising performance. Furthermore, it enables numerous exciting applications, like providing mentioned objects' coordinates in chains of thoughts and comparing user-pointed regions similarities. Our code, model and dataset are accessed at https://github.com/shikras/shikra.\", 'authors': ['Rui Zhao', 'Feng Zhu', 'Richong Zhang', 'Weili Zeng', 'Zhao Zhang', 'Keqin Chen'], 'published': '2023-06-27', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'svit-scaling-up-visual-instruction-tuning', 'arxiv_id': '2307.04087', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2307.04087v3', 'url_pdf': 'https://arxiv.org/pdf/2307.04087v3.pdf', 'title': 'SVIT: Scaling up Visual Instruction Tuning', 'abstract': \"Thanks to the emerging of foundation models, the large language and vision models are integrated to acquire the multimodal ability of visual captioning, question answering, etc. Although existing multimodal models present impressive performance of visual understanding and reasoning, their limits are still largely under-explored due to the scarcity of high-quality instruction tuning data. To push the limits of multimodal capability, we Scale up Visual Instruction Tuning (SVIT) by constructing a dataset of 4.2 million visual instruction tuning data including 1.6M conversation question-answer (QA) pairs, 1.6M complex reasoning QA pairs, 1.0M referring QA pairs and 106K detailed image descriptions. Besides the volume, the proposed dataset is also featured by the high quality and rich diversity, which is generated by prompting GPT-4 with the abundant manual annotations of images. We also propose a new data recipe to select subset with better diversity and balance, which evokes model's superior capabilities. Extensive experiments verify that SVIT-v1.5, trained on the proposed dataset, outperforms state-of-the-art Multimodal Large Language Models on popular benchmarks. The data and code are publicly available at https://github.com/BAAI-DCAI/Visual-Instruction-Tuning.\", 'authors': ['Tiejun Huang', 'Muyang He', 'Boya Wu', 'Bo Zhao'], 'published': '2023-07-09', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'chatspot-bootstrapping-multimodal-llms-via', 'arxiv_id': '2307.09474', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2307.09474v1', 'url_pdf': 'https://arxiv.org/pdf/2307.09474v1.pdf', 'title': 'ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning', 'abstract': \"Human-AI interactivity is a critical aspect that reflects the usability of multimodal large language models (MLLMs). However, existing end-to-end MLLMs only allow users to interact with them through language instructions, leading to the limitation of the interactive accuracy and efficiency. In this study, we present precise referring instructions that utilize diverse reference representations such as points and boxes as referring prompts to refer to the special region. This enables MLLMs to focus on the region of interest and achieve finer-grained interaction. Based on precise referring instruction, we propose ChatSpot, a unified end-to-end multimodal large language model that supports diverse forms of interactivity including mouse clicks, drag-and-drop, and drawing boxes, which provides a more flexible and seamless interactive experience. We also construct a multi-grained vision-language instruction-following dataset based on existing datasets and GPT-4 generating. Furthermore, we design a series of evaluation tasks to assess the effectiveness of region recognition and interaction. Experimental results showcase ChatSpot's promising performance.\", 'authors': ['Xiangyu Zhang', 'Chunrui Han', 'Runpei Dong', 'Yuang Peng', 'Jianjian Sun', 'HongYu Zhou', 'Haoran Wei', 'Jinrong Yang', 'Zheng Ge', 'En Yu', 'Liang Zhao'], 'published': '2023-07-18', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'seed-bench-benchmarking-multimodal-llms-with', 'arxiv_id': '2307.16125', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2307.16125v2', 'url_pdf': 'https://arxiv.org/pdf/2307.16125v2.pdf', 'title': 'SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension', 'abstract': 'Based on powerful Large Language Models (LLMs), recent generative Multimodal Large Language Models (MLLMs) have gained prominence as a pivotal research area, exhibiting remarkable capability for both comprehension and generation. In this work, we address the evaluation of generative comprehension in MLLMs as a preliminary step towards a comprehensive assessment of generative models, by introducing a benchmark named SEED-Bench. SEED-Bench consists of 19K multiple choice questions with accurate human annotations (x 6 larger than existing benchmarks), which spans 12 evaluation dimensions including the comprehension of both the image and video modality. We develop an advanced pipeline for generating multiple-choice questions that target specific evaluation dimensions, integrating both automatic filtering and manual verification processes. Multiple-choice questions with groundtruth options derived from human annotation enables an objective and efficient assessment of model performance, eliminating the need for human or GPT intervention during evaluation. We further evaluate the performance of 18 models across all 12 dimensions, covering both the spatial and temporal understanding. By revealing the limitations of existing MLLMs through evaluation results, we aim for SEED-Bench to provide insights for motivating future research. We will launch and consistently maintain a leaderboard to provide a platform for the community to assess and investigate model capability.', 'authors': ['Ying Shan', 'Yixiao Ge', 'Yuying Ge', 'Guangzhi Wang', 'Rui Wang', 'Bohao Li'], 'published': '2023-07-30', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'lisa-reasoning-segmentation-via-large', 'arxiv_id': '2308.00692', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.00692v3', 'url_pdf': 'https://arxiv.org/pdf/2308.00692v3.pdf', 'title': 'LISA: Reasoning Segmentation via Large Language Model', 'abstract': 'Although perception systems have made remarkable advancements in recent years, they still rely on explicit human instruction or pre-defined categories to identify the target objects before executing visual recognition tasks. Such systems cannot actively reason and comprehend implicit user intention. In this work, we propose a new segmentation task -- reasoning segmentation. The task is designed to output a segmentation mask given a complex and implicit query text. Furthermore, we establish a benchmark comprising over one thousand image-instruction-mask data samples, incorporating intricate reasoning and world knowledge for evaluation purposes. Finally, we present LISA: large Language Instructed Segmentation Assistant, which inherits the language generation capabilities of multimodal Large Language Models (LLMs) while also possessing the ability to produce segmentation masks. We expand the original vocabulary with a <SEG> token and propose the embedding-as-mask paradigm to unlock the segmentation capability. Remarkably, LISA can handle cases involving complex reasoning and world knowledge. Also, it demonstrates robust zero-shot capability when trained exclusively on reasoning-free datasets. In addition, fine-tuning the model with merely 239 reasoning segmentation data samples results in further performance enhancement. Both quantitative and qualitative experiments show our method effectively unlocks new reasoning segmentation capabilities for multimodal LLMs. Code, models, and data are available at https://github.com/dvlab-research/LISA.', 'authors': ['Jiaya Jia', 'Shu Liu', 'Yuhui Yuan', 'Yanwei Li', 'Yukang Chen', 'Zhuotao Tian', 'Xin Lai'], 'published': '2023-08-01', 'conference': None, 'conference_url_abs': 'http://openaccess.thecvf.com//content/CVPR2024/html/Lai_LISA_Reasoning_Segmentation_via_Large_Language_Model_CVPR_2024_paper.html', 'conference_url_pdf': 'http://openaccess.thecvf.com//content/CVPR2024/papers/Lai_LISA_Reasoning_Segmentation_via_Large_Language_Model_CVPR_2024_paper.pdf', 'proceeding': 'cvpr-2024-1'}, {'id': 'empowering-vision-language-models-to-follow', 'arxiv_id': '2308.04152', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.04152v4', 'url_pdf': 'https://arxiv.org/pdf/2308.04152v4.pdf', 'title': 'Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions', 'abstract': \"Recent advancements in Multimodal Large Language Models (MLLMs) have been utilizing Visual Prompt Generators (VPGs) to convert visual features into tokens that LLMs can recognize. This is achieved by training the VPGs on millions of image-caption pairs, where the VPG-generated tokens of images are fed into a frozen LLM to generate the corresponding captions. However, this image-captioning based training objective inherently biases the VPG to concentrate solely on the primary visual contents sufficient for caption generation, often neglecting other visual details. This shortcoming results in MLLMs' underperformance in comprehending demonstrative instructions consisting of multiple, interleaved, and multimodal instructions that demonstrate the required context to complete a task. To address this issue, we introduce a generic and lightweight Visual Prompt Generator Complete module (VPG-C), which can infer and complete the missing details essential for comprehending demonstrative instructions. Further, we propose a synthetic discriminative training strategy to fine-tune VPG-C, eliminating the need for supervised demonstrative instructions. As for evaluation, we build DEMON, a comprehensive benchmark for demonstrative instruction understanding. Synthetically trained with the proposed strategy, VPG-C achieves significantly stronger zero-shot performance across all tasks of DEMON. Further evaluation on the MME and OwlEval benchmarks also demonstrate the superiority of VPG-C. Our benchmark, code, and pre-trained models are available at https://github.com/DCDmllm/Cheetah.\", 'authors': ['Wenqiao Zhang', 'Wei Ji', 'Yueting Zhuang', 'Siliang Tang', 'Tat-Seng Chua', 'Hanwang Zhang', 'Minghe Gao', 'Zhiqi Ge', 'Kaihang Pan', 'Juncheng Li'], 'published': '2023-08-08', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'link-context-learning-for-multimodal-llms', 'arxiv_id': '2308.07891', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.07891v1', 'url_pdf': 'https://arxiv.org/pdf/2308.07891v1.pdf', 'title': 'Link-Context Learning for Multimodal LLMs', 'abstract': 'The ability to learn from context with novel concepts, and deliver appropriate responses are essential in human conversations. Despite current Multimodal Large Language Models (MLLMs) and Large Language Models (LLMs) being trained on mega-scale datasets, recognizing unseen images or understanding novel concepts in a training-free manner remains a challenge. In-Context Learning (ICL) explores training-free few-shot learning, where models are encouraged to ``learn to learn\" from limited tasks and generalize to unseen tasks. In this work, we propose link-context learning (LCL), which emphasizes \"reasoning from cause and effect\" to augment the learning capabilities of MLLMs. LCL goes beyond traditional ICL by explicitly strengthening the causal relationship between the support set and the query set. By providing demonstrations with causal links, LCL guides the model to discern not only the analogy but also the underlying causal associations between data points, which empowers MLLMs to recognize unseen images and understand novel concepts more effectively. To facilitate the evaluation of this novel approach, we introduce the ISEKAI dataset, comprising exclusively of unseen generated image-label pairs designed for link-context learning. Extensive experiments show that our LCL-MLLM exhibits strong link-context learning capabilities to novel concepts over vanilla MLLMs. Code and data will be released at https://github.com/isekai-portal/Link-Context-Learning.', 'authors': ['Ziwei Liu', 'Rui Zhao', 'Feng Zhu', 'Zhao Zhang', 'Weichen Fan', 'Yan Tai'], 'published': '2023-08-15', 'conference': None, 'conference_url_abs': 'http://openaccess.thecvf.com//content/CVPR2024/html/Tai_Link-Context_Learning_for_Multimodal_LLMs_CVPR_2024_paper.html', 'conference_url_pdf': 'http://openaccess.thecvf.com//content/CVPR2024/papers/Tai_Link-Context_Learning_for_Multimodal_LLMs_CVPR_2024_paper.pdf', 'proceeding': 'cvpr-2024-1'}, {'id': 'fashionlogo-prompting-multimodal-large', 'arxiv_id': '2308.09012', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.09012v2', 'url_pdf': 'https://arxiv.org/pdf/2308.09012v2.pdf', 'title': 'FashionLOGO: Prompting Multimodal Large Language Models for Fashion Logo Embeddings', 'abstract': \"Logo embedding models convert the product logos in images into vectors, enabling their utilization for logo recognition and detection within e-commerce platforms. This facilitates the enforcement of intellectual property rights and enhances product search capabilities. However, current methods treat logo embedding as a purely visual problem. A noteworthy issue is that visual models capture features more than logos. Instead, we view this as a multimodal task, using text as auxiliary information to facilitate the visual model's understanding of the logo. The emerging Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in both visual and textual understanding. Inspired by this, we propose an approach, \\\\textbf{FashionLOGO}, to explore how to prompt MLLMs to generate appropriate text for product images, which can help visual models achieve better logo embeddings. We adopt a cross-attention transformer block that enables visual embedding to automatically learn supplementary knowledge from textual embedding. Our extensive experiments on real-world datasets prove that FashionLOGO is capable of generating generic and robust logo embeddings, achieving state-of-the-art performance in all benchmarks.\", 'authors': ['Walton Wang', 'Minghui Qiu', 'Min Yang', 'Yulin Su', 'Da Li', 'Zhen Wang'], 'published': '2023-08-17', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'wanjuan-a-comprehensive-multimodal-dataset', 'arxiv_id': '2308.10755', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.10755v3', 'url_pdf': 'https://arxiv.org/pdf/2308.10755v3.pdf', 'title': 'WanJuan: A Comprehensive Multimodal Dataset for Advancing English and Chinese Large Models', 'abstract': 'The rise in popularity of ChatGPT and GPT-4 has significantly accelerated the development of large models, leading to the creation of numerous impressive large language models(LLMs) and multimodal large language models (MLLMs). These cutting-edge models owe their remarkable performance to high-quality data. However, the details of the training data used in leading paradigms are often kept confidential. This lack of transparency, coupled with the scarcity of open-source data, impedes further developments within the community. As a response, this paper presents \"Wan Juan\", a large-scale multimodal dataset composed of both Chinese and English data, collected from a wide range of web sources. The dataset incorporates text, image-text, and video modalities, with a total volume exceeding 2TB. It was utilized in the training of InternLM, a model that demonstrated significant advantages in multi-dimensional evaluations when compared to models of a similar scale. All data can be accessed at https://opendatalab.org.cn/WanJuan1.0.', 'authors': ['Dahua Lin', 'Jiaqi Wang', 'Hang Yan', 'Wei Li', 'Bin Wang', 'Jiantao Qiu', 'Chao Xu', 'Zhenjiang Jin', 'Conghui He'], 'published': '2023-08-21', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'stablellava-enhanced-visual-instruction', 'arxiv_id': '2308.10253', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.10253v2', 'url_pdf': 'https://arxiv.org/pdf/2308.10253v2.pdf', 'title': 'StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data', 'abstract': \"The remarkable multimodal capabilities demonstrated by OpenAI's GPT-4 have sparked significant interest in the development of multimodal Large Language Models (LLMs). A primary research objective of such models is to align visual and textual modalities effectively while comprehending human instructions. Current methodologies often rely on annotations derived from benchmark datasets to construct image-dialogue datasets for training purposes, akin to instruction tuning in LLMs. However, these datasets often exhibit domain bias, potentially constraining the generative capabilities of the models. In an effort to mitigate these limitations, we propose a novel data collection methodology that synchronously synthesizes images and dialogues for visual instruction tuning. This approach harnesses the power of generative models, marrying the abilities of ChatGPT and text-to-image generative models to yield a diverse and controllable dataset with varied image content. Additionally, datasets can be arbitrarily scaled. This not only provides greater flexibility compared to existing methodologies but also significantly enhances several model capabilities. Our research includes comprehensive experiments conducted on various datasets. The results emphasize substantial enhancements in more than ten commonly assessed capabilities. Additionally, our model achieves state-of-the-art results across multiple widely recognized multimodal benchmarks.\", 'authors': ['Yunchao Wei', 'Ling Chen', 'Chunhua Shen', 'Guosheng Lin', 'Bin Fu', 'Zhibin Wang', 'Gang Yu', 'Chi Zhang', 'Yanda Li'], 'published': '2023-08-20', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'instructiongpt-4-a-200-instruction-paradigm', 'arxiv_id': '2308.12067', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.12067v2', 'url_pdf': 'https://arxiv.org/pdf/2308.12067v2.pdf', 'title': 'InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4', 'abstract': 'Multimodal large language models are typically trained in two stages: first pre-training on image-text pairs, and then fine-tuning using supervised vision-language instruction data. Recent studies have shown that large language models can achieve satisfactory results even with a limited amount of high-quality instruction-following data. In this paper, we introduce InstructionGPT-4, which is fine-tuned on a small dataset comprising only 200 examples, amounting to approximately 6\\\\% of the instruction-following data used in the alignment dataset for MiniGPT-4. To achieve this, we first propose several metrics to access the quality of multimodal instruction data. Based on these metrics, we present an effective and trainable data selector to automatically identify and filter low-quality vision-language data. By employing this method, InstructionGPT-4 outperforms the original MiniGPT-4 on various evaluations. Overall, our findings demonstrate that less but high-quality instruction tuning data is efficient in enabling multimodal large language models to generate better output. Our code is available at https://github.com/waltonfuture/InstructionGPT-4.', 'authors': ['Lichao Sun', 'Weiran Huang', 'Zihao Jiang', 'Lai Wei'], 'published': '2023-08-23', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'vigc-visual-instruction-generation-and', 'arxiv_id': '2308.12714', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.12714v3', 'url_pdf': 'https://arxiv.org/pdf/2308.12714v3.pdf', 'title': 'VIGC: Visual Instruction Generation and Correction', 'abstract': \"The integration of visual encoders and large language models (LLMs) has driven recent progress in multimodal large language models (MLLMs). However, the scarcity of high-quality instruction-tuning data for vision-language tasks remains a challenge. The current leading paradigm, such as LLaVA, relies on language-only GPT-4 to generate data, which requires pre-annotated image captions and detection bounding boxes, suffering from understanding image details. A practical solution to this problem would be to utilize the available multimodal large language models (MLLMs) to generate instruction data for vision-language tasks. However, it's worth noting that the currently accessible MLLMs are not as powerful as their LLM counterparts, as they tend to produce inadequate responses and generate false information. As a solution for addressing the current issue, this paper proposes the Visual Instruction Generation and Correction (VIGC) framework that enables multimodal large language models to generate instruction-tuning data and progressively enhance its quality on-the-fly. Specifically, Visual Instruction Generation (VIG) guides the vision-language model to generate diverse instruction-tuning data. To ensure generation quality, Visual Instruction Correction (VIC) adopts an iterative update mechanism to correct any inaccuracies in data produced by VIG, effectively reducing the risk of hallucination. Leveraging the diverse, high-quality data generated by VIGC, we finetune mainstream models and validate data quality based on various evaluations. Experimental results demonstrate that VIGC not only compensates for the shortcomings of language-only data generation methods, but also effectively enhances the benchmark performance. The models, datasets, and code are available at https://opendatalab.github.io/VIGC.\", 'authors': ['Conghui He', 'Jiaqi Wang', 'Wei Li', 'Weijia Li', 'Xiaoyi Dong', 'Pan Zhang', 'Huaping Zhong', 'Jiahui Peng', 'Xiao Han', 'Fan Wu', 'Bin Wang'], 'published': '2023-08-24', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'position-enhanced-visual-instruction-tuning', 'arxiv_id': '2308.13437', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.13437v2', 'url_pdf': 'https://arxiv.org/pdf/2308.13437v2.pdf', 'title': 'Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models', 'abstract': 'Recently, Multimodal Large Language Models (MLLMs) that enable Large Language Models (LLMs) to interpret images through visual instruction tuning have achieved significant success. However, existing visual instruction tuning methods only utilize image-language instruction data to align the language and image modalities, lacking a more fine-grained cross-modal alignment. In this paper, we propose Position-enhanced Visual Instruction Tuning (PVIT), which extends the functionality of MLLMs by integrating an additional region-level vision encoder. This integration promotes a more detailed comprehension of images for the MLLM. In addition, to efficiently achieve a fine-grained alignment between the vision modules and the LLM, we design multiple data generation strategies to construct an image-region-language instruction dataset. Finally, we present both quantitative experiments and qualitative analysis that demonstrate the superiority of the proposed model. Code and data will be released at https://github.com/PVIT-official/PVIT.', 'authors': ['Yang Liu', 'Maosong Sun', 'Peng Li', 'Xiaoyue Mi', 'Fuwen Luo', 'Ruoyu Qin', 'Chi Chen'], 'published': '2023-08-25', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'mllm-dataengine-an-iterative-refinement', 'arxiv_id': '2308.13566', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.13566v2', 'url_pdf': 'https://arxiv.org/pdf/2308.13566v2.pdf', 'title': 'MLLM-DataEngine: An Iterative Refinement Approach for MLLM', 'abstract': 'Despite the great advance of Multimodal Large Language Models (MLLMs) in both instruction dataset building and benchmarking, the independence of training and evaluation makes current MLLMs hard to further improve their capability under the guidance of evaluation results with a relatively low human cost. In this paper, we propose MLLM-DataEngine, a novel closed-loop system that bridges data generation, model training, and evaluation. Within each loop iteration, the MLLM-DataEngine first analyze the weakness of the model based on the evaluation results, then generate a proper incremental dataset for the next training iteration and enhance the model capability iteratively. Compared with previous data collection methods which are separate from the benchmarking, the data generated by MLLM-DataEngine shows better targeting, quality, and correctness. For targeting, we propose an Adaptive Bad-case Sampling module, which adjusts the ratio of different types of data within each incremental dataset based on the benchmarking results. For quality, we resort to GPT-4 to generate high-quality data with each given data type. For correctness, prompt design is critical for the data generation results. Rather than previous hand-crafted prompt, we propose an Interactive Prompt Optimization strategy, which optimizes the prompt with the multi-round interaction between human and GPT, and improve the correctness of generated data greatly. Through extensive experiments, we find our MLLM-DataEngine could boost the MLLM capability in a targeted and automatic manner, with only a few human participation. We hope it could be a general solution for the following MLLMs building. The MLLM-DataEngine has been open-sourced and is now available at https://github.com/opendatalab/MLLM-DataEngine.', 'authors': ['Conghui He', 'Jiaqi Wang', 'Xiaoyi Dong', 'Pan Zhang', 'Siyuan Huang', 'Bin Wang', 'Linke Ouyang', 'Zhiyuan Zhao'], 'published': '2023-08-25', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'socratis-are-large-multimodal-models', 'arxiv_id': '2308.16741', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.16741v3', 'url_pdf': 'https://arxiv.org/pdf/2308.16741v3.pdf', 'title': 'Socratis: Are large multimodal models emotionally aware?', 'abstract': 'Existing emotion prediction benchmarks contain coarse emotion labels which do not consider the diversity of emotions that an image and text can elicit in humans due to various reasons. Learning diverse reactions to multimodal content is important as intelligent machines take a central role in generating and delivering content to society. To address this gap, we propose Socratis, a societal reactions benchmark, where each image-caption (IC) pair is annotated with multiple emotions and the reasons for feeling them. Socratis contains 18K free-form reactions for 980 emotions on 2075 image-caption pairs from 5 widely-read news and image-caption (IC) datasets. We benchmark the capability of state-of-the-art multimodal large language models to generate the reasons for feeling an emotion given an IC pair. Based on a preliminary human study, we observe that humans prefer human-written reasons over 2 times more often than machine-generated ones. This shows our task is harder than standard generation tasks because it starkly contrasts recent findings where humans cannot tell apart machine vs human-written news articles, for instance. We further see that current captioning metrics based on large vision-language models also fail to correlate with human preferences. We hope that these findings and our benchmark will inspire further research on training emotionally aware models.', 'authors': ['Kate Saenko', 'Bryan A. Plummer', 'Saadia Gabriel', 'Reuben Tan', 'Arijit Ray', 'Katherine Deng'], 'published': '2023-08-31', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'multiway-adapater-adapting-large-scale-multi', 'arxiv_id': '2309.01516', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.01516v3', 'url_pdf': 'https://arxiv.org/pdf/2309.01516v3.pdf', 'title': 'MultiWay-Adapater: Adapting large-scale multi-modal models for scalable image-text retrieval', 'abstract': \"As Multimodal Large Language Models (MLLMs) grow in size, adapting them to specialized tasks becomes increasingly challenging due to high computational and memory demands. Indeed, traditional fine-tuning methods are costly, due to the need for extensive, task-specific training. While efficient adaptation methods exist that aim to reduce these costs, in practice they suffer from shallow inter-modal alignment, which severely hurts model effectiveness. To tackle these computational challenges and improve inter-modal alignment, we introduce the MultiWay-Adapter (MWA), a novel framework featuring an 'Alignment Enhancer'. This enhancer deepens inter-modal alignment, enabling high transferability with minimal tuning effort. Our experiments show that unlike prior efficient tuning approaches, MWA maintains model effectiveness, while reducing training time by up-to 57%. MWA is also lightweight, increasing model size by only 2-3% (in terms of parameters) for state-of-the-art foundation models like BEiT-3 Large. These results demonstrate that MWA provides an efficient and effective adaptation method for MLLMs, significantly broadening their applicability.\", 'authors': ['Gerardo Aragon Camarasa', 'Richard McCreadie', 'George Killick', 'Zijun Long'], 'published': '2023-09-04', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'next-gpt-any-to-any-multimodal-llm', 'arxiv_id': '2309.05519', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.05519v3', 'url_pdf': 'https://arxiv.org/pdf/2309.05519v3.pdf', 'title': 'NExT-GPT: Any-to-Any Multimodal LLM', 'abstract': 'While recently Multimodal Large Language Models (MM-LLMs) have made exciting strides, they mostly fall prey to the limitation of only input-side multimodal understanding, without the ability to produce content in multiple modalities. As we humans always perceive the world and communicate with people through various modalities, developing any-to-any MM-LLMs capable of accepting and delivering content in any modality becomes essential to human-level AI. To fill the gap, we present an end-to-end general-purpose any-to-any MM-LLM system, NExT-GPT. We connect an LLM with multimodal adaptors and different diffusion decoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio. By leveraging the existing well-trained highly-performing encoders and decoders, NExT-GPT is tuned with only a small amount of parameter (1%) of certain projection layers, which not only benefits low-cost training and also facilitates convenient expansion to more potential modalities. Moreover, we introduce a modality-switching instruction tuning (MosIT) and manually curate a high-quality dataset for MosIT, based on which NExT-GPT is empowered with complex cross-modal semantic understanding and content generation. Overall, our research showcases the promising possibility of building an AI agent capable of modeling universal modalities, paving the way for more human-like AI research in the community. Project page: https://next-gpt.github.io/', 'authors': ['Tat-Seng Chua', 'Wei Ji', 'Leigang Qu', 'Hao Fei', 'Shengqiong Wu'], 'published': '2023-09-11', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'hilm-d-towards-high-resolution-understanding', 'arxiv_id': '2309.05186', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.05186v1', 'url_pdf': 'https://arxiv.org/pdf/2309.05186v1.pdf', 'title': 'HiLM-D: Towards High-Resolution Understanding in Multimodal Large Language Models for Autonomous Driving', 'abstract': \"Autonomous driving systems generally employ separate models for different tasks resulting in intricate designs. For the first time, we leverage singular multimodal large language models (MLLMs) to consolidate multiple autonomous driving tasks from videos, i.e., the Risk Object Localization and Intention and Suggestion Prediction (ROLISP) task. ROLISP uses natural language to simultaneously identify and interpret risk objects, understand ego-vehicle intentions, and provide motion suggestions, eliminating the necessity for task-specific architectures. However, lacking high-resolution (HR) information, existing MLLMs often miss small objects (e.g., traffic cones) and overly focus on salient ones (e.g., large trucks) when applied to ROLISP. We propose HiLM-D (Towards High-Resolution Understanding in MLLMs for Autonomous Driving), an efficient method to incorporate HR information into MLLMs for the ROLISP task. Especially, HiLM-D integrates two branches: (i) the low-resolution reasoning branch, can be any MLLMs, processes low-resolution videos to caption risk objects and discern ego-vehicle intentions/suggestions; (ii) the high-resolution perception branch (HR-PB), prominent to HiLM-D,, ingests HR images to enhance detection by capturing vision-specific HR feature maps and prioritizing all potential risks over merely salient objects. Our HR-PB serves as a plug-and-play module, seamlessly fitting into current MLLMs. Experiments on the ROLISP benchmark reveal HiLM-D's notable advantage over leading MLLMs, with improvements of 4.8% in BLEU-4 for captioning and 17.2% in mIoU for detection.\", 'authors': ['Xiaomeng Li', 'Wei zhang', 'Hang Xu', 'Jianhua Han', 'Xinpeng Ding'], 'published': '2023-09-11', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'investigating-the-catastrophic-forgetting-in', 'arxiv_id': '2309.10313', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.10313v4', 'url_pdf': 'https://arxiv.org/pdf/2309.10313v4.pdf', 'title': 'Investigating the Catastrophic Forgetting in Multimodal Large Language Models', 'abstract': 'Following the success of GPT4, there has been a surge in interest in multimodal large language model (MLLM) research. This line of research focuses on developing general-purpose LLMs through fine-tuning pre-trained LLMs and vision models. However, catastrophic forgetting, a notorious phenomenon where the fine-tuned model fails to retain similar performance compared to the pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM). In this paper, we introduce EMT: Evaluating MulTimodality for evaluating the catastrophic forgetting in MLLMs, by treating each MLLM as an image classifier. We first apply EMT to evaluate several open-source fine-tuned MLLMs and we discover that almost all evaluated MLLMs fail to retain the same performance levels as their vision encoders on standard image classification tasks. Moreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess performance throughout the fine-tuning. Interestingly, our results suggest that early-stage fine-tuning on an image dataset improves performance across other image datasets, by enhancing the alignment of text and visual features. However, as fine-tuning proceeds, the MLLMs begin to hallucinate, resulting in a significant loss of generalizability, even when the image encoder remains frozen. Our results suggest that MLLMs have yet to demonstrate performance on par with their vision models on standard image classification tasks and the current MLLM fine-tuning procedure still has room for improvement.', 'authors': ['Yi Ma', 'Yong Jae Lee', 'Qing Qu', 'Mu Cai', 'Xiao Li', 'Shengbang Tong', 'Yuexiang Zhai'], 'published': '2023-09-19', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'dreamllm-synergistic-multimodal-comprehension', 'arxiv_id': '2309.11499', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.11499v2', 'url_pdf': 'https://arxiv.org/pdf/2309.11499v2.pdf', 'title': 'DreamLLM: Synergistic Multimodal Comprehension and Creation', 'abstract': \"This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy. Project page: https://dreamllm.github.io.\", 'authors': ['Li Yi', 'Kaisheng Ma', 'Xiangyu Zhang', 'Xiangwen Kong', 'Haoran Wei', 'HongYu Zhou', 'Jianjian Sun', 'Liang Zhao', 'Jinrong Yang', 'Zheng Ge', 'Zekun Qi', 'Yuang Peng', 'Chunrui Han', 'Runpei Dong'], 'published': '2023-09-20', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'how-robust-is-google-s-bard-to-adversarial', 'arxiv_id': '2309.11751', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.11751v2', 'url_pdf': 'https://arxiv.org/pdf/2309.11751v2.pdf', 'title': \"How Robust is Google's Bard to Adversarial Image Attacks?\", 'abstract': \"Multimodal Large Language Models (MLLMs) that integrate text and other modalities (especially vision) have achieved unprecedented performance in various multimodal tasks. However, due to the unsolved adversarial robustness problem of vision models, MLLMs can have more severe safety and security risks by introducing the vision inputs. In this work, we study the adversarial robustness of Google's Bard, a competitive chatbot to ChatGPT that released its multimodal capability recently, to better understand the vulnerabilities of commercial MLLMs. By attacking white-box surrogate vision encoders or MLLMs, the generated adversarial examples can mislead Bard to output wrong image descriptions with a 22% success rate based solely on the transferability. We show that the adversarial examples can also attack other MLLMs, e.g., a 26% attack success rate against Bing Chat and a 86% attack success rate against ERNIE bot. Moreover, we identify two defense mechanisms of Bard, including face detection and toxicity detection of images. We design corresponding attacks to evade these defenses, demonstrating that the current defenses of Bard are also vulnerable. We hope this work can deepen our understanding on the robustness of MLLMs and facilitate future research on defenses. Our code is available at https://github.com/thu-ml/Attack-Bard. Update: GPT-4V is available at October 2023. We further evaluate its robustness under the same set of adversarial examples, achieving a 45% attack success rate.\", 'authors': ['Jun Zhu', 'Hang Su', 'Yu Tian', 'Yichi Zhang', 'Xiao Yang', 'Zhengwei Fang', 'Jiawei Chen', 'Huanran Chen', 'Yinpeng Dong'], 'published': '2023-09-21', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'vdc-versatile-data-cleanser-for-detecting', 'arxiv_id': '2309.16211', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.16211v2', 'url_pdf': 'https://arxiv.org/pdf/2309.16211v2.pdf', 'title': 'VDC: Versatile Data Cleanser based on Visual-Linguistic Inconsistency by Multimodal Large Language Models', 'abstract': 'The role of data in building AI systems has recently been emphasized by the emerging concept of data-centric AI. Unfortunately, in the real-world, datasets may contain dirty samples, such as poisoned samples from backdoor attack, noisy labels in crowdsourcing, and even hybrids of them. The presence of such dirty samples makes the DNNs vunerable and unreliable.Hence, it is critical to detect dirty samples to improve the quality and realiability of dataset. Existing detectors only focus on detecting poisoned samples or noisy labels, that are often prone to weak generalization when dealing with dirty samples from other domains.In this paper, we find a commonality of various dirty samples is visual-linguistic inconsistency between images and associated labels. To capture the semantic inconsistency between modalities, we propose versatile data cleanser (VDC) leveraging the surpassing capabilities of multimodal large language models (MLLM) in cross-modal alignment and reasoning.It consists of three consecutive modules: the visual question generation module to generate insightful questions about the image; the visual question answering module to acquire the semantics of the visual content by answering the questions with MLLM; followed by the visual answer evaluation module to evaluate the inconsistency.Extensive experiments demonstrate its superior performance and generalization to various categories and types of dirty samples. The code is available at \\\\url{https://github.com/zihao-ai/vdc}.', 'authors': ['Baoyuan Wu', 'Bingzhe Wu', 'Shaokui Wei', 'Mingda Zhang', 'Zihao Zhu'], 'published': '2023-09-28', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'guiding-instruction-based-image-editing-via', 'arxiv_id': '2309.17102', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.17102v2', 'url_pdf': 'https://arxiv.org/pdf/2309.17102v2.pdf', 'title': 'Guiding Instruction-based Image Editing via Multimodal Large Language Models', 'abstract': 'Instruction-based image editing improves the controllability and flexibility of image manipulation via natural commands without elaborate descriptions or regional masks. However, human instructions are sometimes too brief for current methods to capture and follow. Multimodal large language models (MLLMs) show promising capabilities in cross-modal understanding and visual-aware response generation via LMs. We investigate how MLLMs facilitate edit instructions and present MLLM-Guided Image Editing (MGIE). MGIE learns to derive expressive instructions and provides explicit guidance. The editing model jointly captures this visual imagination and performs manipulation through end-to-end training. We evaluate various aspects of Photoshop-style modification, global photo optimization, and local editing. Extensive experimental results demonstrate that expressive instructions are crucial to instruction-based image editing, and our MGIE can lead to a notable improvement in automatic metrics and human evaluation while maintaining competitive inference efficiency.', 'authors': ['Zhe Gan', 'Yinfei Yang', 'William Yang Wang', 'Xianzhi Du', 'Wenze Hu', 'Tsu-Jui Fu'], 'published': '2023-09-29', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'reformulating-vision-language-foundation', 'arxiv_id': '2310.00653', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.00653v1', 'url_pdf': 'https://arxiv.org/pdf/2310.00653v1.pdf', 'title': 'Reformulating Vision-Language Foundation Models and Datasets Towards Universal Multimodal Assistants', 'abstract': \"Recent Multimodal Large Language Models (MLLMs) exhibit impressive abilities to perceive images and follow open-ended instructions. The capabilities of MLLMs depend on two crucial factors: the model architecture to facilitate the feature alignment of visual modules and large language models; the multimodal instruction tuning datasets for human instruction following. (i) For the model architecture, most existing models introduce an external bridge module to connect vision encoders with language models, which needs an additional feature-alignment pre-training. In this work, we discover that compact pre-trained vision language models can inherently serve as ``out-of-the-box'' bridges between vision and language. Based on this, we propose Muffin framework, which directly employs pre-trained vision-language models to act as providers of visual signals. (ii) For the multimodal instruction tuning datasets, existing methods omit the complementary relationship between different datasets and simply mix datasets from different tasks. Instead, we propose UniMM-Chat dataset which explores the complementarities of datasets to generate 1.1M high-quality and diverse multimodal instructions. We merge information describing the same image from diverse datasets and transforms it into more knowledge-intensive conversation data. Experimental results demonstrate the effectiveness of the Muffin framework and UniMM-Chat dataset. Muffin achieves state-of-the-art performance on a wide range of vision-language tasks, significantly surpassing state-of-the-art models like LLaVA and InstructBLIP. Our model and dataset are all accessible at https://github.com/thunlp/muffin.\", 'authors': ['Maosong Sun', 'Hai-Tao Zheng', 'Zhiyuan Liu', 'Dahai Li', 'Jiao Xue', 'Yinxv Pan', 'Shan Wang', 'Chongyi Wang', 'Yue Zhao', 'Haoye Zhang', 'Yuan YAO', 'Jinyi Hu', 'Tianyu Yu'], 'published': '2023-10-01', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'drivegpt4-interpretable-end-to-end-autonomous', 'arxiv_id': '2310.01412', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.01412v5', 'url_pdf': 'https://arxiv.org/pdf/2310.01412v5.pdf', 'title': 'DriveGPT4: Interpretable End-to-end Autonomous Driving via Large Language Model', 'abstract': 'Multimodal large language models (MLLMs) have emerged as a prominent area of interest within the research community, given their proficiency in handling and reasoning with non-textual data, including images and videos. This study seeks to extend the application of MLLMs to the realm of autonomous driving by introducing DriveGPT4, a novel interpretable end-to-end autonomous driving system based on LLMs. Capable of processing multi-frame video inputs and textual queries, DriveGPT4 facilitates the interpretation of vehicle actions, offers pertinent reasoning, and effectively addresses a diverse range of questions posed by users. Furthermore, DriveGPT4 predicts low-level vehicle control signals in an end-to-end fashion.These advanced capabilities are achieved through the utilization of a bespoke visual instruction tuning dataset, specifically tailored for autonomous driving applications, in conjunction with a mix-finetuning training strategy. DriveGPT4 represents the pioneering effort to leverage LLMs for the development of an interpretable end-to-end autonomous driving solution. Evaluations conducted on the BDD-X dataset showcase the superior qualitative and quantitative performance of DriveGPT4. Additionally, the fine-tuning of domain-specific data enables DriveGPT4 to yield close or even improved results in terms of autonomous driving grounding when contrasted with GPT4-V.', 'authors': ['Kwan-Yee. K. Wong', 'Hengshuang Zhao', 'Zhenguo Li', 'Yong Guo', 'Zhen Zhao', 'Enze Xie', 'Yujia Zhang', 'Zhenhua Xu'], 'published': '2023-10-02', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'minigpt-5-interleaved-vision-and-language', 'arxiv_id': '2310.02239', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.02239v3', 'url_pdf': 'https://arxiv.org/pdf/2310.02239v3.pdf', 'title': 'MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens', 'abstract': 'The effectiveness of Multimodal Large Language Models (MLLMs) demonstrates a profound capability in multimodal understanding. However, the simultaneous generation of images with coherent texts is still underdeveloped. Addressing this, we introduce a novel interleaved vision-and-language generation method, centered around the concept of ``generative vokens\". These vokens serve as pivotal elements contributing to coherent image-text outputs. Our method is marked by a unique two-stage training strategy for description-free multimodal generation, which does not necessitate extensive descriptions of images. We integrate classifier-free guidance to enhance the alignment of generated images and texts, ensuring more seamless and contextually relevant multimodal interactions. Our model, MiniGPT-5, exhibits substantial improvement over the baseline models on multimodal generation datasets, including MMDialog and VIST. The human evaluation shows MiniGPT-5 is better than the baseline model on more than 56\\\\% cases for multimodal generation, highlighting its efficacy across diverse benchmarks.', 'authors': ['Xin Eric Wang', 'Xuehai He', 'Kaizhi Zheng'], 'published': '2023-10-03', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'towards-end-to-end-embodied-decision-making', 'arxiv_id': '2310.02071', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.02071v4', 'url_pdf': 'https://arxiv.org/pdf/2310.02071v4.pdf', 'title': 'Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond', 'abstract': 'In this study, we explore the potential of Multimodal Large Language Models (MLLMs) in improving embodied decision-making processes for agents. While Large Language Models (LLMs) have been widely used due to their advanced reasoning skills and vast world knowledge, MLLMs like GPT4-Vision offer enhanced visual understanding and reasoning capabilities. We investigate whether state-of-the-art MLLMs can handle embodied decision-making in an end-to-end manner and whether collaborations between LLMs and MLLMs can enhance decision-making. To address these questions, we introduce a new benchmark called PCA-EVAL, which evaluates embodied decision-making from the perspectives of Perception, Cognition, and Action. Additionally, we propose HOLMES, a multi-agent cooperation framework that allows LLMs to leverage MLLMs and APIs to gather multimodal information for informed decision-making. We compare end-to-end embodied decision-making and HOLMES on our benchmark and find that the GPT4-Vision model demonstrates strong end-to-end embodied decision-making abilities, outperforming GPT4-HOLMES in terms of average decision accuracy (+3%). However, this performance is exclusive to the latest GPT4-Vision model, surpassing the open-source state-of-the-art MLLM by 26%. Our results indicate that powerful MLLMs like GPT4-Vision hold promise for decision-making in embodied agents, offering new avenues for MLLM research. Code and data are open at https://github.com/pkunlp-icler/PCA-EVAL/.', 'authors': ['Peiyi Wang', 'Baobao Chang', 'Tianyu Liu', 'Yuchi Wang', 'Zefan Cai', 'Haozhe Zhao', 'Shuhuai Ren', 'Yichi Zhang', 'Liang Chen'], 'published': '2023-10-03', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'kosmos-g-generating-images-in-context-with', 'arxiv_id': '2310.02992', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.02992v3', 'url_pdf': 'https://arxiv.org/pdf/2310.02992v3.pdf', 'title': 'Kosmos-G: Generating Images in Context with Multimodal Large Language Models', 'abstract': 'Recent advancements in subject-driven image generation have made significant strides. However, current methods still fall short in diverse application scenarios, as they require test-time tuning and cannot accept interleaved multi-image and text input. These limitations keep them far from the ultimate goal of \"image as a foreign language in image generation.\" This paper presents Kosmos-G, a model that leverages the advanced multimodal perception capabilities of Multimodal Large Language Models (MLLMs) to tackle the aforementioned challenge. Our approach aligns the output space of MLLM with CLIP using the textual modality as an anchor and performs compositional instruction tuning on curated data. Kosmos-G demonstrates an impressive capability of zero-shot subject-driven generation with interleaved multi-image and text input. Notably, the score distillation instruction tuning requires no modifications to the image decoder. This allows for a seamless substitution of CLIP and effortless integration with a myriad of U-Net techniques ranging from fine-grained controls to personalized image decoder variants. We posit Kosmos-G as an initial attempt towards the goal of \"image as a foreign language in image generation.\" The code can be found at https://aka.ms/Kosmos-G', 'authors': ['Furu Wei', 'Wenhu Chen', 'Zhiliang Peng', 'Shaohan Huang', 'Li Dong', 'Xichen Pan'], 'published': '2023-10-04', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'fine-grained-audio-visual-joint', 'arxiv_id': '2310.05863', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.05863v2', 'url_pdf': 'https://arxiv.org/pdf/2310.05863v2.pdf', 'title': 'Fine-grained Audio-Visual Joint Representations for Multimodal Large Language Models', 'abstract': 'Audio-visual large language models (LLM) have drawn significant attention, yet the fine-grained combination of both input streams is rather under-explored, which is challenging but necessary for LLMs to understand general video inputs. To this end, a fine-grained audio-visual joint representation (FAVOR) learning framework for multimodal LLMs is proposed in this paper, which extends a text-based LLM to simultaneously perceive speech and audio events in the audio input stream and images or videos in the visual input stream, at the frame level. To fuse the audio and visual feature streams into joint representations and to align the joint space with the LLM input embedding space, we propose a causal Q-Former structure with a causal attention module to enhance the capture of causal relations of the audio-visual frames across time. An audio-visual evaluation benchmark (AVEB) is also proposed which comprises six representative single-modal tasks with five cross-modal tasks reflecting audio-visual co-reasoning abilities. While achieving competitive single-modal performance on audio, speech and image tasks in AVEB, FAVOR achieved over 20% accuracy improvements on the video question-answering task when fine-grained information or temporal causal reasoning is required. FAVOR, in addition, demonstrated remarkable video comprehension and reasoning abilities on tasks that are unprecedented by other multimodal LLMs. An interactive demo of FAVOR is available at https://github.com/BriansIDP/AudioVisualLLM.git, and the training code and model checkpoints will be released soon.', 'authors': ['Chao Zhang', 'Zejun Ma', 'Lu Lu', 'Wei Li', 'Tian Tan', 'Xianzhao Chen', 'Changli Tang', 'Wenyi Yu', 'Guangzhi Sun'], 'published': '2023-10-09', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'can-we-edit-multimodal-large-language-models', 'arxiv_id': '2310.08475', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.08475v5', 'url_pdf': 'https://arxiv.org/pdf/2310.08475v5.pdf', 'title': 'Can We Edit Multimodal Large Language Models?', 'abstract': 'In this paper, we focus on editing Multimodal Large Language Models (MLLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights. Code and dataset are available in https://github.com/zjunlp/EasyEdit.', 'authors': ['Ningyu Zhang', 'Huajun Chen', 'Yongheng Wang', 'Xi Chen', 'Qingbin Liu', 'Bozhong Tian', 'Siyuan Cheng'], 'published': '2023-10-12', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'autorepo-a-general-framework-for-multi-modal', 'arxiv_id': '2310.07944', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.07944v2', 'url_pdf': 'https://arxiv.org/pdf/2310.07944v2.pdf', 'title': 'AutoRepo: A general framework for multi-modal LLM-based automated construction reporting', 'abstract': 'Ensuring the safety, quality, and timely completion of construction projects is paramount, with construction inspections serving as a vital instrument towards these goals. Nevertheless, the predominantly manual approach of present-day inspections frequently results in inefficiencies and inadequate information management. Such methods often fall short of providing holistic, exhaustive assessments, consequently engendering regulatory oversights and potential safety hazards. To address this issue, this paper presents a novel framework named AutoRepo for automated generation of construction inspection reports. The unmanned vehicles efficiently perform construction inspections and collect scene information, while the multimodal large language models (LLMs) are leveraged to automatically generate the inspection reports. The framework was applied and tested on a real-world construction site, demonstrating its potential to expedite the inspection process, significantly reduce resource allocation, and produce high-quality, regulatory standard-compliant inspection reports. This research thus underscores the immense potential of multimodal large language models in revolutionizing construction inspection practices, signaling a significant leap forward towards a more efficient and safer construction management paradigm.', 'authors': ['Heng Li', 'Runhao Guo', 'Jing Li', 'Xincong Yang', 'Hongxu Pu'], 'published': '2023-10-11', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'mm-bigbench-evaluating-multimodal-models-on', 'arxiv_id': '2310.09036', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.09036v1', 'url_pdf': 'https://arxiv.org/pdf/2310.09036v1.pdf', 'title': 'MM-BigBench: Evaluating Multimodal Models on Multimodal Content Comprehension Tasks', 'abstract': \"The popularity of multimodal large language models (MLLMs) has triggered a recent surge in research efforts dedicated to evaluating these models. Nevertheless, existing evaluation studies of MLLMs primarily focus on the comprehension and reasoning of unimodal (vision) content, neglecting performance evaluations in the domain of multimodal (vision-language) content understanding. Beyond multimodal reasoning, tasks related to multimodal content comprehension necessitate a profound understanding of multimodal contexts, achieved through the multimodal interaction to obtain a final answer. In this paper, we introduce a comprehensive assessment framework called MM-BigBench, which incorporates a diverse range of metrics to offer an extensive evaluation of the performance of various models and instructions across a wide spectrum of diverse multimodal content comprehension tasks. Consequently, our work complements research on the performance of MLLMs in multimodal comprehension tasks, achieving a more comprehensive and holistic evaluation of MLLMs. To begin, we employ the Best Performance metric to ascertain each model's performance upper bound on different datasets. Subsequently, the Mean Relative Gain metric offers an assessment of the overall performance of various models and instructions, while the Stability metric measures their sensitivity. Furthermore, previous research centers on evaluating models independently or solely assessing instructions, neglecting the adaptability between models and instructions. We propose the Adaptability metric to quantify the adaptability between models and instructions. Our paper evaluates a total of 20 language models (14 MLLMs) on 14 multimodal datasets spanning 6 tasks, with 10 instructions for each task, and derives novel insights. Our code will be released at https://github.com/declare-lab/MM-BigBench.\", 'authors': ['Soujanya Poria', 'XiaoMing Fu', 'Yifei Zhang', 'Qi Sun', 'Yang Li', 'Daling Wang', 'Ming Wang', 'Shi Feng', 'Wenfang Wu', 'Xiaocui Yang'], 'published': '2023-10-13', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'robollm-robotic-vision-tasks-grounded-on', 'arxiv_id': '2310.10221', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.10221v2', 'url_pdf': 'https://arxiv.org/pdf/2310.10221v2.pdf', 'title': 'RoboLLM: Robotic Vision Tasks Grounded on Multimodal Large Language Models', 'abstract': 'Robotic vision applications often necessitate a wide range of visual perception tasks, such as object detection, segmentation, and identification. While there have been substantial advances in these individual tasks, integrating specialized models into a unified vision pipeline presents significant engineering challenges and costs. Recently, Multimodal Large Language Models (MLLMs) have emerged as novel backbones for various downstream tasks. We argue that leveraging the pre-training capabilities of MLLMs enables the creation of a simplified framework, thus mitigating the need for task-specific encoders. Specifically, the large-scale pretrained knowledge in MLLMs allows for easier fine-tuning to downstream robotic vision tasks and yields superior performance. We introduce the RoboLLM framework, equipped with a BEiT-3 backbone, to address all visual perception tasks in the ARMBench challenge-a large-scale robotic manipulation dataset about real-world warehouse scenarios. RoboLLM not only outperforms existing baselines but also substantially reduces the engineering burden associated with model selection and tuning. The source code is publicly available at https://github.com/longkukuhi/armbench.', 'authors': ['Gerardo Aragon Camarasa', 'Richard McCreadie', 'George Killick', 'Zijun Long'], 'published': '2023-10-16', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'benchmarking-sequential-visual-input', 'arxiv_id': '2310.13473', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.13473v1', 'url_pdf': 'https://arxiv.org/pdf/2310.13473v1.pdf', 'title': 'Benchmarking Sequential Visual Input Reasoning and Prediction in Multimodal Large Language Models', 'abstract': \"Multimodal large language models (MLLMs) have shown great potential in perception and interpretation tasks, but their capabilities in predictive reasoning remain under-explored. To address this gap, we introduce a novel benchmark that assesses the predictive reasoning capabilities of MLLMs across diverse scenarios. Our benchmark targets three important domains: abstract pattern reasoning, human activity prediction, and physical interaction prediction. We further develop three evaluation methods powered by large language model to robustly quantify a model's performance in predicting and reasoning the future based on multi-visual context. Empirical experiments confirm the soundness of the proposed benchmark and evaluation methods via rigorous testing and reveal pros and cons of current popular MLLMs in the task of predictive reasoning. Lastly, our proposed benchmark provides a standardized evaluation framework for MLLMs and can facilitate the development of more advanced models that can reason and predict over complex long sequence of multimodal input.\", 'authors': ['Jianwei Yin', 'Tiancheng Zhao', 'Kangjia Zhao', 'Yu Shu', 'Leigang Sha', 'Mingwei Zhu'], 'published': '2023-10-20', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'woodpecker-hallucination-correction-for', 'arxiv_id': '2310.16045', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.16045v2', 'url_pdf': 'https://arxiv.org/pdf/2310.16045v2.pdf', 'title': 'Woodpecker: Hallucination Correction for Multimodal Large Language Models', 'abstract': 'Hallucination is a big shadow hanging over the rapidly evolving Multimodal Large Language Models (MLLMs), referring to the phenomenon that the generated text is inconsistent with the image content. In order to mitigate hallucinations, existing studies mainly resort to an instruction-tuning manner that requires retraining the models with specific data. In this paper, we pave a different way, introducing a training-free method named Woodpecker. Like a woodpecker heals trees, it picks out and corrects hallucinations from the generated text. Concretely, Woodpecker consists of five stages: key concept extraction, question formulation, visual knowledge validation, visual claim generation, and hallucination correction. Implemented in a post-remedy manner, Woodpecker can easily serve different MLLMs, while being interpretable by accessing intermediate outputs of the five stages. We evaluate Woodpecker both quantitatively and qualitatively and show the huge potential of this new paradigm. On the POPE benchmark, our method obtains a 30.66%/24.33% improvement in accuracy over the baseline MiniGPT-4/mPLUG-Owl. The source code is released at https://github.com/BradyFU/Woodpecker.', 'authors': ['Enhong Chen', 'Xing Sun', 'Ke Li', 'Yunhang Shen', 'Dianbo Sui', 'Hao Wang', 'Tong Xu', 'Sirui Zhao', 'Chaoyou Fu', 'Shukang Yin'], 'published': '2023-10-24', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'visual-cropping-improves-zero-shot-question', 'arxiv_id': '2310.16033', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.16033v3', 'url_pdf': 'https://arxiv.org/pdf/2310.16033v3.pdf', 'title': 'Towards Perceiving Small Visual Details in Zero-shot Visual Question Answering with Multimodal LLMs', 'abstract': \"Multimodal Large Language Models (MLLMs) have recently achieved promising zero-shot accuracy on visual question answering (VQA) -- a fundamental task affecting various downstream applications and domains. Given the great potential for the broad use of these models, it is important to investigate their limitations in dealing with different image and question properties. In this work, we investigate whether MLLMs can perceive small details as well as large details in images. In particular, we show that their zero-shot accuracy in answering visual questions is very sensitive to the size of the visual subject of the question, declining up to 46% with size. Furthermore, we show that this effect is causal by observing that human visual cropping can significantly mitigate their sensitivity to size. Inspired by the usefulness of human cropping, we then propose five automatic visual cropping methods -- leveraging either external localization models or the decision process of the given MLLM itself -- as inference time mechanisms to improve the zero-shot performance of MLLMs. We study their effectiveness on four popular VQA datasets, and a subset of the VQAv2 dataset tailored towards fine visual details. Our findings suggest that MLLMs should be used with caution in detail-sensitive VQA applications, and that visual cropping is a promising direction to improve their zero-shot performance. To facilitate further investigation of MLLMs' behaviors, our code and data are publicly released.\", 'authors': ['Filip Ilievski', 'Prateek Chhikara', 'Mahyar Khayatkhoei', 'Jiarui Zhang'], 'published': '2023-10-24', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'chef-a-comprehensive-evaluation-framework-for', 'arxiv_id': '2311.02692', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2311.02692v1', 'url_pdf': 'https://arxiv.org/pdf/2311.02692v1.pdf', 'title': 'ChEF: A Comprehensive Evaluation Framework for Standardized Assessment of Multimodal Large Language Models', 'abstract': \"Multimodal Large Language Models (MLLMs) have shown impressive abilities in interacting with visual content with myriad potential downstream tasks. However, even though a list of benchmarks has been proposed, the capabilities and limitations of MLLMs are still not comprehensively understood, due to a lack of a standardized and holistic evaluation framework. To this end, we present the first Comprehensive Evaluation Framework (ChEF) that can holistically profile each MLLM and fairly compare different MLLMs. First, we structure ChEF as four modular components, i.e., Scenario as scalable multimodal datasets, Instruction as flexible instruction retrieving formulae, Inferencer as reliable question answering strategies, and Metric as indicative task-specific score functions. Based on them, ChEF facilitates versatile evaluations in a standardized framework, and new evaluations can be built by designing new Recipes (systematic selection of these four components). Notably, current MLLM benchmarks can be readily summarized as recipes of ChEF. Second, we introduce 6 new recipes to quantify competent MLLMs' desired capabilities (or called desiderata, i.e., calibration, in-context learning, instruction following, language performance, hallucination, and robustness) as reliable agents that can perform real-world multimodal interactions. Third, we conduct a large-scale evaluation of 9 prominent MLLMs on 9 scenarios and 6 desiderata. Our evaluation summarized over 20 valuable observations concerning the generalizability of MLLMs across various scenarios and the composite capability of MLLMs required for multimodal interactions. We will publicly release all the detailed implementations for further analysis, as well as an easy-to-use modular toolkit for the integration of new recipes and models, so that ChEF can be a growing evaluation framework for the MLLM community.\", 'authors': ['Jing Shao', 'Yu Qiao', 'Lu Sheng', 'Zhenfei Yin', 'Hongxing Fan', 'Zhipin Wang', 'Zhelun Shi'], 'published': '2023-11-05', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'octavius-mitigating-task-interference-in', 'arxiv_id': '2311.02684', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2311.02684v3', 'url_pdf': 'https://arxiv.org/pdf/2311.02684v3.pdf', 'title': 'Octavius: Mitigating Task Interference in MLLMs via LoRA-MoE', 'abstract': 'Recent studies have demonstrated Large Language Models (LLMs) can extend their zero-shot generalization capabilities to multimodal learning through instruction tuning. As more modalities and downstream tasks are introduced, negative conflicts and interference may have a worse impact on performance. While this phenomenon has been overlooked in previous work, we propose a novel and extensible framework, called Octavius, for comprehensive studies and experimentation on multimodal learning with Multimodal Large Language Models (MLLMs). Specifically, we combine the well-known Mixture-of-Experts (MoE) and one of the representative PEFT techniques, i.e., LoRA, designing a novel LLM-based decoder, called LoRA-MoE, for multimodal learning. To the best of our knowledge, we are one of the pioneering efforts to introduce MoE into MLLMs to address this problem. The experimental results (about 20% improvement) have shown the effectiveness and versatility of our design in various 2D and 3D downstream tasks. Code and datasets are available at https://openlamm.github.io/tutorial/.', 'authors': ['Jing Shao', 'Yu Qiao', 'Wanli Ouyang', 'Lu Sheng', 'Si Liu', 'Zhenfei Yin', 'Huayang Liu', 'Zhen Wang', 'Ziqin Wang', 'Zeren Chen'], 'published': '2023-11-05', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'what-large-language-models-bring-to-text-rich', 'arxiv_id': '2311.07306', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2311.07306v1', 'url_pdf': 'https://arxiv.org/pdf/2311.07306v1.pdf', 'title': 'What Large Language Models Bring to Text-rich VQA?', 'abstract': \"Text-rich VQA, namely Visual Question Answering based on text recognition in the images, is a cross-modal task that requires both image comprehension and text recognition. In this work, we focus on investigating the advantages and bottlenecks of LLM-based approaches in addressing this problem. To address the above concern, we separate the vision and language modules, where we leverage external OCR models to recognize texts in the image and Large Language Models (LLMs) to answer the question given texts. The whole framework is training-free benefiting from the in-context ability of LLMs. This pipeline achieved superior performance compared to the majority of existing Multimodal Large Language Models (MLLM) on four text-rich VQA datasets. Besides, based on the ablation study, we find that LLM brings stronger comprehension ability and may introduce helpful knowledge for the VQA problem. The bottleneck for LLM to address text-rich VQA problems may primarily lie in visual part. We also combine the OCR module with MLLMs and pleasantly find that the combination of OCR module with MLLM also works. It's worth noting that not all MLLMs can comprehend the OCR information, which provides insights into how to train an MLLM that preserves the abilities of LLM.\", 'authors': ['Fei Tan', 'Zechao Li', 'Rui Zhao', 'Jinghui Lu', 'Xinzhe Ni', 'Wei Tang', 'Xuejing Liu'], 'published': '2023-11-13', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}], 'docs': [Document(metadata={'title': 'Multimodal large language models for inclusive collaboration learning tasks'}, page_content='This PhD project leverages advancements in multimodal large language models to build an inclusive collaboration feedback loop, in order to facilitate the automated detection, modeling, and feedback for participants developing general collaboration skills. This topic is important given the role of collaboration as an essential 21st century skill, the potential to ground large language models within learning theory and real-world practice, and the expressive potential of transformer models to support equity and inclusion. We address some concerns of integrating advances in natural language processing into downstream tasks such as the learning analytics feedback loop.'), Document(metadata={'title': 'Personality-aware Human-centric Multimodal Reasoning: A New Task, Dataset and Baselines'}, page_content=\"Personality traits, emotions, and beliefs shape individuals' behavioral choices and decision-making processes. However, for one thing, the affective computing community normally focused on predicting personality traits but overlooks their application in behavior prediction. For another, the multimodal reasoning task emphasized the prediction of future states and behaviors but often neglected the incorporation of individual personality traits. In this work, we introduce a new task called Personality-aware Human-centric Multimodal Reasoning (PHMR) (T1), with the goal of forecasting the future behavior of a particular individual using multimodal information from past instances, while integrating personality factors. We accordingly construct a new dataset based on six television shows, encompassing 225 characters and 12k samples. To establish a benchmark for the task, we propose seven baseline methods: three adapted from related tasks, two pre-trained model, and two multimodal large language models. The experimental results demonstrate that incorporating personality traits enhances human-centric multimodal reasoning performance. To further solve the lack of personality annotation in real-life scenes, we introduce an extension task called Personality-predicted Human-centric Multimodal Reasoning task (T2) along with the corresponding dataset and method. We will make our dataset and code available on GitHub.\"), Document(metadata={'title': 'Visual Transformation Telling'}, page_content='Humans can naturally reason from superficial state differences (e.g. ground wetness) to transformations descriptions (e.g. raining) according to their life experience. In this paper, we propose a new visual reasoning task to test this transformation reasoning ability in real-world scenarios, called \\\\textbf{V}isual \\\\textbf{T}ransformation \\\\textbf{T}elling (VTT). Given a series of states (i.e. images), VTT requires to describe the transformation occurring between every two adjacent states. Different from existing visual reasoning tasks that focus on surface state reasoning, the advantage of VTT is that it captures the underlying causes, e.g. actions or events, behind the differences among states. We collect a novel dataset to support the study of transformation reasoning from two existing instructional video datasets, CrossTask and COIN, comprising 13,547 samples. Each sample involves the key state images along with their transformation descriptions. Our dataset covers diverse real-world activities, providing a rich resource for training and evaluation. To construct an initial benchmark for VTT, we test several models, including traditional visual storytelling methods (CST, GLACNet, Densecap) and advanced multimodal large language models (LLaVA v1.5-7B, Qwen-VL-chat, Gemini Pro Vision, GPT-4o, and GPT-4). Experimental results reveal that even state-of-the-art models still face challenges in VTT, highlighting substantial areas for improvement.'), Document(metadata={'title': 'Listen, Think, and Understand'}, page_content='The ability of artificial intelligence (AI) systems to perceive and comprehend audio signals is crucial for many applications. Although significant progress has been made in this area since the development of AudioSet, most existing models are designed to map audio inputs to pre-defined, discrete sound label sets. In contrast, humans possess the ability to not only classify sounds into general categories, but also to listen to the finer details of the sounds, explain the reason for the predictions, think about what the sound infers, and understand the scene and what action needs to be taken, if any. Such capabilities beyond perception are not yet present in existing audio models. On the other hand, modern large language models (LLMs) exhibit emerging reasoning ability but they lack audio perception capabilities. Therefore, we ask the question: can we build a model that has both audio perception and a reasoning ability? In this paper, we propose a new audio foundation model, called LTU (Listen, Think, and Understand). To train LTU, we created a new OpenAQA-5M dataset consisting of 1.9 million closed-ended and 3.7 million open-ended, diverse (audio, question, answer) tuples, and have used an autoregressive training framework with a perception-to-understanding curriculum. LTU demonstrates strong performance and generalization ability on conventional audio tasks such as classification and captioning. More importantly, it exhibits emerging audio reasoning and comprehension abilities that are absent in existing audio models. To the best of our knowledge, LTU is one of the first multimodal large language models that focus on general audio (rather than just speech) understanding.'), Document(metadata={'title': 'What Makes for Good Visual Tokenizers for Large Language Models?'}, page_content='We empirically investigate proper pre-training methods to build good visual tokenizers, making Large Language Models (LLMs) powerful Multimodal Large Language Models (MLLMs). In our benchmark, which is curated to evaluate MLLMs visual semantic understanding and fine-grained perception capabilities, we discussed different visual tokenizers pre-trained with dominant methods (i.e., DeiT, CLIP, MAE, DINO), and observe that: i) Fully/weakly supervised models capture more semantics than self-supervised models, but the gap is narrowed by scaling up the pre-training dataset. ii) Self-supervised models are better at fine-grained perception, where patch-level supervision is particularly effective. iii) Tuning the visual tokenizer leads to the loss of semantics obtained from large-scale pretraining, which is unfavorable with relatively small-scale instruction-tuning dataset. Given the findings, we reviewed methods that attempted to unify semantics and fine-grained visual understanding, e.g., patch-level feature distillation with semantically-rich targets. We obtain an intriguing insight mask-based strategies that were once all the rage may not be applicable for obtaining good visual tokenizers. Based on this critical observation, we obtain a new MLLM equipped with a tailored Good Visual Tokenizer (GVT), which exhibits strong visual comprehension capability at multiple scales. In particular, without introducing extra parameters and task-specific fine-tuning, GVT achieves superior performance on visual question answering, image captioning, and other fine-grained visual understanding tasks such as object counting and multi-class identification.'), Document(metadata={'title': 'Dynamic Clue Bottlenecks: Towards Interpretable-by-Design Visual Question Answering'}, page_content='Recent advances in multimodal large language models (LLMs) have shown extreme effectiveness in visual question answering (VQA). However, the design nature of these end-to-end models prevents them from being interpretable to humans, undermining trust and applicability in critical domains. While post-hoc rationales offer certain insight into understanding model behavior, these explanations are not guaranteed to be faithful to the model. In this paper, we address these shortcomings by introducing an interpretable by design model that factors model decisions into intermediate human-legible explanations, and allows people to easily understand why a model fails or succeeds. We propose the Dynamic Clue Bottleneck Model ( (DCLUB), a method that is designed towards an inherently interpretable VQA system. DCLUB provides an explainable intermediate space before the VQA decision and is faithful from the beginning, while maintaining comparable performance to black-box systems. Given a question, DCLUB first returns a set of visual clues: natural language statements of visually salient evidence from the image, and then generates the output based solely on the visual clues. To supervise and evaluate the generation of VQA explanations within DCLUB, we collect a dataset of 1.7k reasoning-focused questions with visual clues. Evaluations show that our inherently interpretable system can improve 4.64% over a comparable black-box system in reasoning-focused questions while preserving 99.43% of performance on VQA-v2.'), Document(metadata={'title': 'PathAsst: A Generative Foundation AI Assistant Towards Artificial General Intelligence of Pathology'}, page_content=\"As advances in large language models (LLMs) and multimodal techniques continue to mature, the development of general-purpose multimodal large language models (MLLMs) has surged, offering significant applications in interpreting natural images. However, the field of pathology has largely remained untapped, particularly in gathering high-quality data and designing comprehensive model frameworks. To bridge the gap in pathology MLLMs, we present PathAsst, a multimodal generative foundation AI assistant to revolutionize diagnostic and predictive analytics in pathology. The development of PathAsst involves three pivotal steps: data acquisition, CLIP model adaptation, and the training of PathAsst's multimodal generative capabilities. Firstly, we collect over 207K high-quality pathology image-text pairs from authoritative sources. Leveraging the advanced power of ChatGPT, we generate over 180K instruction-following samples. Furthermore, we devise additional instruction-following data specifically tailored for invoking eight pathology-specific sub-models we prepared, allowing the PathAsst to effectively collaborate with these models, enhancing its diagnostic ability. Secondly, by leveraging the collected data, we construct PathCLIP, a pathology-dedicated CLIP, to enhance PathAsst's capabilities in interpreting pathology images. Finally, we integrate PathCLIP with the Vicuna-13b and utilize pathology-specific instruction-tuning data to enhance the multimodal generation capacity of PathAsst and bolster its synergistic interactions with sub-models. The experimental results of PathAsst show the potential of harnessing AI-powered generative foundation model to improve pathology diagnosis and treatment processes.\"), Document(metadata={'title': 'Contextual Object Detection with Multimodal Large Language Models'}, page_content='Recent Multimodal Large Language Models (MLLMs) are remarkable in vision-language tasks, such as image captioning and question answering, but lack the essential perception ability, i.e., object detection. In this work, we address this limitation by introducing a novel research problem of contextual object detection -- understanding visible objects within different human-AI interactive contexts. Three representative scenarios are investigated, including the language cloze test, visual captioning, and question answering. Moreover, we present ContextDET, a unified multimodal model that is capable of end-to-end differentiable modeling of visual-language contexts, so as to locate, identify, and associate visual objects with language inputs for human-AI interaction. Our ContextDET involves three key submodels: (i) a visual encoder for extracting visual representations, (ii) a pre-trained LLM for multimodal context decoding, and (iii) a visual decoder for predicting bounding boxes given contextual object words. The new generate-then-detect framework enables us to detect object words within human vocabulary. Extensive experiments show the advantages of ContextDET on our proposed CODE benchmark, open-vocabulary detection, and referring image segmentation. Github: https://github.com/yuhangzang/ContextDET.'), Document(metadata={'title': 'A Survey on Multimodal Large Language Models'}, page_content='Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.'), Document(metadata={'title': 'MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models'}, page_content='Multimodal Large Language Model (MLLM) relies on the powerful LLM to perform multimodal tasks, showing amazing emergent abilities in recent studies, such as writing poems based on an image. However, it is difficult for these case studies to fully reflect the performance of MLLM, lacking a comprehensive evaluation. In this paper, we fill in this blank, presenting the first comprehensive MLLM Evaluation benchmark MME. It measures both perception and cognition abilities on a total of 14 subtasks. In order to avoid data leakage that may arise from direct use of public datasets for evaluation, the annotations of instruction-answer pairs are all manually designed. The concise instruction design allows us to fairly compare MLLMs, instead of struggling in prompt engineering. Besides, with such an instruction, we can also easily carry out quantitative statistics. A total of 30 advanced MLLMs are comprehensively evaluated on our MME, which not only suggests that existing MLLMs still have a large room for improvement, but also reveals the potential directions for the subsequent model optimization. The data application manner and online leaderboards are released at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation.'), Document(metadata={'title': 'Kosmos-2: Grounding Multimodal Large Language Models to the World'}, page_content=\"We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. Specifically, we represent refer expressions as links in Markdown, i.e., ``[text span](bounding boxes)'', where object descriptions are sequences of location tokens. Together with multimodal corpora, we construct large-scale data of grounded image-text pairs (called GrIT) to train the model. In addition to the existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), Kosmos-2 integrates the grounding capability into downstream applications. We evaluate Kosmos-2 on a wide range of tasks, including (i) multimodal grounding, such as referring expression comprehension, and phrase grounding, (ii) multimodal referring, such as referring expression generation, (iii) perception-language tasks, and (iv) language understanding and generation. This work lays out the foundation for the development of Embodiment AI and sheds light on the big convergence of language, multimodal perception, action, and world modeling, which is a key step toward artificial general intelligence. Code and pretrained models are available at https://aka.ms/kosmos-2.\"), Document(metadata={'title': \"Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic\"}, page_content=\"In human conversations, individuals can indicate relevant regions within a scene while addressing others. In turn, the other person can then respond by referring to specific regions if necessary. This natural referential ability in dialogue remains absent in current Multimodal Large Language Models (MLLMs). To fill this gap, this paper proposes an MLLM called Shikra, which can handle spatial coordinate inputs and outputs in natural language. Its architecture consists of a vision encoder, an alignment layer, and a LLM. It is designed to be straightforward and simple, without the need for extra vocabularies, position encoder, pre-/post-detection modules, or external plug-in models. All inputs and outputs are in natural language form. Referential dialogue is a superset of various vision-language (VL) tasks. Shikra can naturally handle location-related tasks like REC and PointQA, as well as conventional VL tasks such as Image Captioning and VQA. Experimental results showcase Shikra's promising performance. Furthermore, it enables numerous exciting applications, like providing mentioned objects' coordinates in chains of thoughts and comparing user-pointed regions similarities. Our code, model and dataset are accessed at https://github.com/shikras/shikra.\"), Document(metadata={'title': 'SVIT: Scaling up Visual Instruction Tuning'}, page_content=\"Thanks to the emerging of foundation models, the large language and vision models are integrated to acquire the multimodal ability of visual captioning, question answering, etc. Although existing multimodal models present impressive performance of visual understanding and reasoning, their limits are still largely under-explored due to the scarcity of high-quality instruction tuning data. To push the limits of multimodal capability, we Scale up Visual Instruction Tuning (SVIT) by constructing a dataset of 4.2 million visual instruction tuning data including 1.6M conversation question-answer (QA) pairs, 1.6M complex reasoning QA pairs, 1.0M referring QA pairs and 106K detailed image descriptions. Besides the volume, the proposed dataset is also featured by the high quality and rich diversity, which is generated by prompting GPT-4 with the abundant manual annotations of images. We also propose a new data recipe to select subset with better diversity and balance, which evokes model's superior capabilities. Extensive experiments verify that SVIT-v1.5, trained on the proposed dataset, outperforms state-of-the-art Multimodal Large Language Models on popular benchmarks. The data and code are publicly available at https://github.com/BAAI-DCAI/Visual-Instruction-Tuning.\"), Document(metadata={'title': 'ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning'}, page_content=\"Human-AI interactivity is a critical aspect that reflects the usability of multimodal large language models (MLLMs). However, existing end-to-end MLLMs only allow users to interact with them through language instructions, leading to the limitation of the interactive accuracy and efficiency. In this study, we present precise referring instructions that utilize diverse reference representations such as points and boxes as referring prompts to refer to the special region. This enables MLLMs to focus on the region of interest and achieve finer-grained interaction. Based on precise referring instruction, we propose ChatSpot, a unified end-to-end multimodal large language model that supports diverse forms of interactivity including mouse clicks, drag-and-drop, and drawing boxes, which provides a more flexible and seamless interactive experience. We also construct a multi-grained vision-language instruction-following dataset based on existing datasets and GPT-4 generating. Furthermore, we design a series of evaluation tasks to assess the effectiveness of region recognition and interaction. Experimental results showcase ChatSpot's promising performance.\"), Document(metadata={'title': 'SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension'}, page_content='Based on powerful Large Language Models (LLMs), recent generative Multimodal Large Language Models (MLLMs) have gained prominence as a pivotal research area, exhibiting remarkable capability for both comprehension and generation. In this work, we address the evaluation of generative comprehension in MLLMs as a preliminary step towards a comprehensive assessment of generative models, by introducing a benchmark named SEED-Bench. SEED-Bench consists of 19K multiple choice questions with accurate human annotations (x 6 larger than existing benchmarks), which spans 12 evaluation dimensions including the comprehension of both the image and video modality. We develop an advanced pipeline for generating multiple-choice questions that target specific evaluation dimensions, integrating both automatic filtering and manual verification processes. Multiple-choice questions with groundtruth options derived from human annotation enables an objective and efficient assessment of model performance, eliminating the need for human or GPT intervention during evaluation. We further evaluate the performance of 18 models across all 12 dimensions, covering both the spatial and temporal understanding. By revealing the limitations of existing MLLMs through evaluation results, we aim for SEED-Bench to provide insights for motivating future research. We will launch and consistently maintain a leaderboard to provide a platform for the community to assess and investigate model capability.'), Document(metadata={'title': 'LISA: Reasoning Segmentation via Large Language Model'}, page_content='Although perception systems have made remarkable advancements in recent years, they still rely on explicit human instruction or pre-defined categories to identify the target objects before executing visual recognition tasks. Such systems cannot actively reason and comprehend implicit user intention. In this work, we propose a new segmentation task -- reasoning segmentation. The task is designed to output a segmentation mask given a complex and implicit query text. Furthermore, we establish a benchmark comprising over one thousand image-instruction-mask data samples, incorporating intricate reasoning and world knowledge for evaluation purposes. Finally, we present LISA: large Language Instructed Segmentation Assistant, which inherits the language generation capabilities of multimodal Large Language Models (LLMs) while also possessing the ability to produce segmentation masks. We expand the original vocabulary with a <SEG> token and propose the embedding-as-mask paradigm to unlock the segmentation capability. Remarkably, LISA can handle cases involving complex reasoning and world knowledge. Also, it demonstrates robust zero-shot capability when trained exclusively on reasoning-free datasets. In addition, fine-tuning the model with merely 239 reasoning segmentation data samples results in further performance enhancement. Both quantitative and qualitative experiments show our method effectively unlocks new reasoning segmentation capabilities for multimodal LLMs. Code, models, and data are available at https://github.com/dvlab-research/LISA.'), Document(metadata={'title': 'Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions'}, page_content=\"Recent advancements in Multimodal Large Language Models (MLLMs) have been utilizing Visual Prompt Generators (VPGs) to convert visual features into tokens that LLMs can recognize. This is achieved by training the VPGs on millions of image-caption pairs, where the VPG-generated tokens of images are fed into a frozen LLM to generate the corresponding captions. However, this image-captioning based training objective inherently biases the VPG to concentrate solely on the primary visual contents sufficient for caption generation, often neglecting other visual details. This shortcoming results in MLLMs' underperformance in comprehending demonstrative instructions consisting of multiple, interleaved, and multimodal instructions that demonstrate the required context to complete a task. To address this issue, we introduce a generic and lightweight Visual Prompt Generator Complete module (VPG-C), which can infer and complete the missing details essential for comprehending demonstrative instructions. Further, we propose a synthetic discriminative training strategy to fine-tune VPG-C, eliminating the need for supervised demonstrative instructions. As for evaluation, we build DEMON, a comprehensive benchmark for demonstrative instruction understanding. Synthetically trained with the proposed strategy, VPG-C achieves significantly stronger zero-shot performance across all tasks of DEMON. Further evaluation on the MME and OwlEval benchmarks also demonstrate the superiority of VPG-C. Our benchmark, code, and pre-trained models are available at https://github.com/DCDmllm/Cheetah.\"), Document(metadata={'title': 'Link-Context Learning for Multimodal LLMs'}, page_content='The ability to learn from context with novel concepts, and deliver appropriate responses are essential in human conversations. Despite current Multimodal Large Language Models (MLLMs) and Large Language Models (LLMs) being trained on mega-scale datasets, recognizing unseen images or understanding novel concepts in a training-free manner remains a challenge. In-Context Learning (ICL) explores training-free few-shot learning, where models are encouraged to ``learn to learn\" from limited tasks and generalize to unseen tasks. In this work, we propose link-context learning (LCL), which emphasizes \"reasoning from cause and effect\" to augment the learning capabilities of MLLMs. LCL goes beyond traditional ICL by explicitly strengthening the causal relationship between the support set and the query set. By providing demonstrations with causal links, LCL guides the model to discern not only the analogy but also the underlying causal associations between data points, which empowers MLLMs to recognize unseen images and understand novel concepts more effectively. To facilitate the evaluation of this novel approach, we introduce the ISEKAI dataset, comprising exclusively of unseen generated image-label pairs designed for link-context learning. Extensive experiments show that our LCL-MLLM exhibits strong link-context learning capabilities to novel concepts over vanilla MLLMs. Code and data will be released at https://github.com/isekai-portal/Link-Context-Learning.'), Document(metadata={'title': 'FashionLOGO: Prompting Multimodal Large Language Models for Fashion Logo Embeddings'}, page_content=\"Logo embedding models convert the product logos in images into vectors, enabling their utilization for logo recognition and detection within e-commerce platforms. This facilitates the enforcement of intellectual property rights and enhances product search capabilities. However, current methods treat logo embedding as a purely visual problem. A noteworthy issue is that visual models capture features more than logos. Instead, we view this as a multimodal task, using text as auxiliary information to facilitate the visual model's understanding of the logo. The emerging Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in both visual and textual understanding. Inspired by this, we propose an approach, \\\\textbf{FashionLOGO}, to explore how to prompt MLLMs to generate appropriate text for product images, which can help visual models achieve better logo embeddings. We adopt a cross-attention transformer block that enables visual embedding to automatically learn supplementary knowledge from textual embedding. Our extensive experiments on real-world datasets prove that FashionLOGO is capable of generating generic and robust logo embeddings, achieving state-of-the-art performance in all benchmarks.\"), Document(metadata={'title': 'WanJuan: A Comprehensive Multimodal Dataset for Advancing English and Chinese Large Models'}, page_content='The rise in popularity of ChatGPT and GPT-4 has significantly accelerated the development of large models, leading to the creation of numerous impressive large language models(LLMs) and multimodal large language models (MLLMs). These cutting-edge models owe their remarkable performance to high-quality data. However, the details of the training data used in leading paradigms are often kept confidential. This lack of transparency, coupled with the scarcity of open-source data, impedes further developments within the community. As a response, this paper presents \"Wan Juan\", a large-scale multimodal dataset composed of both Chinese and English data, collected from a wide range of web sources. The dataset incorporates text, image-text, and video modalities, with a total volume exceeding 2TB. It was utilized in the training of InternLM, a model that demonstrated significant advantages in multi-dimensional evaluations when compared to models of a similar scale. All data can be accessed at https://opendatalab.org.cn/WanJuan1.0.'), Document(metadata={'title': 'StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data'}, page_content=\"The remarkable multimodal capabilities demonstrated by OpenAI's GPT-4 have sparked significant interest in the development of multimodal Large Language Models (LLMs). A primary research objective of such models is to align visual and textual modalities effectively while comprehending human instructions. Current methodologies often rely on annotations derived from benchmark datasets to construct image-dialogue datasets for training purposes, akin to instruction tuning in LLMs. However, these datasets often exhibit domain bias, potentially constraining the generative capabilities of the models. In an effort to mitigate these limitations, we propose a novel data collection methodology that synchronously synthesizes images and dialogues for visual instruction tuning. This approach harnesses the power of generative models, marrying the abilities of ChatGPT and text-to-image generative models to yield a diverse and controllable dataset with varied image content. Additionally, datasets can be arbitrarily scaled. This not only provides greater flexibility compared to existing methodologies but also significantly enhances several model capabilities. Our research includes comprehensive experiments conducted on various datasets. The results emphasize substantial enhancements in more than ten commonly assessed capabilities. Additionally, our model achieves state-of-the-art results across multiple widely recognized multimodal benchmarks.\"), Document(metadata={'title': 'InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4'}, page_content='Multimodal large language models are typically trained in two stages: first pre-training on image-text pairs, and then fine-tuning using supervised vision-language instruction data. Recent studies have shown that large language models can achieve satisfactory results even with a limited amount of high-quality instruction-following data. In this paper, we introduce InstructionGPT-4, which is fine-tuned on a small dataset comprising only 200 examples, amounting to approximately 6\\\\% of the instruction-following data used in the alignment dataset for MiniGPT-4. To achieve this, we first propose several metrics to access the quality of multimodal instruction data. Based on these metrics, we present an effective and trainable data selector to automatically identify and filter low-quality vision-language data. By employing this method, InstructionGPT-4 outperforms the original MiniGPT-4 on various evaluations. Overall, our findings demonstrate that less but high-quality instruction tuning data is efficient in enabling multimodal large language models to generate better output. Our code is available at https://github.com/waltonfuture/InstructionGPT-4.'), Document(metadata={'title': 'VIGC: Visual Instruction Generation and Correction'}, page_content=\"The integration of visual encoders and large language models (LLMs) has driven recent progress in multimodal large language models (MLLMs). However, the scarcity of high-quality instruction-tuning data for vision-language tasks remains a challenge. The current leading paradigm, such as LLaVA, relies on language-only GPT-4 to generate data, which requires pre-annotated image captions and detection bounding boxes, suffering from understanding image details. A practical solution to this problem would be to utilize the available multimodal large language models (MLLMs) to generate instruction data for vision-language tasks. However, it's worth noting that the currently accessible MLLMs are not as powerful as their LLM counterparts, as they tend to produce inadequate responses and generate false information. As a solution for addressing the current issue, this paper proposes the Visual Instruction Generation and Correction (VIGC) framework that enables multimodal large language models to generate instruction-tuning data and progressively enhance its quality on-the-fly. Specifically, Visual Instruction Generation (VIG) guides the vision-language model to generate diverse instruction-tuning data. To ensure generation quality, Visual Instruction Correction (VIC) adopts an iterative update mechanism to correct any inaccuracies in data produced by VIG, effectively reducing the risk of hallucination. Leveraging the diverse, high-quality data generated by VIGC, we finetune mainstream models and validate data quality based on various evaluations. Experimental results demonstrate that VIGC not only compensates for the shortcomings of language-only data generation methods, but also effectively enhances the benchmark performance. The models, datasets, and code are available at https://opendatalab.github.io/VIGC.\"), Document(metadata={'title': 'Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models'}, page_content='Recently, Multimodal Large Language Models (MLLMs) that enable Large Language Models (LLMs) to interpret images through visual instruction tuning have achieved significant success. However, existing visual instruction tuning methods only utilize image-language instruction data to align the language and image modalities, lacking a more fine-grained cross-modal alignment. In this paper, we propose Position-enhanced Visual Instruction Tuning (PVIT), which extends the functionality of MLLMs by integrating an additional region-level vision encoder. This integration promotes a more detailed comprehension of images for the MLLM. In addition, to efficiently achieve a fine-grained alignment between the vision modules and the LLM, we design multiple data generation strategies to construct an image-region-language instruction dataset. Finally, we present both quantitative experiments and qualitative analysis that demonstrate the superiority of the proposed model. Code and data will be released at https://github.com/PVIT-official/PVIT.'), Document(metadata={'title': 'MLLM-DataEngine: An Iterative Refinement Approach for MLLM'}, page_content='Despite the great advance of Multimodal Large Language Models (MLLMs) in both instruction dataset building and benchmarking, the independence of training and evaluation makes current MLLMs hard to further improve their capability under the guidance of evaluation results with a relatively low human cost. In this paper, we propose MLLM-DataEngine, a novel closed-loop system that bridges data generation, model training, and evaluation. Within each loop iteration, the MLLM-DataEngine first analyze the weakness of the model based on the evaluation results, then generate a proper incremental dataset for the next training iteration and enhance the model capability iteratively. Compared with previous data collection methods which are separate from the benchmarking, the data generated by MLLM-DataEngine shows better targeting, quality, and correctness. For targeting, we propose an Adaptive Bad-case Sampling module, which adjusts the ratio of different types of data within each incremental dataset based on the benchmarking results. For quality, we resort to GPT-4 to generate high-quality data with each given data type. For correctness, prompt design is critical for the data generation results. Rather than previous hand-crafted prompt, we propose an Interactive Prompt Optimization strategy, which optimizes the prompt with the multi-round interaction between human and GPT, and improve the correctness of generated data greatly. Through extensive experiments, we find our MLLM-DataEngine could boost the MLLM capability in a targeted and automatic manner, with only a few human participation. We hope it could be a general solution for the following MLLMs building. The MLLM-DataEngine has been open-sourced and is now available at https://github.com/opendatalab/MLLM-DataEngine.'), Document(metadata={'title': 'Socratis: Are large multimodal models emotionally aware?'}, page_content='Existing emotion prediction benchmarks contain coarse emotion labels which do not consider the diversity of emotions that an image and text can elicit in humans due to various reasons. Learning diverse reactions to multimodal content is important as intelligent machines take a central role in generating and delivering content to society. To address this gap, we propose Socratis, a societal reactions benchmark, where each image-caption (IC) pair is annotated with multiple emotions and the reasons for feeling them. Socratis contains 18K free-form reactions for 980 emotions on 2075 image-caption pairs from 5 widely-read news and image-caption (IC) datasets. We benchmark the capability of state-of-the-art multimodal large language models to generate the reasons for feeling an emotion given an IC pair. Based on a preliminary human study, we observe that humans prefer human-written reasons over 2 times more often than machine-generated ones. This shows our task is harder than standard generation tasks because it starkly contrasts recent findings where humans cannot tell apart machine vs human-written news articles, for instance. We further see that current captioning metrics based on large vision-language models also fail to correlate with human preferences. We hope that these findings and our benchmark will inspire further research on training emotionally aware models.'), Document(metadata={'title': 'MultiWay-Adapater: Adapting large-scale multi-modal models for scalable image-text retrieval'}, page_content=\"As Multimodal Large Language Models (MLLMs) grow in size, adapting them to specialized tasks becomes increasingly challenging due to high computational and memory demands. Indeed, traditional fine-tuning methods are costly, due to the need for extensive, task-specific training. While efficient adaptation methods exist that aim to reduce these costs, in practice they suffer from shallow inter-modal alignment, which severely hurts model effectiveness. To tackle these computational challenges and improve inter-modal alignment, we introduce the MultiWay-Adapter (MWA), a novel framework featuring an 'Alignment Enhancer'. This enhancer deepens inter-modal alignment, enabling high transferability with minimal tuning effort. Our experiments show that unlike prior efficient tuning approaches, MWA maintains model effectiveness, while reducing training time by up-to 57%. MWA is also lightweight, increasing model size by only 2-3% (in terms of parameters) for state-of-the-art foundation models like BEiT-3 Large. These results demonstrate that MWA provides an efficient and effective adaptation method for MLLMs, significantly broadening their applicability.\"), Document(metadata={'title': 'NExT-GPT: Any-to-Any Multimodal LLM'}, page_content='While recently Multimodal Large Language Models (MM-LLMs) have made exciting strides, they mostly fall prey to the limitation of only input-side multimodal understanding, without the ability to produce content in multiple modalities. As we humans always perceive the world and communicate with people through various modalities, developing any-to-any MM-LLMs capable of accepting and delivering content in any modality becomes essential to human-level AI. To fill the gap, we present an end-to-end general-purpose any-to-any MM-LLM system, NExT-GPT. We connect an LLM with multimodal adaptors and different diffusion decoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio. By leveraging the existing well-trained highly-performing encoders and decoders, NExT-GPT is tuned with only a small amount of parameter (1%) of certain projection layers, which not only benefits low-cost training and also facilitates convenient expansion to more potential modalities. Moreover, we introduce a modality-switching instruction tuning (MosIT) and manually curate a high-quality dataset for MosIT, based on which NExT-GPT is empowered with complex cross-modal semantic understanding and content generation. Overall, our research showcases the promising possibility of building an AI agent capable of modeling universal modalities, paving the way for more human-like AI research in the community. Project page: https://next-gpt.github.io/'), Document(metadata={'title': 'HiLM-D: Towards High-Resolution Understanding in Multimodal Large Language Models for Autonomous Driving'}, page_content=\"Autonomous driving systems generally employ separate models for different tasks resulting in intricate designs. For the first time, we leverage singular multimodal large language models (MLLMs) to consolidate multiple autonomous driving tasks from videos, i.e., the Risk Object Localization and Intention and Suggestion Prediction (ROLISP) task. ROLISP uses natural language to simultaneously identify and interpret risk objects, understand ego-vehicle intentions, and provide motion suggestions, eliminating the necessity for task-specific architectures. However, lacking high-resolution (HR) information, existing MLLMs often miss small objects (e.g., traffic cones) and overly focus on salient ones (e.g., large trucks) when applied to ROLISP. We propose HiLM-D (Towards High-Resolution Understanding in MLLMs for Autonomous Driving), an efficient method to incorporate HR information into MLLMs for the ROLISP task. Especially, HiLM-D integrates two branches: (i) the low-resolution reasoning branch, can be any MLLMs, processes low-resolution videos to caption risk objects and discern ego-vehicle intentions/suggestions; (ii) the high-resolution perception branch (HR-PB), prominent to HiLM-D,, ingests HR images to enhance detection by capturing vision-specific HR feature maps and prioritizing all potential risks over merely salient objects. Our HR-PB serves as a plug-and-play module, seamlessly fitting into current MLLMs. Experiments on the ROLISP benchmark reveal HiLM-D's notable advantage over leading MLLMs, with improvements of 4.8% in BLEU-4 for captioning and 17.2% in mIoU for detection.\"), Document(metadata={'title': 'Investigating the Catastrophic Forgetting in Multimodal Large Language Models'}, page_content='Following the success of GPT4, there has been a surge in interest in multimodal large language model (MLLM) research. This line of research focuses on developing general-purpose LLMs through fine-tuning pre-trained LLMs and vision models. However, catastrophic forgetting, a notorious phenomenon where the fine-tuned model fails to retain similar performance compared to the pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM). In this paper, we introduce EMT: Evaluating MulTimodality for evaluating the catastrophic forgetting in MLLMs, by treating each MLLM as an image classifier. We first apply EMT to evaluate several open-source fine-tuned MLLMs and we discover that almost all evaluated MLLMs fail to retain the same performance levels as their vision encoders on standard image classification tasks. Moreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess performance throughout the fine-tuning. Interestingly, our results suggest that early-stage fine-tuning on an image dataset improves performance across other image datasets, by enhancing the alignment of text and visual features. However, as fine-tuning proceeds, the MLLMs begin to hallucinate, resulting in a significant loss of generalizability, even when the image encoder remains frozen. Our results suggest that MLLMs have yet to demonstrate performance on par with their vision models on standard image classification tasks and the current MLLM fine-tuning procedure still has room for improvement.'), Document(metadata={'title': 'DreamLLM: Synergistic Multimodal Comprehension and Creation'}, page_content=\"This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy. Project page: https://dreamllm.github.io.\"), Document(metadata={'title': \"How Robust is Google's Bard to Adversarial Image Attacks?\"}, page_content=\"Multimodal Large Language Models (MLLMs) that integrate text and other modalities (especially vision) have achieved unprecedented performance in various multimodal tasks. However, due to the unsolved adversarial robustness problem of vision models, MLLMs can have more severe safety and security risks by introducing the vision inputs. In this work, we study the adversarial robustness of Google's Bard, a competitive chatbot to ChatGPT that released its multimodal capability recently, to better understand the vulnerabilities of commercial MLLMs. By attacking white-box surrogate vision encoders or MLLMs, the generated adversarial examples can mislead Bard to output wrong image descriptions with a 22% success rate based solely on the transferability. We show that the adversarial examples can also attack other MLLMs, e.g., a 26% attack success rate against Bing Chat and a 86% attack success rate against ERNIE bot. Moreover, we identify two defense mechanisms of Bard, including face detection and toxicity detection of images. We design corresponding attacks to evade these defenses, demonstrating that the current defenses of Bard are also vulnerable. We hope this work can deepen our understanding on the robustness of MLLMs and facilitate future research on defenses. Our code is available at https://github.com/thu-ml/Attack-Bard. Update: GPT-4V is available at October 2023. We further evaluate its robustness under the same set of adversarial examples, achieving a 45% attack success rate.\"), Document(metadata={'title': 'VDC: Versatile Data Cleanser based on Visual-Linguistic Inconsistency by Multimodal Large Language Models'}, page_content='The role of data in building AI systems has recently been emphasized by the emerging concept of data-centric AI. Unfortunately, in the real-world, datasets may contain dirty samples, such as poisoned samples from backdoor attack, noisy labels in crowdsourcing, and even hybrids of them. The presence of such dirty samples makes the DNNs vunerable and unreliable.Hence, it is critical to detect dirty samples to improve the quality and realiability of dataset. Existing detectors only focus on detecting poisoned samples or noisy labels, that are often prone to weak generalization when dealing with dirty samples from other domains.In this paper, we find a commonality of various dirty samples is visual-linguistic inconsistency between images and associated labels. To capture the semantic inconsistency between modalities, we propose versatile data cleanser (VDC) leveraging the surpassing capabilities of multimodal large language models (MLLM) in cross-modal alignment and reasoning.It consists of three consecutive modules: the visual question generation module to generate insightful questions about the image; the visual question answering module to acquire the semantics of the visual content by answering the questions with MLLM; followed by the visual answer evaluation module to evaluate the inconsistency.Extensive experiments demonstrate its superior performance and generalization to various categories and types of dirty samples. The code is available at \\\\url{https://github.com/zihao-ai/vdc}.'), Document(metadata={'title': 'Guiding Instruction-based Image Editing via Multimodal Large Language Models'}, page_content='Instruction-based image editing improves the controllability and flexibility of image manipulation via natural commands without elaborate descriptions or regional masks. However, human instructions are sometimes too brief for current methods to capture and follow. Multimodal large language models (MLLMs) show promising capabilities in cross-modal understanding and visual-aware response generation via LMs. We investigate how MLLMs facilitate edit instructions and present MLLM-Guided Image Editing (MGIE). MGIE learns to derive expressive instructions and provides explicit guidance. The editing model jointly captures this visual imagination and performs manipulation through end-to-end training. We evaluate various aspects of Photoshop-style modification, global photo optimization, and local editing. Extensive experimental results demonstrate that expressive instructions are crucial to instruction-based image editing, and our MGIE can lead to a notable improvement in automatic metrics and human evaluation while maintaining competitive inference efficiency.'), Document(metadata={'title': 'Reformulating Vision-Language Foundation Models and Datasets Towards Universal Multimodal Assistants'}, page_content=\"Recent Multimodal Large Language Models (MLLMs) exhibit impressive abilities to perceive images and follow open-ended instructions. The capabilities of MLLMs depend on two crucial factors: the model architecture to facilitate the feature alignment of visual modules and large language models; the multimodal instruction tuning datasets for human instruction following. (i) For the model architecture, most existing models introduce an external bridge module to connect vision encoders with language models, which needs an additional feature-alignment pre-training. In this work, we discover that compact pre-trained vision language models can inherently serve as ``out-of-the-box'' bridges between vision and language. Based on this, we propose Muffin framework, which directly employs pre-trained vision-language models to act as providers of visual signals. (ii) For the multimodal instruction tuning datasets, existing methods omit the complementary relationship between different datasets and simply mix datasets from different tasks. Instead, we propose UniMM-Chat dataset which explores the complementarities of datasets to generate 1.1M high-quality and diverse multimodal instructions. We merge information describing the same image from diverse datasets and transforms it into more knowledge-intensive conversation data. Experimental results demonstrate the effectiveness of the Muffin framework and UniMM-Chat dataset. Muffin achieves state-of-the-art performance on a wide range of vision-language tasks, significantly surpassing state-of-the-art models like LLaVA and InstructBLIP. Our model and dataset are all accessible at https://github.com/thunlp/muffin.\"), Document(metadata={'title': 'DriveGPT4: Interpretable End-to-end Autonomous Driving via Large Language Model'}, page_content='Multimodal large language models (MLLMs) have emerged as a prominent area of interest within the research community, given their proficiency in handling and reasoning with non-textual data, including images and videos. This study seeks to extend the application of MLLMs to the realm of autonomous driving by introducing DriveGPT4, a novel interpretable end-to-end autonomous driving system based on LLMs. Capable of processing multi-frame video inputs and textual queries, DriveGPT4 facilitates the interpretation of vehicle actions, offers pertinent reasoning, and effectively addresses a diverse range of questions posed by users. Furthermore, DriveGPT4 predicts low-level vehicle control signals in an end-to-end fashion.These advanced capabilities are achieved through the utilization of a bespoke visual instruction tuning dataset, specifically tailored for autonomous driving applications, in conjunction with a mix-finetuning training strategy. DriveGPT4 represents the pioneering effort to leverage LLMs for the development of an interpretable end-to-end autonomous driving solution. Evaluations conducted on the BDD-X dataset showcase the superior qualitative and quantitative performance of DriveGPT4. Additionally, the fine-tuning of domain-specific data enables DriveGPT4 to yield close or even improved results in terms of autonomous driving grounding when contrasted with GPT4-V.'), Document(metadata={'title': 'MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens'}, page_content='The effectiveness of Multimodal Large Language Models (MLLMs) demonstrates a profound capability in multimodal understanding. However, the simultaneous generation of images with coherent texts is still underdeveloped. Addressing this, we introduce a novel interleaved vision-and-language generation method, centered around the concept of ``generative vokens\". These vokens serve as pivotal elements contributing to coherent image-text outputs. Our method is marked by a unique two-stage training strategy for description-free multimodal generation, which does not necessitate extensive descriptions of images. We integrate classifier-free guidance to enhance the alignment of generated images and texts, ensuring more seamless and contextually relevant multimodal interactions. Our model, MiniGPT-5, exhibits substantial improvement over the baseline models on multimodal generation datasets, including MMDialog and VIST. The human evaluation shows MiniGPT-5 is better than the baseline model on more than 56\\\\% cases for multimodal generation, highlighting its efficacy across diverse benchmarks.'), Document(metadata={'title': 'Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond'}, page_content='In this study, we explore the potential of Multimodal Large Language Models (MLLMs) in improving embodied decision-making processes for agents. While Large Language Models (LLMs) have been widely used due to their advanced reasoning skills and vast world knowledge, MLLMs like GPT4-Vision offer enhanced visual understanding and reasoning capabilities. We investigate whether state-of-the-art MLLMs can handle embodied decision-making in an end-to-end manner and whether collaborations between LLMs and MLLMs can enhance decision-making. To address these questions, we introduce a new benchmark called PCA-EVAL, which evaluates embodied decision-making from the perspectives of Perception, Cognition, and Action. Additionally, we propose HOLMES, a multi-agent cooperation framework that allows LLMs to leverage MLLMs and APIs to gather multimodal information for informed decision-making. We compare end-to-end embodied decision-making and HOLMES on our benchmark and find that the GPT4-Vision model demonstrates strong end-to-end embodied decision-making abilities, outperforming GPT4-HOLMES in terms of average decision accuracy (+3%). However, this performance is exclusive to the latest GPT4-Vision model, surpassing the open-source state-of-the-art MLLM by 26%. Our results indicate that powerful MLLMs like GPT4-Vision hold promise for decision-making in embodied agents, offering new avenues for MLLM research. Code and data are open at https://github.com/pkunlp-icler/PCA-EVAL/.'), Document(metadata={'title': 'Kosmos-G: Generating Images in Context with Multimodal Large Language Models'}, page_content='Recent advancements in subject-driven image generation have made significant strides. However, current methods still fall short in diverse application scenarios, as they require test-time tuning and cannot accept interleaved multi-image and text input. These limitations keep them far from the ultimate goal of \"image as a foreign language in image generation.\" This paper presents Kosmos-G, a model that leverages the advanced multimodal perception capabilities of Multimodal Large Language Models (MLLMs) to tackle the aforementioned challenge. Our approach aligns the output space of MLLM with CLIP using the textual modality as an anchor and performs compositional instruction tuning on curated data. Kosmos-G demonstrates an impressive capability of zero-shot subject-driven generation with interleaved multi-image and text input. Notably, the score distillation instruction tuning requires no modifications to the image decoder. This allows for a seamless substitution of CLIP and effortless integration with a myriad of U-Net techniques ranging from fine-grained controls to personalized image decoder variants. We posit Kosmos-G as an initial attempt towards the goal of \"image as a foreign language in image generation.\" The code can be found at https://aka.ms/Kosmos-G'), Document(metadata={'title': 'Fine-grained Audio-Visual Joint Representations for Multimodal Large Language Models'}, page_content='Audio-visual large language models (LLM) have drawn significant attention, yet the fine-grained combination of both input streams is rather under-explored, which is challenging but necessary for LLMs to understand general video inputs. To this end, a fine-grained audio-visual joint representation (FAVOR) learning framework for multimodal LLMs is proposed in this paper, which extends a text-based LLM to simultaneously perceive speech and audio events in the audio input stream and images or videos in the visual input stream, at the frame level. To fuse the audio and visual feature streams into joint representations and to align the joint space with the LLM input embedding space, we propose a causal Q-Former structure with a causal attention module to enhance the capture of causal relations of the audio-visual frames across time. An audio-visual evaluation benchmark (AVEB) is also proposed which comprises six representative single-modal tasks with five cross-modal tasks reflecting audio-visual co-reasoning abilities. While achieving competitive single-modal performance on audio, speech and image tasks in AVEB, FAVOR achieved over 20% accuracy improvements on the video question-answering task when fine-grained information or temporal causal reasoning is required. FAVOR, in addition, demonstrated remarkable video comprehension and reasoning abilities on tasks that are unprecedented by other multimodal LLMs. An interactive demo of FAVOR is available at https://github.com/BriansIDP/AudioVisualLLM.git, and the training code and model checkpoints will be released soon.'), Document(metadata={'title': 'Can We Edit Multimodal Large Language Models?'}, page_content='In this paper, we focus on editing Multimodal Large Language Models (MLLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights. Code and dataset are available in https://github.com/zjunlp/EasyEdit.'), Document(metadata={'title': 'AutoRepo: A general framework for multi-modal LLM-based automated construction reporting'}, page_content='Ensuring the safety, quality, and timely completion of construction projects is paramount, with construction inspections serving as a vital instrument towards these goals. Nevertheless, the predominantly manual approach of present-day inspections frequently results in inefficiencies and inadequate information management. Such methods often fall short of providing holistic, exhaustive assessments, consequently engendering regulatory oversights and potential safety hazards. To address this issue, this paper presents a novel framework named AutoRepo for automated generation of construction inspection reports. The unmanned vehicles efficiently perform construction inspections and collect scene information, while the multimodal large language models (LLMs) are leveraged to automatically generate the inspection reports. The framework was applied and tested on a real-world construction site, demonstrating its potential to expedite the inspection process, significantly reduce resource allocation, and produce high-quality, regulatory standard-compliant inspection reports. This research thus underscores the immense potential of multimodal large language models in revolutionizing construction inspection practices, signaling a significant leap forward towards a more efficient and safer construction management paradigm.'), Document(metadata={'title': 'MM-BigBench: Evaluating Multimodal Models on Multimodal Content Comprehension Tasks'}, page_content=\"The popularity of multimodal large language models (MLLMs) has triggered a recent surge in research efforts dedicated to evaluating these models. Nevertheless, existing evaluation studies of MLLMs primarily focus on the comprehension and reasoning of unimodal (vision) content, neglecting performance evaluations in the domain of multimodal (vision-language) content understanding. Beyond multimodal reasoning, tasks related to multimodal content comprehension necessitate a profound understanding of multimodal contexts, achieved through the multimodal interaction to obtain a final answer. In this paper, we introduce a comprehensive assessment framework called MM-BigBench, which incorporates a diverse range of metrics to offer an extensive evaluation of the performance of various models and instructions across a wide spectrum of diverse multimodal content comprehension tasks. Consequently, our work complements research on the performance of MLLMs in multimodal comprehension tasks, achieving a more comprehensive and holistic evaluation of MLLMs. To begin, we employ the Best Performance metric to ascertain each model's performance upper bound on different datasets. Subsequently, the Mean Relative Gain metric offers an assessment of the overall performance of various models and instructions, while the Stability metric measures their sensitivity. Furthermore, previous research centers on evaluating models independently or solely assessing instructions, neglecting the adaptability between models and instructions. We propose the Adaptability metric to quantify the adaptability between models and instructions. Our paper evaluates a total of 20 language models (14 MLLMs) on 14 multimodal datasets spanning 6 tasks, with 10 instructions for each task, and derives novel insights. Our code will be released at https://github.com/declare-lab/MM-BigBench.\"), Document(metadata={'title': 'RoboLLM: Robotic Vision Tasks Grounded on Multimodal Large Language Models'}, page_content='Robotic vision applications often necessitate a wide range of visual perception tasks, such as object detection, segmentation, and identification. While there have been substantial advances in these individual tasks, integrating specialized models into a unified vision pipeline presents significant engineering challenges and costs. Recently, Multimodal Large Language Models (MLLMs) have emerged as novel backbones for various downstream tasks. We argue that leveraging the pre-training capabilities of MLLMs enables the creation of a simplified framework, thus mitigating the need for task-specific encoders. Specifically, the large-scale pretrained knowledge in MLLMs allows for easier fine-tuning to downstream robotic vision tasks and yields superior performance. We introduce the RoboLLM framework, equipped with a BEiT-3 backbone, to address all visual perception tasks in the ARMBench challenge-a large-scale robotic manipulation dataset about real-world warehouse scenarios. RoboLLM not only outperforms existing baselines but also substantially reduces the engineering burden associated with model selection and tuning. The source code is publicly available at https://github.com/longkukuhi/armbench.'), Document(metadata={'title': 'Benchmarking Sequential Visual Input Reasoning and Prediction in Multimodal Large Language Models'}, page_content=\"Multimodal large language models (MLLMs) have shown great potential in perception and interpretation tasks, but their capabilities in predictive reasoning remain under-explored. To address this gap, we introduce a novel benchmark that assesses the predictive reasoning capabilities of MLLMs across diverse scenarios. Our benchmark targets three important domains: abstract pattern reasoning, human activity prediction, and physical interaction prediction. We further develop three evaluation methods powered by large language model to robustly quantify a model's performance in predicting and reasoning the future based on multi-visual context. Empirical experiments confirm the soundness of the proposed benchmark and evaluation methods via rigorous testing and reveal pros and cons of current popular MLLMs in the task of predictive reasoning. Lastly, our proposed benchmark provides a standardized evaluation framework for MLLMs and can facilitate the development of more advanced models that can reason and predict over complex long sequence of multimodal input.\"), Document(metadata={'title': 'Woodpecker: Hallucination Correction for Multimodal Large Language Models'}, page_content='Hallucination is a big shadow hanging over the rapidly evolving Multimodal Large Language Models (MLLMs), referring to the phenomenon that the generated text is inconsistent with the image content. In order to mitigate hallucinations, existing studies mainly resort to an instruction-tuning manner that requires retraining the models with specific data. In this paper, we pave a different way, introducing a training-free method named Woodpecker. Like a woodpecker heals trees, it picks out and corrects hallucinations from the generated text. Concretely, Woodpecker consists of five stages: key concept extraction, question formulation, visual knowledge validation, visual claim generation, and hallucination correction. Implemented in a post-remedy manner, Woodpecker can easily serve different MLLMs, while being interpretable by accessing intermediate outputs of the five stages. We evaluate Woodpecker both quantitatively and qualitatively and show the huge potential of this new paradigm. On the POPE benchmark, our method obtains a 30.66%/24.33% improvement in accuracy over the baseline MiniGPT-4/mPLUG-Owl. The source code is released at https://github.com/BradyFU/Woodpecker.'), Document(metadata={'title': 'Towards Perceiving Small Visual Details in Zero-shot Visual Question Answering with Multimodal LLMs'}, page_content=\"Multimodal Large Language Models (MLLMs) have recently achieved promising zero-shot accuracy on visual question answering (VQA) -- a fundamental task affecting various downstream applications and domains. Given the great potential for the broad use of these models, it is important to investigate their limitations in dealing with different image and question properties. In this work, we investigate whether MLLMs can perceive small details as well as large details in images. In particular, we show that their zero-shot accuracy in answering visual questions is very sensitive to the size of the visual subject of the question, declining up to 46% with size. Furthermore, we show that this effect is causal by observing that human visual cropping can significantly mitigate their sensitivity to size. Inspired by the usefulness of human cropping, we then propose five automatic visual cropping methods -- leveraging either external localization models or the decision process of the given MLLM itself -- as inference time mechanisms to improve the zero-shot performance of MLLMs. We study their effectiveness on four popular VQA datasets, and a subset of the VQAv2 dataset tailored towards fine visual details. Our findings suggest that MLLMs should be used with caution in detail-sensitive VQA applications, and that visual cropping is a promising direction to improve their zero-shot performance. To facilitate further investigation of MLLMs' behaviors, our code and data are publicly released.\"), Document(metadata={'title': 'ChEF: A Comprehensive Evaluation Framework for Standardized Assessment of Multimodal Large Language Models'}, page_content=\"Multimodal Large Language Models (MLLMs) have shown impressive abilities in interacting with visual content with myriad potential downstream tasks. However, even though a list of benchmarks has been proposed, the capabilities and limitations of MLLMs are still not comprehensively understood, due to a lack of a standardized and holistic evaluation framework. To this end, we present the first Comprehensive Evaluation Framework (ChEF) that can holistically profile each MLLM and fairly compare different MLLMs. First, we structure ChEF as four modular components, i.e., Scenario as scalable multimodal datasets, Instruction as flexible instruction retrieving formulae, Inferencer as reliable question answering strategies, and Metric as indicative task-specific score functions. Based on them, ChEF facilitates versatile evaluations in a standardized framework, and new evaluations can be built by designing new Recipes (systematic selection of these four components). Notably, current MLLM benchmarks can be readily summarized as recipes of ChEF. Second, we introduce 6 new recipes to quantify competent MLLMs' desired capabilities (or called desiderata, i.e., calibration, in-context learning, instruction following, language performance, hallucination, and robustness) as reliable agents that can perform real-world multimodal interactions. Third, we conduct a large-scale evaluation of 9 prominent MLLMs on 9 scenarios and 6 desiderata. Our evaluation summarized over 20 valuable observations concerning the generalizability of MLLMs across various scenarios and the composite capability of MLLMs required for multimodal interactions. We will publicly release all the detailed implementations for further analysis, as well as an easy-to-use modular toolkit for the integration of new recipes and models, so that ChEF can be a growing evaluation framework for the MLLM community.\"), Document(metadata={'title': 'Octavius: Mitigating Task Interference in MLLMs via LoRA-MoE'}, page_content='Recent studies have demonstrated Large Language Models (LLMs) can extend their zero-shot generalization capabilities to multimodal learning through instruction tuning. As more modalities and downstream tasks are introduced, negative conflicts and interference may have a worse impact on performance. While this phenomenon has been overlooked in previous work, we propose a novel and extensible framework, called Octavius, for comprehensive studies and experimentation on multimodal learning with Multimodal Large Language Models (MLLMs). Specifically, we combine the well-known Mixture-of-Experts (MoE) and one of the representative PEFT techniques, i.e., LoRA, designing a novel LLM-based decoder, called LoRA-MoE, for multimodal learning. To the best of our knowledge, we are one of the pioneering efforts to introduce MoE into MLLMs to address this problem. The experimental results (about 20% improvement) have shown the effectiveness and versatility of our design in various 2D and 3D downstream tasks. Code and datasets are available at https://openlamm.github.io/tutorial/.'), Document(metadata={'title': 'What Large Language Models Bring to Text-rich VQA?'}, page_content=\"Text-rich VQA, namely Visual Question Answering based on text recognition in the images, is a cross-modal task that requires both image comprehension and text recognition. In this work, we focus on investigating the advantages and bottlenecks of LLM-based approaches in addressing this problem. To address the above concern, we separate the vision and language modules, where we leverage external OCR models to recognize texts in the image and Large Language Models (LLMs) to answer the question given texts. The whole framework is training-free benefiting from the in-context ability of LLMs. This pipeline achieved superior performance compared to the majority of existing Multimodal Large Language Models (MLLM) on four text-rich VQA datasets. Besides, based on the ablation study, we find that LLM brings stronger comprehension ability and may introduce helpful knowledge for the VQA problem. The bottleneck for LLM to address text-rich VQA problems may primarily lie in visual part. We also combine the OCR module with MLLMs and pleasantly find that the combination of OCR module with MLLM also works. It's worth noting that not all MLLMs can comprehend the OCR information, which provides insights into how to train an MLLM that preserves the abilities of LLM.\")], 'results': [Document(id='9e9ac81d-692f-441b-8926-4b6ba73ad784', metadata={'title': 'A Survey on Multimodal Large Language Models'}, page_content='Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.'), Document(id='128c9e1b-adb3-49e0-a873-881354331b12', metadata={'title': 'A Survey on Multimodal Large Language Models'}, page_content='Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.'), Document(id='01a7f21a-97a8-45a2-a73c-74526485470f', metadata={'title': 'A Survey on Multimodal Large Language Models'}, page_content='Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.')], 'context': '[1] A Survey on Multimodal Large Language Models\\nRecently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.\\n\\n[2] A Survey on Multimodal Large Language Models\\nRecently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.\\n\\n[3] A Survey on Multimodal Large Language Models\\nRecently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.', 'final_answer': \"Multimodal Large Language Models (MLLMs) are a growing area of research, exemplified by models like GPT-4V that leverage LLMs for multimodal tasks [1, 2, 3]. These models demonstrate new capabilities such as generating stories from images and performing math reasoning without OCR, which are not typical of traditional multimodal approaches, hinting at a potential path toward artificial general intelligence [1, 2, 3]. Current research is heavily focused on creating MLLMs that can rival or surpass GPT-4V [1, 2, 3]. Key areas of investigation include MLLM architecture, training methodologies, data utilization, and evaluation techniques [1, 2, 3]. Furthermore, research is exploring how to broaden MLLMs' capabilities to encompass finer granularity, a wider range of modalities and languages, and diverse application scenarios. This includes tackling challenges like multimodal hallucination and incorporating advanced techniques like Multimodal In-Context Learning (M-ICL), Multimodal Chain-of-Thought (M-CoT), and LLM-Aided Visual Reasoning (LAVR) [1, 2, 3].\", 'expert_opinion': 'Multimodal Large Language Models (MLLMs) are a rising research area, with models like GPT-4V using LLMs to perform multimodal tasks [1, 2, 3]. These models exhibit emergent capabilities like image-based story writing and OCR-free math reasoning, which are uncommon in traditional multimodal methods, suggesting a possible route to artificial general intelligence [1, 2, 3]. Research is focused on developing MLLMs that can compete with or outperform GPT-4V [1, 2, 3]. The key aspects of MLLMs under investigation include architecture, training strategies, data, and evaluation methods [1, 2, 3]. Research also explores expanding MLLMs to support more granularity, modalities, languages, and scenarios, as well as addressing multimodal hallucination and incorporating techniques like Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR) [1, 2, 3].', 'reviewer_opinion': \"The expert's response accurately summarizes the information provided in the context documents. All claims are supported by citations to the provided documents. The response directly addresses the user's query about multimodal LLMs, avoiding the information the user specified they were not interested in.\", 'messages': [AIMessage(content='Retrieving expert insights based on the papers and cite the papers...', additional_kwargs={}, response_metadata={}, id='22fe505c-90a0-4118-95eb-6045f2c066c7'), HumanMessage(content=\"Context: [1] A Survey on Multimodal Large Language Models\\nRecently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.\\n\\n[2] A Survey on Multimodal Large Language Models\\nRecently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.\\n\\n[3] A Survey on Multimodal Large Language Models\\nRecently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.\\n\\nQuery: latest advances in multimodal learning Consider User's response for the search query: I dont want to know about latest advancements, I want to know about multimodal llms\", additional_kwargs={}, response_metadata={}, id='c9f3c950-8322-4da3-a95b-1115429cd167'), AIMessage(content='Multimodal Large Language Models (MLLMs) are a rising research area, with models like GPT-4V using LLMs to perform multimodal tasks [1, 2, 3]. These models exhibit emergent capabilities like image-based story writing and OCR-free math reasoning, which are uncommon in traditional multimodal methods, suggesting a possible route to artificial general intelligence [1, 2, 3]. Research is focused on developing MLLMs that can compete with or outperform GPT-4V [1, 2, 3]. The key aspects of MLLMs under investigation include architecture, training strategies, data, and evaluation methods [1, 2, 3]. Research also explores expanding MLLMs to support more granularity, modalities, languages, and scenarios, as well as addressing multimodal hallucination and incorporating techniques like Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR) [1, 2, 3].', additional_kwargs={}, response_metadata={}, id='5d83e61e-b24a-4136-81d1-2c670290ed0f'), AIMessage(content=\"🧪 Reviewing the expert's findings for validity...\", additional_kwargs={}, response_metadata={}, id='53d69017-f1f4-47e6-bb32-dd68bf9bc464'), HumanMessage(content='Expert said:\\nMultimodal Large Language Models (MLLMs) are a rising research area, with models like GPT-4V using LLMs to perform multimodal tasks [1, 2, 3]. These models exhibit emergent capabilities like image-based story writing and OCR-free math reasoning, which are uncommon in traditional multimodal methods, suggesting a possible route to artificial general intelligence [1, 2, 3]. Research is focused on developing MLLMs that can compete with or outperform GPT-4V [1, 2, 3]. The key aspects of MLLMs under investigation include architecture, training strategies, data, and evaluation methods [1, 2, 3]. Research also explores expanding MLLMs to support more granularity, modalities, languages, and scenarios, as well as addressing multimodal hallucination and incorporating techniques like Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR) [1, 2, 3].', additional_kwargs={}, response_metadata={}, id='f5d7f20d-0d50-4ae5-98b5-9d750c4eaa60'), AIMessage(content=\"The expert's response accurately summarizes the information provided in the context documents. All claims are supported by citations to the provided documents. The response directly addresses the user's query about multimodal LLMs, avoiding the information the user specified they were not interested in.\", additional_kwargs={}, response_metadata={}, id='da43df89-5565-45f7-a7ca-2cadfe5fe841'), AIMessage(content='🧠 Synthesizing expert and reviewer input...', additional_kwargs={}, response_metadata={}, id='a8c88e72-36ea-42e5-b04d-989449074e06'), HumanMessage(content=\"Expert Opinion:\\nMultimodal Large Language Models (MLLMs) are a rising research area, with models like GPT-4V using LLMs to perform multimodal tasks [1, 2, 3]. These models exhibit emergent capabilities like image-based story writing and OCR-free math reasoning, which are uncommon in traditional multimodal methods, suggesting a possible route to artificial general intelligence [1, 2, 3]. Research is focused on developing MLLMs that can compete with or outperform GPT-4V [1, 2, 3]. The key aspects of MLLMs under investigation include architecture, training strategies, data, and evaluation methods [1, 2, 3]. Research also explores expanding MLLMs to support more granularity, modalities, languages, and scenarios, as well as addressing multimodal hallucination and incorporating techniques like Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR) [1, 2, 3].\\n\\nReviewer Feedback:\\nThe expert's response accurately summarizes the information provided in the context documents. All claims are supported by citations to the provided documents. The response directly addresses the user's query about multimodal LLMs, avoiding the information the user specified they were not interested in.\\n\\nFinal Question: latest advances in multimodal learning Consider User's response for the search query: I dont want to know about latest advancements, I want to know about multimodal llms\", additional_kwargs={}, response_metadata={}, id='8839537b-c869-4fa8-89ed-7e6e2e068c3f'), AIMessage(content=\"Multimodal Large Language Models (MLLMs) are a growing area of research, exemplified by models like GPT-4V that leverage LLMs for multimodal tasks [1, 2, 3]. These models demonstrate new capabilities such as generating stories from images and performing math reasoning without OCR, which are not typical of traditional multimodal approaches, hinting at a potential path toward artificial general intelligence [1, 2, 3]. Current research is heavily focused on creating MLLMs that can rival or surpass GPT-4V [1, 2, 3]. Key areas of investigation include MLLM architecture, training methodologies, data utilization, and evaluation techniques [1, 2, 3]. Furthermore, research is exploring how to broaden MLLMs' capabilities to encompass finer granularity, a wider range of modalities and languages, and diverse application scenarios. This includes tackling challenges like multimodal hallucination and incorporating advanced techniques like Multimodal In-Context Learning (M-ICL), Multimodal Chain-of-Thought (M-CoT), and LLM-Aided Visual Reasoning (LAVR) [1, 2, 3].\", additional_kwargs={}, response_metadata={}, id='3d559587-e7b5-43df-b4a6-0eb472a47d39')], 'retry_count': 2}\n{'query': \"latest advances in multimodal learning Consider User's response for the search query: I dont want to know about latest advancements, I want to know about multimodal llms\", 'search_query': 'multimodal large language models', 'papers': [{'id': 'multimodal-large-language-models-for', 'arxiv_id': None, 'nips_id': None, 'url_abs': 'https://aclanthology.org/2022.naacl-srw.26', 'url_pdf': 'https://aclanthology.org/2022.naacl-srw.26.pdf', 'title': 'Multimodal large language models for inclusive collaboration learning tasks', 'abstract': 'This PhD project leverages advancements in multimodal large language models to build an inclusive collaboration feedback loop, in order to facilitate the automated detection, modeling, and feedback for participants developing general collaboration skills. This topic is important given the role of collaboration as an essential 21st century skill, the potential to ground large language models within learning theory and real-world practice, and the expressive potential of transformer models to support equity and inclusion. We address some concerns of integrating advances in natural language processing into downstream tasks such as the learning analytics feedback loop.', 'authors': ['Armanda Lewis'], 'published': None, 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': 'naacl-acl-2022-7'}, {'id': 'personality-aware-human-centric-multimodal', 'arxiv_id': '2304.02313', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2304.02313v2', 'url_pdf': 'https://arxiv.org/pdf/2304.02313v2.pdf', 'title': 'Personality-aware Human-centric Multimodal Reasoning: A New Task, Dataset and Baselines', 'abstract': \"Personality traits, emotions, and beliefs shape individuals' behavioral choices and decision-making processes. However, for one thing, the affective computing community normally focused on predicting personality traits but overlooks their application in behavior prediction. For another, the multimodal reasoning task emphasized the prediction of future states and behaviors but often neglected the incorporation of individual personality traits. In this work, we introduce a new task called Personality-aware Human-centric Multimodal Reasoning (PHMR) (T1), with the goal of forecasting the future behavior of a particular individual using multimodal information from past instances, while integrating personality factors. We accordingly construct a new dataset based on six television shows, encompassing 225 characters and 12k samples. To establish a benchmark for the task, we propose seven baseline methods: three adapted from related tasks, two pre-trained model, and two multimodal large language models. The experimental results demonstrate that incorporating personality traits enhances human-centric multimodal reasoning performance. To further solve the lack of personality annotation in real-life scenes, we introduce an extension task called Personality-predicted Human-centric Multimodal Reasoning task (T2) along with the corresponding dataset and method. We will make our dataset and code available on GitHub.\", 'authors': ['Rui Xia', 'Xiangqing Shen', 'Yaochen Zhu'], 'published': '2023-04-05', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'visual-transformation-telling', 'arxiv_id': '2305.01928', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2305.01928v2', 'url_pdf': 'https://arxiv.org/pdf/2305.01928v2.pdf', 'title': 'Visual Transformation Telling', 'abstract': 'Humans can naturally reason from superficial state differences (e.g. ground wetness) to transformations descriptions (e.g. raining) according to their life experience. In this paper, we propose a new visual reasoning task to test this transformation reasoning ability in real-world scenarios, called \\\\textbf{V}isual \\\\textbf{T}ransformation \\\\textbf{T}elling (VTT). Given a series of states (i.e. images), VTT requires to describe the transformation occurring between every two adjacent states. Different from existing visual reasoning tasks that focus on surface state reasoning, the advantage of VTT is that it captures the underlying causes, e.g. actions or events, behind the differences among states. We collect a novel dataset to support the study of transformation reasoning from two existing instructional video datasets, CrossTask and COIN, comprising 13,547 samples. Each sample involves the key state images along with their transformation descriptions. Our dataset covers diverse real-world activities, providing a rich resource for training and evaluation. To construct an initial benchmark for VTT, we test several models, including traditional visual storytelling methods (CST, GLACNet, Densecap) and advanced multimodal large language models (LLaVA v1.5-7B, Qwen-VL-chat, Gemini Pro Vision, GPT-4o, and GPT-4). Experimental results reveal that even state-of-the-art models still face challenges in VTT, highlighting substantial areas for improvement.', 'authors': ['Liang Pang', 'Xin Hong', 'Wanqing Cui', 'Xueqi Cheng', 'Jiafeng Guo', 'Yanyan Lan'], 'published': '2023-05-03', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'listen-think-and-understand', 'arxiv_id': '2305.10790', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2305.10790v3', 'url_pdf': 'https://arxiv.org/pdf/2305.10790v3.pdf', 'title': 'Listen, Think, and Understand', 'abstract': 'The ability of artificial intelligence (AI) systems to perceive and comprehend audio signals is crucial for many applications. Although significant progress has been made in this area since the development of AudioSet, most existing models are designed to map audio inputs to pre-defined, discrete sound label sets. In contrast, humans possess the ability to not only classify sounds into general categories, but also to listen to the finer details of the sounds, explain the reason for the predictions, think about what the sound infers, and understand the scene and what action needs to be taken, if any. Such capabilities beyond perception are not yet present in existing audio models. On the other hand, modern large language models (LLMs) exhibit emerging reasoning ability but they lack audio perception capabilities. Therefore, we ask the question: can we build a model that has both audio perception and a reasoning ability? In this paper, we propose a new audio foundation model, called LTU (Listen, Think, and Understand). To train LTU, we created a new OpenAQA-5M dataset consisting of 1.9 million closed-ended and 3.7 million open-ended, diverse (audio, question, answer) tuples, and have used an autoregressive training framework with a perception-to-understanding curriculum. LTU demonstrates strong performance and generalization ability on conventional audio tasks such as classification and captioning. More importantly, it exhibits emerging audio reasoning and comprehension abilities that are absent in existing audio models. To the best of our knowledge, LTU is one of the first multimodal large language models that focus on general audio (rather than just speech) understanding.', 'authors': ['James Glass', 'Leonid Karlinsky', 'Alexander H. Liu', 'Hongyin Luo', 'Yuan Gong'], 'published': '2023-05-18', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'what-makes-for-good-visual-tokenizers-for', 'arxiv_id': '2305.12223', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2305.12223v2', 'url_pdf': 'https://arxiv.org/pdf/2305.12223v2.pdf', 'title': 'What Makes for Good Visual Tokenizers for Large Language Models?', 'abstract': 'We empirically investigate proper pre-training methods to build good visual tokenizers, making Large Language Models (LLMs) powerful Multimodal Large Language Models (MLLMs). In our benchmark, which is curated to evaluate MLLMs visual semantic understanding and fine-grained perception capabilities, we discussed different visual tokenizers pre-trained with dominant methods (i.e., DeiT, CLIP, MAE, DINO), and observe that: i) Fully/weakly supervised models capture more semantics than self-supervised models, but the gap is narrowed by scaling up the pre-training dataset. ii) Self-supervised models are better at fine-grained perception, where patch-level supervision is particularly effective. iii) Tuning the visual tokenizer leads to the loss of semantics obtained from large-scale pretraining, which is unfavorable with relatively small-scale instruction-tuning dataset. Given the findings, we reviewed methods that attempted to unify semantics and fine-grained visual understanding, e.g., patch-level feature distillation with semantically-rich targets. We obtain an intriguing insight mask-based strategies that were once all the rage may not be applicable for obtaining good visual tokenizers. Based on this critical observation, we obtain a new MLLM equipped with a tailored Good Visual Tokenizer (GVT), which exhibits strong visual comprehension capability at multiple scales. In particular, without introducing extra parameters and task-specific fine-tuning, GVT achieves superior performance on visual question answering, image captioning, and other fine-grained visual understanding tasks such as object counting and multi-class identification.', 'authors': ['Ying Shan', 'Mohan Kankanhalli', 'Xiaohan Ding', 'Yixiao Ge', 'Guangzhi Wang'], 'published': '2023-05-20', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'interpretable-by-design-visual-question', 'arxiv_id': '2305.14882', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2305.14882v2', 'url_pdf': 'https://arxiv.org/pdf/2305.14882v2.pdf', 'title': 'Dynamic Clue Bottlenecks: Towards Interpretable-by-Design Visual Question Answering', 'abstract': 'Recent advances in multimodal large language models (LLMs) have shown extreme effectiveness in visual question answering (VQA). However, the design nature of these end-to-end models prevents them from being interpretable to humans, undermining trust and applicability in critical domains. While post-hoc rationales offer certain insight into understanding model behavior, these explanations are not guaranteed to be faithful to the model. In this paper, we address these shortcomings by introducing an interpretable by design model that factors model decisions into intermediate human-legible explanations, and allows people to easily understand why a model fails or succeeds. We propose the Dynamic Clue Bottleneck Model ( (DCLUB), a method that is designed towards an inherently interpretable VQA system. DCLUB provides an explainable intermediate space before the VQA decision and is faithful from the beginning, while maintaining comparable performance to black-box systems. Given a question, DCLUB first returns a set of visual clues: natural language statements of visually salient evidence from the image, and then generates the output based solely on the visual clues. To supervise and evaluate the generation of VQA explanations within DCLUB, we collect a dataset of 1.7k reasoning-focused questions with visual clues. Evaluations show that our inherently interpretable system can improve 4.64% over a comparable black-box system in reasoning-focused questions while preserving 99.43% of performance on VQA-v2.', 'authors': ['Dan Roth', 'Mark Yatskar', 'Sihao Chen', 'Ben Zhou', 'Xingyu Fu'], 'published': '2023-05-24', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'pathasst-redefining-pathology-through', 'arxiv_id': '2305.15072', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2305.15072v2', 'url_pdf': 'https://arxiv.org/pdf/2305.15072v2.pdf', 'title': 'PathAsst: A Generative Foundation AI Assistant Towards Artificial General Intelligence of Pathology', 'abstract': \"As advances in large language models (LLMs) and multimodal techniques continue to mature, the development of general-purpose multimodal large language models (MLLMs) has surged, offering significant applications in interpreting natural images. However, the field of pathology has largely remained untapped, particularly in gathering high-quality data and designing comprehensive model frameworks. To bridge the gap in pathology MLLMs, we present PathAsst, a multimodal generative foundation AI assistant to revolutionize diagnostic and predictive analytics in pathology. The development of PathAsst involves three pivotal steps: data acquisition, CLIP model adaptation, and the training of PathAsst's multimodal generative capabilities. Firstly, we collect over 207K high-quality pathology image-text pairs from authoritative sources. Leveraging the advanced power of ChatGPT, we generate over 180K instruction-following samples. Furthermore, we devise additional instruction-following data specifically tailored for invoking eight pathology-specific sub-models we prepared, allowing the PathAsst to effectively collaborate with these models, enhancing its diagnostic ability. Secondly, by leveraging the collected data, we construct PathCLIP, a pathology-dedicated CLIP, to enhance PathAsst's capabilities in interpreting pathology images. Finally, we integrate PathCLIP with the Vicuna-13b and utilize pathology-specific instruction-tuning data to enhance the multimodal generation capacity of PathAsst and bolster its synergistic interactions with sub-models. The experimental results of PathAsst show the potential of harnessing AI-powered generative foundation model to improve pathology diagnosis and treatment processes.\", 'authors': ['Yunlong Zhang', 'Zhongyi Shui', 'Lin Sun', 'Lin Yang', 'Honglin Li', 'Kai Zhang', 'Sunyi Zheng', 'Chenglu Zhu', 'Yuxuan Sun'], 'published': '2023-05-24', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'contextual-object-detection-with-multimodal', 'arxiv_id': '2305.18279', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2305.18279v2', 'url_pdf': 'https://arxiv.org/pdf/2305.18279v2.pdf', 'title': 'Contextual Object Detection with Multimodal Large Language Models', 'abstract': 'Recent Multimodal Large Language Models (MLLMs) are remarkable in vision-language tasks, such as image captioning and question answering, but lack the essential perception ability, i.e., object detection. In this work, we address this limitation by introducing a novel research problem of contextual object detection -- understanding visible objects within different human-AI interactive contexts. Three representative scenarios are investigated, including the language cloze test, visual captioning, and question answering. Moreover, we present ContextDET, a unified multimodal model that is capable of end-to-end differentiable modeling of visual-language contexts, so as to locate, identify, and associate visual objects with language inputs for human-AI interaction. Our ContextDET involves three key submodels: (i) a visual encoder for extracting visual representations, (ii) a pre-trained LLM for multimodal context decoding, and (iii) a visual decoder for predicting bounding boxes given contextual object words. The new generate-then-detect framework enables us to detect object words within human vocabulary. Extensive experiments show the advantages of ContextDET on our proposed CODE benchmark, open-vocabulary detection, and referring image segmentation. Github: https://github.com/yuhangzang/ContextDET.', 'authors': ['Chen Change Loy', 'Kaiyang Zhou', 'Jun Han', 'Wei Li', 'Yuhang Zang'], 'published': '2023-05-29', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'a-survey-on-multimodal-large-language-models', 'arxiv_id': '2306.13549', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2306.13549v4', 'url_pdf': 'https://arxiv.org/pdf/2306.13549v4.pdf', 'title': 'A Survey on Multimodal Large Language Models', 'abstract': 'Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.', 'authors': ['Enhong Chen', 'Tong Xu', 'Xing Sun', 'Ke Li', 'Sirui Zhao', 'Chaoyou Fu', 'Shukang Yin'], 'published': '2023-06-23', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'mme-a-comprehensive-evaluation-benchmark-for', 'arxiv_id': '2306.13394', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2306.13394v4', 'url_pdf': 'https://arxiv.org/pdf/2306.13394v4.pdf', 'title': 'MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models', 'abstract': 'Multimodal Large Language Model (MLLM) relies on the powerful LLM to perform multimodal tasks, showing amazing emergent abilities in recent studies, such as writing poems based on an image. However, it is difficult for these case studies to fully reflect the performance of MLLM, lacking a comprehensive evaluation. In this paper, we fill in this blank, presenting the first comprehensive MLLM Evaluation benchmark MME. It measures both perception and cognition abilities on a total of 14 subtasks. In order to avoid data leakage that may arise from direct use of public datasets for evaluation, the annotations of instruction-answer pairs are all manually designed. The concise instruction design allows us to fairly compare MLLMs, instead of struggling in prompt engineering. Besides, with such an instruction, we can also easily carry out quantitative statistics. A total of 30 advanced MLLMs are comprehensively evaluated on our MME, which not only suggests that existing MLLMs still have a large room for improvement, but also reveals the potential directions for the subsequent model optimization. The data application manner and online leaderboards are released at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation.', 'authors': ['Rongrong Ji', 'Yunsheng Wu', 'Jinrui Yang', 'Xing Sun', 'Ke Li', 'Xiawu Zheng', 'Xu Lin', 'Mengdan Zhang', 'Yulei Qin', 'Yunhang Shen', 'Peixian Chen', 'Chaoyou Fu'], 'published': '2023-06-23', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'kosmos-2-grounding-multimodal-large-language', 'arxiv_id': '2306.14824', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2306.14824v3', 'url_pdf': 'https://arxiv.org/pdf/2306.14824v3.pdf', 'title': 'Kosmos-2: Grounding Multimodal Large Language Models to the World', 'abstract': \"We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. Specifically, we represent refer expressions as links in Markdown, i.e., ``[text span](bounding boxes)'', where object descriptions are sequences of location tokens. Together with multimodal corpora, we construct large-scale data of grounded image-text pairs (called GrIT) to train the model. In addition to the existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), Kosmos-2 integrates the grounding capability into downstream applications. We evaluate Kosmos-2 on a wide range of tasks, including (i) multimodal grounding, such as referring expression comprehension, and phrase grounding, (ii) multimodal referring, such as referring expression generation, (iii) perception-language tasks, and (iv) language understanding and generation. This work lays out the foundation for the development of Embodiment AI and sheds light on the big convergence of language, multimodal perception, action, and world modeling, which is a key step toward artificial general intelligence. Code and pretrained models are available at https://aka.ms/kosmos-2.\", 'authors': ['Furu Wei', 'Shuming Ma', 'Shaohan Huang', 'Yaru Hao', 'Li Dong', 'Wenhui Wang', 'Zhiliang Peng'], 'published': '2023-06-26', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'shikra-unleashing-multimodal-llm-s', 'arxiv_id': '2306.15195', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2306.15195v2', 'url_pdf': 'https://arxiv.org/pdf/2306.15195v2.pdf', 'title': \"Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic\", 'abstract': \"In human conversations, individuals can indicate relevant regions within a scene while addressing others. In turn, the other person can then respond by referring to specific regions if necessary. This natural referential ability in dialogue remains absent in current Multimodal Large Language Models (MLLMs). To fill this gap, this paper proposes an MLLM called Shikra, which can handle spatial coordinate inputs and outputs in natural language. Its architecture consists of a vision encoder, an alignment layer, and a LLM. It is designed to be straightforward and simple, without the need for extra vocabularies, position encoder, pre-/post-detection modules, or external plug-in models. All inputs and outputs are in natural language form. Referential dialogue is a superset of various vision-language (VL) tasks. Shikra can naturally handle location-related tasks like REC and PointQA, as well as conventional VL tasks such as Image Captioning and VQA. Experimental results showcase Shikra's promising performance. Furthermore, it enables numerous exciting applications, like providing mentioned objects' coordinates in chains of thoughts and comparing user-pointed regions similarities. Our code, model and dataset are accessed at https://github.com/shikras/shikra.\", 'authors': ['Rui Zhao', 'Feng Zhu', 'Richong Zhang', 'Weili Zeng', 'Zhao Zhang', 'Keqin Chen'], 'published': '2023-06-27', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'svit-scaling-up-visual-instruction-tuning', 'arxiv_id': '2307.04087', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2307.04087v3', 'url_pdf': 'https://arxiv.org/pdf/2307.04087v3.pdf', 'title': 'SVIT: Scaling up Visual Instruction Tuning', 'abstract': \"Thanks to the emerging of foundation models, the large language and vision models are integrated to acquire the multimodal ability of visual captioning, question answering, etc. Although existing multimodal models present impressive performance of visual understanding and reasoning, their limits are still largely under-explored due to the scarcity of high-quality instruction tuning data. To push the limits of multimodal capability, we Scale up Visual Instruction Tuning (SVIT) by constructing a dataset of 4.2 million visual instruction tuning data including 1.6M conversation question-answer (QA) pairs, 1.6M complex reasoning QA pairs, 1.0M referring QA pairs and 106K detailed image descriptions. Besides the volume, the proposed dataset is also featured by the high quality and rich diversity, which is generated by prompting GPT-4 with the abundant manual annotations of images. We also propose a new data recipe to select subset with better diversity and balance, which evokes model's superior capabilities. Extensive experiments verify that SVIT-v1.5, trained on the proposed dataset, outperforms state-of-the-art Multimodal Large Language Models on popular benchmarks. The data and code are publicly available at https://github.com/BAAI-DCAI/Visual-Instruction-Tuning.\", 'authors': ['Tiejun Huang', 'Muyang He', 'Boya Wu', 'Bo Zhao'], 'published': '2023-07-09', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'chatspot-bootstrapping-multimodal-llms-via', 'arxiv_id': '2307.09474', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2307.09474v1', 'url_pdf': 'https://arxiv.org/pdf/2307.09474v1.pdf', 'title': 'ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning', 'abstract': \"Human-AI interactivity is a critical aspect that reflects the usability of multimodal large language models (MLLMs). However, existing end-to-end MLLMs only allow users to interact with them through language instructions, leading to the limitation of the interactive accuracy and efficiency. In this study, we present precise referring instructions that utilize diverse reference representations such as points and boxes as referring prompts to refer to the special region. This enables MLLMs to focus on the region of interest and achieve finer-grained interaction. Based on precise referring instruction, we propose ChatSpot, a unified end-to-end multimodal large language model that supports diverse forms of interactivity including mouse clicks, drag-and-drop, and drawing boxes, which provides a more flexible and seamless interactive experience. We also construct a multi-grained vision-language instruction-following dataset based on existing datasets and GPT-4 generating. Furthermore, we design a series of evaluation tasks to assess the effectiveness of region recognition and interaction. Experimental results showcase ChatSpot's promising performance.\", 'authors': ['Xiangyu Zhang', 'Chunrui Han', 'Runpei Dong', 'Yuang Peng', 'Jianjian Sun', 'HongYu Zhou', 'Haoran Wei', 'Jinrong Yang', 'Zheng Ge', 'En Yu', 'Liang Zhao'], 'published': '2023-07-18', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'seed-bench-benchmarking-multimodal-llms-with', 'arxiv_id': '2307.16125', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2307.16125v2', 'url_pdf': 'https://arxiv.org/pdf/2307.16125v2.pdf', 'title': 'SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension', 'abstract': 'Based on powerful Large Language Models (LLMs), recent generative Multimodal Large Language Models (MLLMs) have gained prominence as a pivotal research area, exhibiting remarkable capability for both comprehension and generation. In this work, we address the evaluation of generative comprehension in MLLMs as a preliminary step towards a comprehensive assessment of generative models, by introducing a benchmark named SEED-Bench. SEED-Bench consists of 19K multiple choice questions with accurate human annotations (x 6 larger than existing benchmarks), which spans 12 evaluation dimensions including the comprehension of both the image and video modality. We develop an advanced pipeline for generating multiple-choice questions that target specific evaluation dimensions, integrating both automatic filtering and manual verification processes. Multiple-choice questions with groundtruth options derived from human annotation enables an objective and efficient assessment of model performance, eliminating the need for human or GPT intervention during evaluation. We further evaluate the performance of 18 models across all 12 dimensions, covering both the spatial and temporal understanding. By revealing the limitations of existing MLLMs through evaluation results, we aim for SEED-Bench to provide insights for motivating future research. We will launch and consistently maintain a leaderboard to provide a platform for the community to assess and investigate model capability.', 'authors': ['Ying Shan', 'Yixiao Ge', 'Yuying Ge', 'Guangzhi Wang', 'Rui Wang', 'Bohao Li'], 'published': '2023-07-30', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'lisa-reasoning-segmentation-via-large', 'arxiv_id': '2308.00692', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.00692v3', 'url_pdf': 'https://arxiv.org/pdf/2308.00692v3.pdf', 'title': 'LISA: Reasoning Segmentation via Large Language Model', 'abstract': 'Although perception systems have made remarkable advancements in recent years, they still rely on explicit human instruction or pre-defined categories to identify the target objects before executing visual recognition tasks. Such systems cannot actively reason and comprehend implicit user intention. In this work, we propose a new segmentation task -- reasoning segmentation. The task is designed to output a segmentation mask given a complex and implicit query text. Furthermore, we establish a benchmark comprising over one thousand image-instruction-mask data samples, incorporating intricate reasoning and world knowledge for evaluation purposes. Finally, we present LISA: large Language Instructed Segmentation Assistant, which inherits the language generation capabilities of multimodal Large Language Models (LLMs) while also possessing the ability to produce segmentation masks. We expand the original vocabulary with a <SEG> token and propose the embedding-as-mask paradigm to unlock the segmentation capability. Remarkably, LISA can handle cases involving complex reasoning and world knowledge. Also, it demonstrates robust zero-shot capability when trained exclusively on reasoning-free datasets. In addition, fine-tuning the model with merely 239 reasoning segmentation data samples results in further performance enhancement. Both quantitative and qualitative experiments show our method effectively unlocks new reasoning segmentation capabilities for multimodal LLMs. Code, models, and data are available at https://github.com/dvlab-research/LISA.', 'authors': ['Jiaya Jia', 'Shu Liu', 'Yuhui Yuan', 'Yanwei Li', 'Yukang Chen', 'Zhuotao Tian', 'Xin Lai'], 'published': '2023-08-01', 'conference': None, 'conference_url_abs': 'http://openaccess.thecvf.com//content/CVPR2024/html/Lai_LISA_Reasoning_Segmentation_via_Large_Language_Model_CVPR_2024_paper.html', 'conference_url_pdf': 'http://openaccess.thecvf.com//content/CVPR2024/papers/Lai_LISA_Reasoning_Segmentation_via_Large_Language_Model_CVPR_2024_paper.pdf', 'proceeding': 'cvpr-2024-1'}, {'id': 'empowering-vision-language-models-to-follow', 'arxiv_id': '2308.04152', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.04152v4', 'url_pdf': 'https://arxiv.org/pdf/2308.04152v4.pdf', 'title': 'Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions', 'abstract': \"Recent advancements in Multimodal Large Language Models (MLLMs) have been utilizing Visual Prompt Generators (VPGs) to convert visual features into tokens that LLMs can recognize. This is achieved by training the VPGs on millions of image-caption pairs, where the VPG-generated tokens of images are fed into a frozen LLM to generate the corresponding captions. However, this image-captioning based training objective inherently biases the VPG to concentrate solely on the primary visual contents sufficient for caption generation, often neglecting other visual details. This shortcoming results in MLLMs' underperformance in comprehending demonstrative instructions consisting of multiple, interleaved, and multimodal instructions that demonstrate the required context to complete a task. To address this issue, we introduce a generic and lightweight Visual Prompt Generator Complete module (VPG-C), which can infer and complete the missing details essential for comprehending demonstrative instructions. Further, we propose a synthetic discriminative training strategy to fine-tune VPG-C, eliminating the need for supervised demonstrative instructions. As for evaluation, we build DEMON, a comprehensive benchmark for demonstrative instruction understanding. Synthetically trained with the proposed strategy, VPG-C achieves significantly stronger zero-shot performance across all tasks of DEMON. Further evaluation on the MME and OwlEval benchmarks also demonstrate the superiority of VPG-C. Our benchmark, code, and pre-trained models are available at https://github.com/DCDmllm/Cheetah.\", 'authors': ['Wenqiao Zhang', 'Wei Ji', 'Yueting Zhuang', 'Siliang Tang', 'Tat-Seng Chua', 'Hanwang Zhang', 'Minghe Gao', 'Zhiqi Ge', 'Kaihang Pan', 'Juncheng Li'], 'published': '2023-08-08', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'link-context-learning-for-multimodal-llms', 'arxiv_id': '2308.07891', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.07891v1', 'url_pdf': 'https://arxiv.org/pdf/2308.07891v1.pdf', 'title': 'Link-Context Learning for Multimodal LLMs', 'abstract': 'The ability to learn from context with novel concepts, and deliver appropriate responses are essential in human conversations. Despite current Multimodal Large Language Models (MLLMs) and Large Language Models (LLMs) being trained on mega-scale datasets, recognizing unseen images or understanding novel concepts in a training-free manner remains a challenge. In-Context Learning (ICL) explores training-free few-shot learning, where models are encouraged to ``learn to learn\" from limited tasks and generalize to unseen tasks. In this work, we propose link-context learning (LCL), which emphasizes \"reasoning from cause and effect\" to augment the learning capabilities of MLLMs. LCL goes beyond traditional ICL by explicitly strengthening the causal relationship between the support set and the query set. By providing demonstrations with causal links, LCL guides the model to discern not only the analogy but also the underlying causal associations between data points, which empowers MLLMs to recognize unseen images and understand novel concepts more effectively. To facilitate the evaluation of this novel approach, we introduce the ISEKAI dataset, comprising exclusively of unseen generated image-label pairs designed for link-context learning. Extensive experiments show that our LCL-MLLM exhibits strong link-context learning capabilities to novel concepts over vanilla MLLMs. Code and data will be released at https://github.com/isekai-portal/Link-Context-Learning.', 'authors': ['Ziwei Liu', 'Rui Zhao', 'Feng Zhu', 'Zhao Zhang', 'Weichen Fan', 'Yan Tai'], 'published': '2023-08-15', 'conference': None, 'conference_url_abs': 'http://openaccess.thecvf.com//content/CVPR2024/html/Tai_Link-Context_Learning_for_Multimodal_LLMs_CVPR_2024_paper.html', 'conference_url_pdf': 'http://openaccess.thecvf.com//content/CVPR2024/papers/Tai_Link-Context_Learning_for_Multimodal_LLMs_CVPR_2024_paper.pdf', 'proceeding': 'cvpr-2024-1'}, {'id': 'fashionlogo-prompting-multimodal-large', 'arxiv_id': '2308.09012', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.09012v2', 'url_pdf': 'https://arxiv.org/pdf/2308.09012v2.pdf', 'title': 'FashionLOGO: Prompting Multimodal Large Language Models for Fashion Logo Embeddings', 'abstract': \"Logo embedding models convert the product logos in images into vectors, enabling their utilization for logo recognition and detection within e-commerce platforms. This facilitates the enforcement of intellectual property rights and enhances product search capabilities. However, current methods treat logo embedding as a purely visual problem. A noteworthy issue is that visual models capture features more than logos. Instead, we view this as a multimodal task, using text as auxiliary information to facilitate the visual model's understanding of the logo. The emerging Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in both visual and textual understanding. Inspired by this, we propose an approach, \\\\textbf{FashionLOGO}, to explore how to prompt MLLMs to generate appropriate text for product images, which can help visual models achieve better logo embeddings. We adopt a cross-attention transformer block that enables visual embedding to automatically learn supplementary knowledge from textual embedding. Our extensive experiments on real-world datasets prove that FashionLOGO is capable of generating generic and robust logo embeddings, achieving state-of-the-art performance in all benchmarks.\", 'authors': ['Walton Wang', 'Minghui Qiu', 'Min Yang', 'Yulin Su', 'Da Li', 'Zhen Wang'], 'published': '2023-08-17', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'wanjuan-a-comprehensive-multimodal-dataset', 'arxiv_id': '2308.10755', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.10755v3', 'url_pdf': 'https://arxiv.org/pdf/2308.10755v3.pdf', 'title': 'WanJuan: A Comprehensive Multimodal Dataset for Advancing English and Chinese Large Models', 'abstract': 'The rise in popularity of ChatGPT and GPT-4 has significantly accelerated the development of large models, leading to the creation of numerous impressive large language models(LLMs) and multimodal large language models (MLLMs). These cutting-edge models owe their remarkable performance to high-quality data. However, the details of the training data used in leading paradigms are often kept confidential. This lack of transparency, coupled with the scarcity of open-source data, impedes further developments within the community. As a response, this paper presents \"Wan Juan\", a large-scale multimodal dataset composed of both Chinese and English data, collected from a wide range of web sources. The dataset incorporates text, image-text, and video modalities, with a total volume exceeding 2TB. It was utilized in the training of InternLM, a model that demonstrated significant advantages in multi-dimensional evaluations when compared to models of a similar scale. All data can be accessed at https://opendatalab.org.cn/WanJuan1.0.', 'authors': ['Dahua Lin', 'Jiaqi Wang', 'Hang Yan', 'Wei Li', 'Bin Wang', 'Jiantao Qiu', 'Chao Xu', 'Zhenjiang Jin', 'Conghui He'], 'published': '2023-08-21', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'stablellava-enhanced-visual-instruction', 'arxiv_id': '2308.10253', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.10253v2', 'url_pdf': 'https://arxiv.org/pdf/2308.10253v2.pdf', 'title': 'StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data', 'abstract': \"The remarkable multimodal capabilities demonstrated by OpenAI's GPT-4 have sparked significant interest in the development of multimodal Large Language Models (LLMs). A primary research objective of such models is to align visual and textual modalities effectively while comprehending human instructions. Current methodologies often rely on annotations derived from benchmark datasets to construct image-dialogue datasets for training purposes, akin to instruction tuning in LLMs. However, these datasets often exhibit domain bias, potentially constraining the generative capabilities of the models. In an effort to mitigate these limitations, we propose a novel data collection methodology that synchronously synthesizes images and dialogues for visual instruction tuning. This approach harnesses the power of generative models, marrying the abilities of ChatGPT and text-to-image generative models to yield a diverse and controllable dataset with varied image content. Additionally, datasets can be arbitrarily scaled. This not only provides greater flexibility compared to existing methodologies but also significantly enhances several model capabilities. Our research includes comprehensive experiments conducted on various datasets. The results emphasize substantial enhancements in more than ten commonly assessed capabilities. Additionally, our model achieves state-of-the-art results across multiple widely recognized multimodal benchmarks.\", 'authors': ['Yunchao Wei', 'Ling Chen', 'Chunhua Shen', 'Guosheng Lin', 'Bin Fu', 'Zhibin Wang', 'Gang Yu', 'Chi Zhang', 'Yanda Li'], 'published': '2023-08-20', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'instructiongpt-4-a-200-instruction-paradigm', 'arxiv_id': '2308.12067', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.12067v2', 'url_pdf': 'https://arxiv.org/pdf/2308.12067v2.pdf', 'title': 'InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4', 'abstract': 'Multimodal large language models are typically trained in two stages: first pre-training on image-text pairs, and then fine-tuning using supervised vision-language instruction data. Recent studies have shown that large language models can achieve satisfactory results even with a limited amount of high-quality instruction-following data. In this paper, we introduce InstructionGPT-4, which is fine-tuned on a small dataset comprising only 200 examples, amounting to approximately 6\\\\% of the instruction-following data used in the alignment dataset for MiniGPT-4. To achieve this, we first propose several metrics to access the quality of multimodal instruction data. Based on these metrics, we present an effective and trainable data selector to automatically identify and filter low-quality vision-language data. By employing this method, InstructionGPT-4 outperforms the original MiniGPT-4 on various evaluations. Overall, our findings demonstrate that less but high-quality instruction tuning data is efficient in enabling multimodal large language models to generate better output. Our code is available at https://github.com/waltonfuture/InstructionGPT-4.', 'authors': ['Lichao Sun', 'Weiran Huang', 'Zihao Jiang', 'Lai Wei'], 'published': '2023-08-23', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'vigc-visual-instruction-generation-and', 'arxiv_id': '2308.12714', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.12714v3', 'url_pdf': 'https://arxiv.org/pdf/2308.12714v3.pdf', 'title': 'VIGC: Visual Instruction Generation and Correction', 'abstract': \"The integration of visual encoders and large language models (LLMs) has driven recent progress in multimodal large language models (MLLMs). However, the scarcity of high-quality instruction-tuning data for vision-language tasks remains a challenge. The current leading paradigm, such as LLaVA, relies on language-only GPT-4 to generate data, which requires pre-annotated image captions and detection bounding boxes, suffering from understanding image details. A practical solution to this problem would be to utilize the available multimodal large language models (MLLMs) to generate instruction data for vision-language tasks. However, it's worth noting that the currently accessible MLLMs are not as powerful as their LLM counterparts, as they tend to produce inadequate responses and generate false information. As a solution for addressing the current issue, this paper proposes the Visual Instruction Generation and Correction (VIGC) framework that enables multimodal large language models to generate instruction-tuning data and progressively enhance its quality on-the-fly. Specifically, Visual Instruction Generation (VIG) guides the vision-language model to generate diverse instruction-tuning data. To ensure generation quality, Visual Instruction Correction (VIC) adopts an iterative update mechanism to correct any inaccuracies in data produced by VIG, effectively reducing the risk of hallucination. Leveraging the diverse, high-quality data generated by VIGC, we finetune mainstream models and validate data quality based on various evaluations. Experimental results demonstrate that VIGC not only compensates for the shortcomings of language-only data generation methods, but also effectively enhances the benchmark performance. The models, datasets, and code are available at https://opendatalab.github.io/VIGC.\", 'authors': ['Conghui He', 'Jiaqi Wang', 'Wei Li', 'Weijia Li', 'Xiaoyi Dong', 'Pan Zhang', 'Huaping Zhong', 'Jiahui Peng', 'Xiao Han', 'Fan Wu', 'Bin Wang'], 'published': '2023-08-24', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'position-enhanced-visual-instruction-tuning', 'arxiv_id': '2308.13437', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.13437v2', 'url_pdf': 'https://arxiv.org/pdf/2308.13437v2.pdf', 'title': 'Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models', 'abstract': 'Recently, Multimodal Large Language Models (MLLMs) that enable Large Language Models (LLMs) to interpret images through visual instruction tuning have achieved significant success. However, existing visual instruction tuning methods only utilize image-language instruction data to align the language and image modalities, lacking a more fine-grained cross-modal alignment. In this paper, we propose Position-enhanced Visual Instruction Tuning (PVIT), which extends the functionality of MLLMs by integrating an additional region-level vision encoder. This integration promotes a more detailed comprehension of images for the MLLM. In addition, to efficiently achieve a fine-grained alignment between the vision modules and the LLM, we design multiple data generation strategies to construct an image-region-language instruction dataset. Finally, we present both quantitative experiments and qualitative analysis that demonstrate the superiority of the proposed model. Code and data will be released at https://github.com/PVIT-official/PVIT.', 'authors': ['Yang Liu', 'Maosong Sun', 'Peng Li', 'Xiaoyue Mi', 'Fuwen Luo', 'Ruoyu Qin', 'Chi Chen'], 'published': '2023-08-25', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'mllm-dataengine-an-iterative-refinement', 'arxiv_id': '2308.13566', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.13566v2', 'url_pdf': 'https://arxiv.org/pdf/2308.13566v2.pdf', 'title': 'MLLM-DataEngine: An Iterative Refinement Approach for MLLM', 'abstract': 'Despite the great advance of Multimodal Large Language Models (MLLMs) in both instruction dataset building and benchmarking, the independence of training and evaluation makes current MLLMs hard to further improve their capability under the guidance of evaluation results with a relatively low human cost. In this paper, we propose MLLM-DataEngine, a novel closed-loop system that bridges data generation, model training, and evaluation. Within each loop iteration, the MLLM-DataEngine first analyze the weakness of the model based on the evaluation results, then generate a proper incremental dataset for the next training iteration and enhance the model capability iteratively. Compared with previous data collection methods which are separate from the benchmarking, the data generated by MLLM-DataEngine shows better targeting, quality, and correctness. For targeting, we propose an Adaptive Bad-case Sampling module, which adjusts the ratio of different types of data within each incremental dataset based on the benchmarking results. For quality, we resort to GPT-4 to generate high-quality data with each given data type. For correctness, prompt design is critical for the data generation results. Rather than previous hand-crafted prompt, we propose an Interactive Prompt Optimization strategy, which optimizes the prompt with the multi-round interaction between human and GPT, and improve the correctness of generated data greatly. Through extensive experiments, we find our MLLM-DataEngine could boost the MLLM capability in a targeted and automatic manner, with only a few human participation. We hope it could be a general solution for the following MLLMs building. The MLLM-DataEngine has been open-sourced and is now available at https://github.com/opendatalab/MLLM-DataEngine.', 'authors': ['Conghui He', 'Jiaqi Wang', 'Xiaoyi Dong', 'Pan Zhang', 'Siyuan Huang', 'Bin Wang', 'Linke Ouyang', 'Zhiyuan Zhao'], 'published': '2023-08-25', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'socratis-are-large-multimodal-models', 'arxiv_id': '2308.16741', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2308.16741v3', 'url_pdf': 'https://arxiv.org/pdf/2308.16741v3.pdf', 'title': 'Socratis: Are large multimodal models emotionally aware?', 'abstract': 'Existing emotion prediction benchmarks contain coarse emotion labels which do not consider the diversity of emotions that an image and text can elicit in humans due to various reasons. Learning diverse reactions to multimodal content is important as intelligent machines take a central role in generating and delivering content to society. To address this gap, we propose Socratis, a societal reactions benchmark, where each image-caption (IC) pair is annotated with multiple emotions and the reasons for feeling them. Socratis contains 18K free-form reactions for 980 emotions on 2075 image-caption pairs from 5 widely-read news and image-caption (IC) datasets. We benchmark the capability of state-of-the-art multimodal large language models to generate the reasons for feeling an emotion given an IC pair. Based on a preliminary human study, we observe that humans prefer human-written reasons over 2 times more often than machine-generated ones. This shows our task is harder than standard generation tasks because it starkly contrasts recent findings where humans cannot tell apart machine vs human-written news articles, for instance. We further see that current captioning metrics based on large vision-language models also fail to correlate with human preferences. We hope that these findings and our benchmark will inspire further research on training emotionally aware models.', 'authors': ['Kate Saenko', 'Bryan A. Plummer', 'Saadia Gabriel', 'Reuben Tan', 'Arijit Ray', 'Katherine Deng'], 'published': '2023-08-31', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'multiway-adapater-adapting-large-scale-multi', 'arxiv_id': '2309.01516', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.01516v3', 'url_pdf': 'https://arxiv.org/pdf/2309.01516v3.pdf', 'title': 'MultiWay-Adapater: Adapting large-scale multi-modal models for scalable image-text retrieval', 'abstract': \"As Multimodal Large Language Models (MLLMs) grow in size, adapting them to specialized tasks becomes increasingly challenging due to high computational and memory demands. Indeed, traditional fine-tuning methods are costly, due to the need for extensive, task-specific training. While efficient adaptation methods exist that aim to reduce these costs, in practice they suffer from shallow inter-modal alignment, which severely hurts model effectiveness. To tackle these computational challenges and improve inter-modal alignment, we introduce the MultiWay-Adapter (MWA), a novel framework featuring an 'Alignment Enhancer'. This enhancer deepens inter-modal alignment, enabling high transferability with minimal tuning effort. Our experiments show that unlike prior efficient tuning approaches, MWA maintains model effectiveness, while reducing training time by up-to 57%. MWA is also lightweight, increasing model size by only 2-3% (in terms of parameters) for state-of-the-art foundation models like BEiT-3 Large. These results demonstrate that MWA provides an efficient and effective adaptation method for MLLMs, significantly broadening their applicability.\", 'authors': ['Gerardo Aragon Camarasa', 'Richard McCreadie', 'George Killick', 'Zijun Long'], 'published': '2023-09-04', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'next-gpt-any-to-any-multimodal-llm', 'arxiv_id': '2309.05519', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.05519v3', 'url_pdf': 'https://arxiv.org/pdf/2309.05519v3.pdf', 'title': 'NExT-GPT: Any-to-Any Multimodal LLM', 'abstract': 'While recently Multimodal Large Language Models (MM-LLMs) have made exciting strides, they mostly fall prey to the limitation of only input-side multimodal understanding, without the ability to produce content in multiple modalities. As we humans always perceive the world and communicate with people through various modalities, developing any-to-any MM-LLMs capable of accepting and delivering content in any modality becomes essential to human-level AI. To fill the gap, we present an end-to-end general-purpose any-to-any MM-LLM system, NExT-GPT. We connect an LLM with multimodal adaptors and different diffusion decoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio. By leveraging the existing well-trained highly-performing encoders and decoders, NExT-GPT is tuned with only a small amount of parameter (1%) of certain projection layers, which not only benefits low-cost training and also facilitates convenient expansion to more potential modalities. Moreover, we introduce a modality-switching instruction tuning (MosIT) and manually curate a high-quality dataset for MosIT, based on which NExT-GPT is empowered with complex cross-modal semantic understanding and content generation. Overall, our research showcases the promising possibility of building an AI agent capable of modeling universal modalities, paving the way for more human-like AI research in the community. Project page: https://next-gpt.github.io/', 'authors': ['Tat-Seng Chua', 'Wei Ji', 'Leigang Qu', 'Hao Fei', 'Shengqiong Wu'], 'published': '2023-09-11', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'hilm-d-towards-high-resolution-understanding', 'arxiv_id': '2309.05186', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.05186v1', 'url_pdf': 'https://arxiv.org/pdf/2309.05186v1.pdf', 'title': 'HiLM-D: Towards High-Resolution Understanding in Multimodal Large Language Models for Autonomous Driving', 'abstract': \"Autonomous driving systems generally employ separate models for different tasks resulting in intricate designs. For the first time, we leverage singular multimodal large language models (MLLMs) to consolidate multiple autonomous driving tasks from videos, i.e., the Risk Object Localization and Intention and Suggestion Prediction (ROLISP) task. ROLISP uses natural language to simultaneously identify and interpret risk objects, understand ego-vehicle intentions, and provide motion suggestions, eliminating the necessity for task-specific architectures. However, lacking high-resolution (HR) information, existing MLLMs often miss small objects (e.g., traffic cones) and overly focus on salient ones (e.g., large trucks) when applied to ROLISP. We propose HiLM-D (Towards High-Resolution Understanding in MLLMs for Autonomous Driving), an efficient method to incorporate HR information into MLLMs for the ROLISP task. Especially, HiLM-D integrates two branches: (i) the low-resolution reasoning branch, can be any MLLMs, processes low-resolution videos to caption risk objects and discern ego-vehicle intentions/suggestions; (ii) the high-resolution perception branch (HR-PB), prominent to HiLM-D,, ingests HR images to enhance detection by capturing vision-specific HR feature maps and prioritizing all potential risks over merely salient objects. Our HR-PB serves as a plug-and-play module, seamlessly fitting into current MLLMs. Experiments on the ROLISP benchmark reveal HiLM-D's notable advantage over leading MLLMs, with improvements of 4.8% in BLEU-4 for captioning and 17.2% in mIoU for detection.\", 'authors': ['Xiaomeng Li', 'Wei zhang', 'Hang Xu', 'Jianhua Han', 'Xinpeng Ding'], 'published': '2023-09-11', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'investigating-the-catastrophic-forgetting-in', 'arxiv_id': '2309.10313', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.10313v4', 'url_pdf': 'https://arxiv.org/pdf/2309.10313v4.pdf', 'title': 'Investigating the Catastrophic Forgetting in Multimodal Large Language Models', 'abstract': 'Following the success of GPT4, there has been a surge in interest in multimodal large language model (MLLM) research. This line of research focuses on developing general-purpose LLMs through fine-tuning pre-trained LLMs and vision models. However, catastrophic forgetting, a notorious phenomenon where the fine-tuned model fails to retain similar performance compared to the pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM). In this paper, we introduce EMT: Evaluating MulTimodality for evaluating the catastrophic forgetting in MLLMs, by treating each MLLM as an image classifier. We first apply EMT to evaluate several open-source fine-tuned MLLMs and we discover that almost all evaluated MLLMs fail to retain the same performance levels as their vision encoders on standard image classification tasks. Moreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess performance throughout the fine-tuning. Interestingly, our results suggest that early-stage fine-tuning on an image dataset improves performance across other image datasets, by enhancing the alignment of text and visual features. However, as fine-tuning proceeds, the MLLMs begin to hallucinate, resulting in a significant loss of generalizability, even when the image encoder remains frozen. Our results suggest that MLLMs have yet to demonstrate performance on par with their vision models on standard image classification tasks and the current MLLM fine-tuning procedure still has room for improvement.', 'authors': ['Yi Ma', 'Yong Jae Lee', 'Qing Qu', 'Mu Cai', 'Xiao Li', 'Shengbang Tong', 'Yuexiang Zhai'], 'published': '2023-09-19', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'dreamllm-synergistic-multimodal-comprehension', 'arxiv_id': '2309.11499', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.11499v2', 'url_pdf': 'https://arxiv.org/pdf/2309.11499v2.pdf', 'title': 'DreamLLM: Synergistic Multimodal Comprehension and Creation', 'abstract': \"This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy. Project page: https://dreamllm.github.io.\", 'authors': ['Li Yi', 'Kaisheng Ma', 'Xiangyu Zhang', 'Xiangwen Kong', 'Haoran Wei', 'HongYu Zhou', 'Jianjian Sun', 'Liang Zhao', 'Jinrong Yang', 'Zheng Ge', 'Zekun Qi', 'Yuang Peng', 'Chunrui Han', 'Runpei Dong'], 'published': '2023-09-20', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'how-robust-is-google-s-bard-to-adversarial', 'arxiv_id': '2309.11751', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.11751v2', 'url_pdf': 'https://arxiv.org/pdf/2309.11751v2.pdf', 'title': \"How Robust is Google's Bard to Adversarial Image Attacks?\", 'abstract': \"Multimodal Large Language Models (MLLMs) that integrate text and other modalities (especially vision) have achieved unprecedented performance in various multimodal tasks. However, due to the unsolved adversarial robustness problem of vision models, MLLMs can have more severe safety and security risks by introducing the vision inputs. In this work, we study the adversarial robustness of Google's Bard, a competitive chatbot to ChatGPT that released its multimodal capability recently, to better understand the vulnerabilities of commercial MLLMs. By attacking white-box surrogate vision encoders or MLLMs, the generated adversarial examples can mislead Bard to output wrong image descriptions with a 22% success rate based solely on the transferability. We show that the adversarial examples can also attack other MLLMs, e.g., a 26% attack success rate against Bing Chat and a 86% attack success rate against ERNIE bot. Moreover, we identify two defense mechanisms of Bard, including face detection and toxicity detection of images. We design corresponding attacks to evade these defenses, demonstrating that the current defenses of Bard are also vulnerable. We hope this work can deepen our understanding on the robustness of MLLMs and facilitate future research on defenses. Our code is available at https://github.com/thu-ml/Attack-Bard. Update: GPT-4V is available at October 2023. We further evaluate its robustness under the same set of adversarial examples, achieving a 45% attack success rate.\", 'authors': ['Jun Zhu', 'Hang Su', 'Yu Tian', 'Yichi Zhang', 'Xiao Yang', 'Zhengwei Fang', 'Jiawei Chen', 'Huanran Chen', 'Yinpeng Dong'], 'published': '2023-09-21', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'vdc-versatile-data-cleanser-for-detecting', 'arxiv_id': '2309.16211', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.16211v2', 'url_pdf': 'https://arxiv.org/pdf/2309.16211v2.pdf', 'title': 'VDC: Versatile Data Cleanser based on Visual-Linguistic Inconsistency by Multimodal Large Language Models', 'abstract': 'The role of data in building AI systems has recently been emphasized by the emerging concept of data-centric AI. Unfortunately, in the real-world, datasets may contain dirty samples, such as poisoned samples from backdoor attack, noisy labels in crowdsourcing, and even hybrids of them. The presence of such dirty samples makes the DNNs vunerable and unreliable.Hence, it is critical to detect dirty samples to improve the quality and realiability of dataset. Existing detectors only focus on detecting poisoned samples or noisy labels, that are often prone to weak generalization when dealing with dirty samples from other domains.In this paper, we find a commonality of various dirty samples is visual-linguistic inconsistency between images and associated labels. To capture the semantic inconsistency between modalities, we propose versatile data cleanser (VDC) leveraging the surpassing capabilities of multimodal large language models (MLLM) in cross-modal alignment and reasoning.It consists of three consecutive modules: the visual question generation module to generate insightful questions about the image; the visual question answering module to acquire the semantics of the visual content by answering the questions with MLLM; followed by the visual answer evaluation module to evaluate the inconsistency.Extensive experiments demonstrate its superior performance and generalization to various categories and types of dirty samples. The code is available at \\\\url{https://github.com/zihao-ai/vdc}.', 'authors': ['Baoyuan Wu', 'Bingzhe Wu', 'Shaokui Wei', 'Mingda Zhang', 'Zihao Zhu'], 'published': '2023-09-28', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'guiding-instruction-based-image-editing-via', 'arxiv_id': '2309.17102', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2309.17102v2', 'url_pdf': 'https://arxiv.org/pdf/2309.17102v2.pdf', 'title': 'Guiding Instruction-based Image Editing via Multimodal Large Language Models', 'abstract': 'Instruction-based image editing improves the controllability and flexibility of image manipulation via natural commands without elaborate descriptions or regional masks. However, human instructions are sometimes too brief for current methods to capture and follow. Multimodal large language models (MLLMs) show promising capabilities in cross-modal understanding and visual-aware response generation via LMs. We investigate how MLLMs facilitate edit instructions and present MLLM-Guided Image Editing (MGIE). MGIE learns to derive expressive instructions and provides explicit guidance. The editing model jointly captures this visual imagination and performs manipulation through end-to-end training. We evaluate various aspects of Photoshop-style modification, global photo optimization, and local editing. Extensive experimental results demonstrate that expressive instructions are crucial to instruction-based image editing, and our MGIE can lead to a notable improvement in automatic metrics and human evaluation while maintaining competitive inference efficiency.', 'authors': ['Zhe Gan', 'Yinfei Yang', 'William Yang Wang', 'Xianzhi Du', 'Wenze Hu', 'Tsu-Jui Fu'], 'published': '2023-09-29', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'reformulating-vision-language-foundation', 'arxiv_id': '2310.00653', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.00653v1', 'url_pdf': 'https://arxiv.org/pdf/2310.00653v1.pdf', 'title': 'Reformulating Vision-Language Foundation Models and Datasets Towards Universal Multimodal Assistants', 'abstract': \"Recent Multimodal Large Language Models (MLLMs) exhibit impressive abilities to perceive images and follow open-ended instructions. The capabilities of MLLMs depend on two crucial factors: the model architecture to facilitate the feature alignment of visual modules and large language models; the multimodal instruction tuning datasets for human instruction following. (i) For the model architecture, most existing models introduce an external bridge module to connect vision encoders with language models, which needs an additional feature-alignment pre-training. In this work, we discover that compact pre-trained vision language models can inherently serve as ``out-of-the-box'' bridges between vision and language. Based on this, we propose Muffin framework, which directly employs pre-trained vision-language models to act as providers of visual signals. (ii) For the multimodal instruction tuning datasets, existing methods omit the complementary relationship between different datasets and simply mix datasets from different tasks. Instead, we propose UniMM-Chat dataset which explores the complementarities of datasets to generate 1.1M high-quality and diverse multimodal instructions. We merge information describing the same image from diverse datasets and transforms it into more knowledge-intensive conversation data. Experimental results demonstrate the effectiveness of the Muffin framework and UniMM-Chat dataset. Muffin achieves state-of-the-art performance on a wide range of vision-language tasks, significantly surpassing state-of-the-art models like LLaVA and InstructBLIP. Our model and dataset are all accessible at https://github.com/thunlp/muffin.\", 'authors': ['Maosong Sun', 'Hai-Tao Zheng', 'Zhiyuan Liu', 'Dahai Li', 'Jiao Xue', 'Yinxv Pan', 'Shan Wang', 'Chongyi Wang', 'Yue Zhao', 'Haoye Zhang', 'Yuan YAO', 'Jinyi Hu', 'Tianyu Yu'], 'published': '2023-10-01', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'drivegpt4-interpretable-end-to-end-autonomous', 'arxiv_id': '2310.01412', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.01412v5', 'url_pdf': 'https://arxiv.org/pdf/2310.01412v5.pdf', 'title': 'DriveGPT4: Interpretable End-to-end Autonomous Driving via Large Language Model', 'abstract': 'Multimodal large language models (MLLMs) have emerged as a prominent area of interest within the research community, given their proficiency in handling and reasoning with non-textual data, including images and videos. This study seeks to extend the application of MLLMs to the realm of autonomous driving by introducing DriveGPT4, a novel interpretable end-to-end autonomous driving system based on LLMs. Capable of processing multi-frame video inputs and textual queries, DriveGPT4 facilitates the interpretation of vehicle actions, offers pertinent reasoning, and effectively addresses a diverse range of questions posed by users. Furthermore, DriveGPT4 predicts low-level vehicle control signals in an end-to-end fashion.These advanced capabilities are achieved through the utilization of a bespoke visual instruction tuning dataset, specifically tailored for autonomous driving applications, in conjunction with a mix-finetuning training strategy. DriveGPT4 represents the pioneering effort to leverage LLMs for the development of an interpretable end-to-end autonomous driving solution. Evaluations conducted on the BDD-X dataset showcase the superior qualitative and quantitative performance of DriveGPT4. Additionally, the fine-tuning of domain-specific data enables DriveGPT4 to yield close or even improved results in terms of autonomous driving grounding when contrasted with GPT4-V.', 'authors': ['Kwan-Yee. K. Wong', 'Hengshuang Zhao', 'Zhenguo Li', 'Yong Guo', 'Zhen Zhao', 'Enze Xie', 'Yujia Zhang', 'Zhenhua Xu'], 'published': '2023-10-02', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'minigpt-5-interleaved-vision-and-language', 'arxiv_id': '2310.02239', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.02239v3', 'url_pdf': 'https://arxiv.org/pdf/2310.02239v3.pdf', 'title': 'MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens', 'abstract': 'The effectiveness of Multimodal Large Language Models (MLLMs) demonstrates a profound capability in multimodal understanding. However, the simultaneous generation of images with coherent texts is still underdeveloped. Addressing this, we introduce a novel interleaved vision-and-language generation method, centered around the concept of ``generative vokens\". These vokens serve as pivotal elements contributing to coherent image-text outputs. Our method is marked by a unique two-stage training strategy for description-free multimodal generation, which does not necessitate extensive descriptions of images. We integrate classifier-free guidance to enhance the alignment of generated images and texts, ensuring more seamless and contextually relevant multimodal interactions. Our model, MiniGPT-5, exhibits substantial improvement over the baseline models on multimodal generation datasets, including MMDialog and VIST. The human evaluation shows MiniGPT-5 is better than the baseline model on more than 56\\\\% cases for multimodal generation, highlighting its efficacy across diverse benchmarks.', 'authors': ['Xin Eric Wang', 'Xuehai He', 'Kaizhi Zheng'], 'published': '2023-10-03', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'towards-end-to-end-embodied-decision-making', 'arxiv_id': '2310.02071', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.02071v4', 'url_pdf': 'https://arxiv.org/pdf/2310.02071v4.pdf', 'title': 'Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond', 'abstract': 'In this study, we explore the potential of Multimodal Large Language Models (MLLMs) in improving embodied decision-making processes for agents. While Large Language Models (LLMs) have been widely used due to their advanced reasoning skills and vast world knowledge, MLLMs like GPT4-Vision offer enhanced visual understanding and reasoning capabilities. We investigate whether state-of-the-art MLLMs can handle embodied decision-making in an end-to-end manner and whether collaborations between LLMs and MLLMs can enhance decision-making. To address these questions, we introduce a new benchmark called PCA-EVAL, which evaluates embodied decision-making from the perspectives of Perception, Cognition, and Action. Additionally, we propose HOLMES, a multi-agent cooperation framework that allows LLMs to leverage MLLMs and APIs to gather multimodal information for informed decision-making. We compare end-to-end embodied decision-making and HOLMES on our benchmark and find that the GPT4-Vision model demonstrates strong end-to-end embodied decision-making abilities, outperforming GPT4-HOLMES in terms of average decision accuracy (+3%). However, this performance is exclusive to the latest GPT4-Vision model, surpassing the open-source state-of-the-art MLLM by 26%. Our results indicate that powerful MLLMs like GPT4-Vision hold promise for decision-making in embodied agents, offering new avenues for MLLM research. Code and data are open at https://github.com/pkunlp-icler/PCA-EVAL/.', 'authors': ['Peiyi Wang', 'Baobao Chang', 'Tianyu Liu', 'Yuchi Wang', 'Zefan Cai', 'Haozhe Zhao', 'Shuhuai Ren', 'Yichi Zhang', 'Liang Chen'], 'published': '2023-10-03', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'kosmos-g-generating-images-in-context-with', 'arxiv_id': '2310.02992', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.02992v3', 'url_pdf': 'https://arxiv.org/pdf/2310.02992v3.pdf', 'title': 'Kosmos-G: Generating Images in Context with Multimodal Large Language Models', 'abstract': 'Recent advancements in subject-driven image generation have made significant strides. However, current methods still fall short in diverse application scenarios, as they require test-time tuning and cannot accept interleaved multi-image and text input. These limitations keep them far from the ultimate goal of \"image as a foreign language in image generation.\" This paper presents Kosmos-G, a model that leverages the advanced multimodal perception capabilities of Multimodal Large Language Models (MLLMs) to tackle the aforementioned challenge. Our approach aligns the output space of MLLM with CLIP using the textual modality as an anchor and performs compositional instruction tuning on curated data. Kosmos-G demonstrates an impressive capability of zero-shot subject-driven generation with interleaved multi-image and text input. Notably, the score distillation instruction tuning requires no modifications to the image decoder. This allows for a seamless substitution of CLIP and effortless integration with a myriad of U-Net techniques ranging from fine-grained controls to personalized image decoder variants. We posit Kosmos-G as an initial attempt towards the goal of \"image as a foreign language in image generation.\" The code can be found at https://aka.ms/Kosmos-G', 'authors': ['Furu Wei', 'Wenhu Chen', 'Zhiliang Peng', 'Shaohan Huang', 'Li Dong', 'Xichen Pan'], 'published': '2023-10-04', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'fine-grained-audio-visual-joint', 'arxiv_id': '2310.05863', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.05863v2', 'url_pdf': 'https://arxiv.org/pdf/2310.05863v2.pdf', 'title': 'Fine-grained Audio-Visual Joint Representations for Multimodal Large Language Models', 'abstract': 'Audio-visual large language models (LLM) have drawn significant attention, yet the fine-grained combination of both input streams is rather under-explored, which is challenging but necessary for LLMs to understand general video inputs. To this end, a fine-grained audio-visual joint representation (FAVOR) learning framework for multimodal LLMs is proposed in this paper, which extends a text-based LLM to simultaneously perceive speech and audio events in the audio input stream and images or videos in the visual input stream, at the frame level. To fuse the audio and visual feature streams into joint representations and to align the joint space with the LLM input embedding space, we propose a causal Q-Former structure with a causal attention module to enhance the capture of causal relations of the audio-visual frames across time. An audio-visual evaluation benchmark (AVEB) is also proposed which comprises six representative single-modal tasks with five cross-modal tasks reflecting audio-visual co-reasoning abilities. While achieving competitive single-modal performance on audio, speech and image tasks in AVEB, FAVOR achieved over 20% accuracy improvements on the video question-answering task when fine-grained information or temporal causal reasoning is required. FAVOR, in addition, demonstrated remarkable video comprehension and reasoning abilities on tasks that are unprecedented by other multimodal LLMs. An interactive demo of FAVOR is available at https://github.com/BriansIDP/AudioVisualLLM.git, and the training code and model checkpoints will be released soon.', 'authors': ['Chao Zhang', 'Zejun Ma', 'Lu Lu', 'Wei Li', 'Tian Tan', 'Xianzhao Chen', 'Changli Tang', 'Wenyi Yu', 'Guangzhi Sun'], 'published': '2023-10-09', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'can-we-edit-multimodal-large-language-models', 'arxiv_id': '2310.08475', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.08475v5', 'url_pdf': 'https://arxiv.org/pdf/2310.08475v5.pdf', 'title': 'Can We Edit Multimodal Large Language Models?', 'abstract': 'In this paper, we focus on editing Multimodal Large Language Models (MLLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights. Code and dataset are available in https://github.com/zjunlp/EasyEdit.', 'authors': ['Ningyu Zhang', 'Huajun Chen', 'Yongheng Wang', 'Xi Chen', 'Qingbin Liu', 'Bozhong Tian', 'Siyuan Cheng'], 'published': '2023-10-12', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'autorepo-a-general-framework-for-multi-modal', 'arxiv_id': '2310.07944', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.07944v2', 'url_pdf': 'https://arxiv.org/pdf/2310.07944v2.pdf', 'title': 'AutoRepo: A general framework for multi-modal LLM-based automated construction reporting', 'abstract': 'Ensuring the safety, quality, and timely completion of construction projects is paramount, with construction inspections serving as a vital instrument towards these goals. Nevertheless, the predominantly manual approach of present-day inspections frequently results in inefficiencies and inadequate information management. Such methods often fall short of providing holistic, exhaustive assessments, consequently engendering regulatory oversights and potential safety hazards. To address this issue, this paper presents a novel framework named AutoRepo for automated generation of construction inspection reports. The unmanned vehicles efficiently perform construction inspections and collect scene information, while the multimodal large language models (LLMs) are leveraged to automatically generate the inspection reports. The framework was applied and tested on a real-world construction site, demonstrating its potential to expedite the inspection process, significantly reduce resource allocation, and produce high-quality, regulatory standard-compliant inspection reports. This research thus underscores the immense potential of multimodal large language models in revolutionizing construction inspection practices, signaling a significant leap forward towards a more efficient and safer construction management paradigm.', 'authors': ['Heng Li', 'Runhao Guo', 'Jing Li', 'Xincong Yang', 'Hongxu Pu'], 'published': '2023-10-11', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'mm-bigbench-evaluating-multimodal-models-on', 'arxiv_id': '2310.09036', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.09036v1', 'url_pdf': 'https://arxiv.org/pdf/2310.09036v1.pdf', 'title': 'MM-BigBench: Evaluating Multimodal Models on Multimodal Content Comprehension Tasks', 'abstract': \"The popularity of multimodal large language models (MLLMs) has triggered a recent surge in research efforts dedicated to evaluating these models. Nevertheless, existing evaluation studies of MLLMs primarily focus on the comprehension and reasoning of unimodal (vision) content, neglecting performance evaluations in the domain of multimodal (vision-language) content understanding. Beyond multimodal reasoning, tasks related to multimodal content comprehension necessitate a profound understanding of multimodal contexts, achieved through the multimodal interaction to obtain a final answer. In this paper, we introduce a comprehensive assessment framework called MM-BigBench, which incorporates a diverse range of metrics to offer an extensive evaluation of the performance of various models and instructions across a wide spectrum of diverse multimodal content comprehension tasks. Consequently, our work complements research on the performance of MLLMs in multimodal comprehension tasks, achieving a more comprehensive and holistic evaluation of MLLMs. To begin, we employ the Best Performance metric to ascertain each model's performance upper bound on different datasets. Subsequently, the Mean Relative Gain metric offers an assessment of the overall performance of various models and instructions, while the Stability metric measures their sensitivity. Furthermore, previous research centers on evaluating models independently or solely assessing instructions, neglecting the adaptability between models and instructions. We propose the Adaptability metric to quantify the adaptability between models and instructions. Our paper evaluates a total of 20 language models (14 MLLMs) on 14 multimodal datasets spanning 6 tasks, with 10 instructions for each task, and derives novel insights. Our code will be released at https://github.com/declare-lab/MM-BigBench.\", 'authors': ['Soujanya Poria', 'XiaoMing Fu', 'Yifei Zhang', 'Qi Sun', 'Yang Li', 'Daling Wang', 'Ming Wang', 'Shi Feng', 'Wenfang Wu', 'Xiaocui Yang'], 'published': '2023-10-13', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'robollm-robotic-vision-tasks-grounded-on', 'arxiv_id': '2310.10221', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.10221v2', 'url_pdf': 'https://arxiv.org/pdf/2310.10221v2.pdf', 'title': 'RoboLLM: Robotic Vision Tasks Grounded on Multimodal Large Language Models', 'abstract': 'Robotic vision applications often necessitate a wide range of visual perception tasks, such as object detection, segmentation, and identification. While there have been substantial advances in these individual tasks, integrating specialized models into a unified vision pipeline presents significant engineering challenges and costs. Recently, Multimodal Large Language Models (MLLMs) have emerged as novel backbones for various downstream tasks. We argue that leveraging the pre-training capabilities of MLLMs enables the creation of a simplified framework, thus mitigating the need for task-specific encoders. Specifically, the large-scale pretrained knowledge in MLLMs allows for easier fine-tuning to downstream robotic vision tasks and yields superior performance. We introduce the RoboLLM framework, equipped with a BEiT-3 backbone, to address all visual perception tasks in the ARMBench challenge-a large-scale robotic manipulation dataset about real-world warehouse scenarios. RoboLLM not only outperforms existing baselines but also substantially reduces the engineering burden associated with model selection and tuning. The source code is publicly available at https://github.com/longkukuhi/armbench.', 'authors': ['Gerardo Aragon Camarasa', 'Richard McCreadie', 'George Killick', 'Zijun Long'], 'published': '2023-10-16', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'benchmarking-sequential-visual-input', 'arxiv_id': '2310.13473', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.13473v1', 'url_pdf': 'https://arxiv.org/pdf/2310.13473v1.pdf', 'title': 'Benchmarking Sequential Visual Input Reasoning and Prediction in Multimodal Large Language Models', 'abstract': \"Multimodal large language models (MLLMs) have shown great potential in perception and interpretation tasks, but their capabilities in predictive reasoning remain under-explored. To address this gap, we introduce a novel benchmark that assesses the predictive reasoning capabilities of MLLMs across diverse scenarios. Our benchmark targets three important domains: abstract pattern reasoning, human activity prediction, and physical interaction prediction. We further develop three evaluation methods powered by large language model to robustly quantify a model's performance in predicting and reasoning the future based on multi-visual context. Empirical experiments confirm the soundness of the proposed benchmark and evaluation methods via rigorous testing and reveal pros and cons of current popular MLLMs in the task of predictive reasoning. Lastly, our proposed benchmark provides a standardized evaluation framework for MLLMs and can facilitate the development of more advanced models that can reason and predict over complex long sequence of multimodal input.\", 'authors': ['Jianwei Yin', 'Tiancheng Zhao', 'Kangjia Zhao', 'Yu Shu', 'Leigang Sha', 'Mingwei Zhu'], 'published': '2023-10-20', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'woodpecker-hallucination-correction-for', 'arxiv_id': '2310.16045', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.16045v2', 'url_pdf': 'https://arxiv.org/pdf/2310.16045v2.pdf', 'title': 'Woodpecker: Hallucination Correction for Multimodal Large Language Models', 'abstract': 'Hallucination is a big shadow hanging over the rapidly evolving Multimodal Large Language Models (MLLMs), referring to the phenomenon that the generated text is inconsistent with the image content. In order to mitigate hallucinations, existing studies mainly resort to an instruction-tuning manner that requires retraining the models with specific data. In this paper, we pave a different way, introducing a training-free method named Woodpecker. Like a woodpecker heals trees, it picks out and corrects hallucinations from the generated text. Concretely, Woodpecker consists of five stages: key concept extraction, question formulation, visual knowledge validation, visual claim generation, and hallucination correction. Implemented in a post-remedy manner, Woodpecker can easily serve different MLLMs, while being interpretable by accessing intermediate outputs of the five stages. We evaluate Woodpecker both quantitatively and qualitatively and show the huge potential of this new paradigm. On the POPE benchmark, our method obtains a 30.66%/24.33% improvement in accuracy over the baseline MiniGPT-4/mPLUG-Owl. The source code is released at https://github.com/BradyFU/Woodpecker.', 'authors': ['Enhong Chen', 'Xing Sun', 'Ke Li', 'Yunhang Shen', 'Dianbo Sui', 'Hao Wang', 'Tong Xu', 'Sirui Zhao', 'Chaoyou Fu', 'Shukang Yin'], 'published': '2023-10-24', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'visual-cropping-improves-zero-shot-question', 'arxiv_id': '2310.16033', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2310.16033v3', 'url_pdf': 'https://arxiv.org/pdf/2310.16033v3.pdf', 'title': 'Towards Perceiving Small Visual Details in Zero-shot Visual Question Answering with Multimodal LLMs', 'abstract': \"Multimodal Large Language Models (MLLMs) have recently achieved promising zero-shot accuracy on visual question answering (VQA) -- a fundamental task affecting various downstream applications and domains. Given the great potential for the broad use of these models, it is important to investigate their limitations in dealing with different image and question properties. In this work, we investigate whether MLLMs can perceive small details as well as large details in images. In particular, we show that their zero-shot accuracy in answering visual questions is very sensitive to the size of the visual subject of the question, declining up to 46% with size. Furthermore, we show that this effect is causal by observing that human visual cropping can significantly mitigate their sensitivity to size. Inspired by the usefulness of human cropping, we then propose five automatic visual cropping methods -- leveraging either external localization models or the decision process of the given MLLM itself -- as inference time mechanisms to improve the zero-shot performance of MLLMs. We study their effectiveness on four popular VQA datasets, and a subset of the VQAv2 dataset tailored towards fine visual details. Our findings suggest that MLLMs should be used with caution in detail-sensitive VQA applications, and that visual cropping is a promising direction to improve their zero-shot performance. To facilitate further investigation of MLLMs' behaviors, our code and data are publicly released.\", 'authors': ['Filip Ilievski', 'Prateek Chhikara', 'Mahyar Khayatkhoei', 'Jiarui Zhang'], 'published': '2023-10-24', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'chef-a-comprehensive-evaluation-framework-for', 'arxiv_id': '2311.02692', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2311.02692v1', 'url_pdf': 'https://arxiv.org/pdf/2311.02692v1.pdf', 'title': 'ChEF: A Comprehensive Evaluation Framework for Standardized Assessment of Multimodal Large Language Models', 'abstract': \"Multimodal Large Language Models (MLLMs) have shown impressive abilities in interacting with visual content with myriad potential downstream tasks. However, even though a list of benchmarks has been proposed, the capabilities and limitations of MLLMs are still not comprehensively understood, due to a lack of a standardized and holistic evaluation framework. To this end, we present the first Comprehensive Evaluation Framework (ChEF) that can holistically profile each MLLM and fairly compare different MLLMs. First, we structure ChEF as four modular components, i.e., Scenario as scalable multimodal datasets, Instruction as flexible instruction retrieving formulae, Inferencer as reliable question answering strategies, and Metric as indicative task-specific score functions. Based on them, ChEF facilitates versatile evaluations in a standardized framework, and new evaluations can be built by designing new Recipes (systematic selection of these four components). Notably, current MLLM benchmarks can be readily summarized as recipes of ChEF. Second, we introduce 6 new recipes to quantify competent MLLMs' desired capabilities (or called desiderata, i.e., calibration, in-context learning, instruction following, language performance, hallucination, and robustness) as reliable agents that can perform real-world multimodal interactions. Third, we conduct a large-scale evaluation of 9 prominent MLLMs on 9 scenarios and 6 desiderata. Our evaluation summarized over 20 valuable observations concerning the generalizability of MLLMs across various scenarios and the composite capability of MLLMs required for multimodal interactions. We will publicly release all the detailed implementations for further analysis, as well as an easy-to-use modular toolkit for the integration of new recipes and models, so that ChEF can be a growing evaluation framework for the MLLM community.\", 'authors': ['Jing Shao', 'Yu Qiao', 'Lu Sheng', 'Zhenfei Yin', 'Hongxing Fan', 'Zhipin Wang', 'Zhelun Shi'], 'published': '2023-11-05', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'octavius-mitigating-task-interference-in', 'arxiv_id': '2311.02684', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2311.02684v3', 'url_pdf': 'https://arxiv.org/pdf/2311.02684v3.pdf', 'title': 'Octavius: Mitigating Task Interference in MLLMs via LoRA-MoE', 'abstract': 'Recent studies have demonstrated Large Language Models (LLMs) can extend their zero-shot generalization capabilities to multimodal learning through instruction tuning. As more modalities and downstream tasks are introduced, negative conflicts and interference may have a worse impact on performance. While this phenomenon has been overlooked in previous work, we propose a novel and extensible framework, called Octavius, for comprehensive studies and experimentation on multimodal learning with Multimodal Large Language Models (MLLMs). Specifically, we combine the well-known Mixture-of-Experts (MoE) and one of the representative PEFT techniques, i.e., LoRA, designing a novel LLM-based decoder, called LoRA-MoE, for multimodal learning. To the best of our knowledge, we are one of the pioneering efforts to introduce MoE into MLLMs to address this problem. The experimental results (about 20% improvement) have shown the effectiveness and versatility of our design in various 2D and 3D downstream tasks. Code and datasets are available at https://openlamm.github.io/tutorial/.', 'authors': ['Jing Shao', 'Yu Qiao', 'Wanli Ouyang', 'Lu Sheng', 'Si Liu', 'Zhenfei Yin', 'Huayang Liu', 'Zhen Wang', 'Ziqin Wang', 'Zeren Chen'], 'published': '2023-11-05', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}, {'id': 'what-large-language-models-bring-to-text-rich', 'arxiv_id': '2311.07306', 'nips_id': None, 'url_abs': 'https://arxiv.org/abs/2311.07306v1', 'url_pdf': 'https://arxiv.org/pdf/2311.07306v1.pdf', 'title': 'What Large Language Models Bring to Text-rich VQA?', 'abstract': \"Text-rich VQA, namely Visual Question Answering based on text recognition in the images, is a cross-modal task that requires both image comprehension and text recognition. In this work, we focus on investigating the advantages and bottlenecks of LLM-based approaches in addressing this problem. To address the above concern, we separate the vision and language modules, where we leverage external OCR models to recognize texts in the image and Large Language Models (LLMs) to answer the question given texts. The whole framework is training-free benefiting from the in-context ability of LLMs. This pipeline achieved superior performance compared to the majority of existing Multimodal Large Language Models (MLLM) on four text-rich VQA datasets. Besides, based on the ablation study, we find that LLM brings stronger comprehension ability and may introduce helpful knowledge for the VQA problem. The bottleneck for LLM to address text-rich VQA problems may primarily lie in visual part. We also combine the OCR module with MLLMs and pleasantly find that the combination of OCR module with MLLM also works. It's worth noting that not all MLLMs can comprehend the OCR information, which provides insights into how to train an MLLM that preserves the abilities of LLM.\", 'authors': ['Fei Tan', 'Zechao Li', 'Rui Zhao', 'Jinghui Lu', 'Xinzhe Ni', 'Wei Tang', 'Xuejing Liu'], 'published': '2023-11-13', 'conference': None, 'conference_url_abs': None, 'conference_url_pdf': None, 'proceeding': None}], 'docs': [Document(metadata={'title': 'Multimodal large language models for inclusive collaboration learning tasks'}, page_content='This PhD project leverages advancements in multimodal large language models to build an inclusive collaboration feedback loop, in order to facilitate the automated detection, modeling, and feedback for participants developing general collaboration skills. This topic is important given the role of collaboration as an essential 21st century skill, the potential to ground large language models within learning theory and real-world practice, and the expressive potential of transformer models to support equity and inclusion. We address some concerns of integrating advances in natural language processing into downstream tasks such as the learning analytics feedback loop.'), Document(metadata={'title': 'Personality-aware Human-centric Multimodal Reasoning: A New Task, Dataset and Baselines'}, page_content=\"Personality traits, emotions, and beliefs shape individuals' behavioral choices and decision-making processes. However, for one thing, the affective computing community normally focused on predicting personality traits but overlooks their application in behavior prediction. For another, the multimodal reasoning task emphasized the prediction of future states and behaviors but often neglected the incorporation of individual personality traits. In this work, we introduce a new task called Personality-aware Human-centric Multimodal Reasoning (PHMR) (T1), with the goal of forecasting the future behavior of a particular individual using multimodal information from past instances, while integrating personality factors. We accordingly construct a new dataset based on six television shows, encompassing 225 characters and 12k samples. To establish a benchmark for the task, we propose seven baseline methods: three adapted from related tasks, two pre-trained model, and two multimodal large language models. The experimental results demonstrate that incorporating personality traits enhances human-centric multimodal reasoning performance. To further solve the lack of personality annotation in real-life scenes, we introduce an extension task called Personality-predicted Human-centric Multimodal Reasoning task (T2) along with the corresponding dataset and method. We will make our dataset and code available on GitHub.\"), Document(metadata={'title': 'Visual Transformation Telling'}, page_content='Humans can naturally reason from superficial state differences (e.g. ground wetness) to transformations descriptions (e.g. raining) according to their life experience. In this paper, we propose a new visual reasoning task to test this transformation reasoning ability in real-world scenarios, called \\\\textbf{V}isual \\\\textbf{T}ransformation \\\\textbf{T}elling (VTT). Given a series of states (i.e. images), VTT requires to describe the transformation occurring between every two adjacent states. Different from existing visual reasoning tasks that focus on surface state reasoning, the advantage of VTT is that it captures the underlying causes, e.g. actions or events, behind the differences among states. We collect a novel dataset to support the study of transformation reasoning from two existing instructional video datasets, CrossTask and COIN, comprising 13,547 samples. Each sample involves the key state images along with their transformation descriptions. Our dataset covers diverse real-world activities, providing a rich resource for training and evaluation. To construct an initial benchmark for VTT, we test several models, including traditional visual storytelling methods (CST, GLACNet, Densecap) and advanced multimodal large language models (LLaVA v1.5-7B, Qwen-VL-chat, Gemini Pro Vision, GPT-4o, and GPT-4). Experimental results reveal that even state-of-the-art models still face challenges in VTT, highlighting substantial areas for improvement.'), Document(metadata={'title': 'Listen, Think, and Understand'}, page_content='The ability of artificial intelligence (AI) systems to perceive and comprehend audio signals is crucial for many applications. Although significant progress has been made in this area since the development of AudioSet, most existing models are designed to map audio inputs to pre-defined, discrete sound label sets. In contrast, humans possess the ability to not only classify sounds into general categories, but also to listen to the finer details of the sounds, explain the reason for the predictions, think about what the sound infers, and understand the scene and what action needs to be taken, if any. Such capabilities beyond perception are not yet present in existing audio models. On the other hand, modern large language models (LLMs) exhibit emerging reasoning ability but they lack audio perception capabilities. Therefore, we ask the question: can we build a model that has both audio perception and a reasoning ability? In this paper, we propose a new audio foundation model, called LTU (Listen, Think, and Understand). To train LTU, we created a new OpenAQA-5M dataset consisting of 1.9 million closed-ended and 3.7 million open-ended, diverse (audio, question, answer) tuples, and have used an autoregressive training framework with a perception-to-understanding curriculum. LTU demonstrates strong performance and generalization ability on conventional audio tasks such as classification and captioning. More importantly, it exhibits emerging audio reasoning and comprehension abilities that are absent in existing audio models. To the best of our knowledge, LTU is one of the first multimodal large language models that focus on general audio (rather than just speech) understanding.'), Document(metadata={'title': 'What Makes for Good Visual Tokenizers for Large Language Models?'}, page_content='We empirically investigate proper pre-training methods to build good visual tokenizers, making Large Language Models (LLMs) powerful Multimodal Large Language Models (MLLMs). In our benchmark, which is curated to evaluate MLLMs visual semantic understanding and fine-grained perception capabilities, we discussed different visual tokenizers pre-trained with dominant methods (i.e., DeiT, CLIP, MAE, DINO), and observe that: i) Fully/weakly supervised models capture more semantics than self-supervised models, but the gap is narrowed by scaling up the pre-training dataset. ii) Self-supervised models are better at fine-grained perception, where patch-level supervision is particularly effective. iii) Tuning the visual tokenizer leads to the loss of semantics obtained from large-scale pretraining, which is unfavorable with relatively small-scale instruction-tuning dataset. Given the findings, we reviewed methods that attempted to unify semantics and fine-grained visual understanding, e.g., patch-level feature distillation with semantically-rich targets. We obtain an intriguing insight mask-based strategies that were once all the rage may not be applicable for obtaining good visual tokenizers. Based on this critical observation, we obtain a new MLLM equipped with a tailored Good Visual Tokenizer (GVT), which exhibits strong visual comprehension capability at multiple scales. In particular, without introducing extra parameters and task-specific fine-tuning, GVT achieves superior performance on visual question answering, image captioning, and other fine-grained visual understanding tasks such as object counting and multi-class identification.'), Document(metadata={'title': 'Dynamic Clue Bottlenecks: Towards Interpretable-by-Design Visual Question Answering'}, page_content='Recent advances in multimodal large language models (LLMs) have shown extreme effectiveness in visual question answering (VQA). However, the design nature of these end-to-end models prevents them from being interpretable to humans, undermining trust and applicability in critical domains. While post-hoc rationales offer certain insight into understanding model behavior, these explanations are not guaranteed to be faithful to the model. In this paper, we address these shortcomings by introducing an interpretable by design model that factors model decisions into intermediate human-legible explanations, and allows people to easily understand why a model fails or succeeds. We propose the Dynamic Clue Bottleneck Model ( (DCLUB), a method that is designed towards an inherently interpretable VQA system. DCLUB provides an explainable intermediate space before the VQA decision and is faithful from the beginning, while maintaining comparable performance to black-box systems. Given a question, DCLUB first returns a set of visual clues: natural language statements of visually salient evidence from the image, and then generates the output based solely on the visual clues. To supervise and evaluate the generation of VQA explanations within DCLUB, we collect a dataset of 1.7k reasoning-focused questions with visual clues. Evaluations show that our inherently interpretable system can improve 4.64% over a comparable black-box system in reasoning-focused questions while preserving 99.43% of performance on VQA-v2.'), Document(metadata={'title': 'PathAsst: A Generative Foundation AI Assistant Towards Artificial General Intelligence of Pathology'}, page_content=\"As advances in large language models (LLMs) and multimodal techniques continue to mature, the development of general-purpose multimodal large language models (MLLMs) has surged, offering significant applications in interpreting natural images. However, the field of pathology has largely remained untapped, particularly in gathering high-quality data and designing comprehensive model frameworks. To bridge the gap in pathology MLLMs, we present PathAsst, a multimodal generative foundation AI assistant to revolutionize diagnostic and predictive analytics in pathology. The development of PathAsst involves three pivotal steps: data acquisition, CLIP model adaptation, and the training of PathAsst's multimodal generative capabilities. Firstly, we collect over 207K high-quality pathology image-text pairs from authoritative sources. Leveraging the advanced power of ChatGPT, we generate over 180K instruction-following samples. Furthermore, we devise additional instruction-following data specifically tailored for invoking eight pathology-specific sub-models we prepared, allowing the PathAsst to effectively collaborate with these models, enhancing its diagnostic ability. Secondly, by leveraging the collected data, we construct PathCLIP, a pathology-dedicated CLIP, to enhance PathAsst's capabilities in interpreting pathology images. Finally, we integrate PathCLIP with the Vicuna-13b and utilize pathology-specific instruction-tuning data to enhance the multimodal generation capacity of PathAsst and bolster its synergistic interactions with sub-models. The experimental results of PathAsst show the potential of harnessing AI-powered generative foundation model to improve pathology diagnosis and treatment processes.\"), Document(metadata={'title': 'Contextual Object Detection with Multimodal Large Language Models'}, page_content='Recent Multimodal Large Language Models (MLLMs) are remarkable in vision-language tasks, such as image captioning and question answering, but lack the essential perception ability, i.e., object detection. In this work, we address this limitation by introducing a novel research problem of contextual object detection -- understanding visible objects within different human-AI interactive contexts. Three representative scenarios are investigated, including the language cloze test, visual captioning, and question answering. Moreover, we present ContextDET, a unified multimodal model that is capable of end-to-end differentiable modeling of visual-language contexts, so as to locate, identify, and associate visual objects with language inputs for human-AI interaction. Our ContextDET involves three key submodels: (i) a visual encoder for extracting visual representations, (ii) a pre-trained LLM for multimodal context decoding, and (iii) a visual decoder for predicting bounding boxes given contextual object words. The new generate-then-detect framework enables us to detect object words within human vocabulary. Extensive experiments show the advantages of ContextDET on our proposed CODE benchmark, open-vocabulary detection, and referring image segmentation. Github: https://github.com/yuhangzang/ContextDET.'), Document(metadata={'title': 'A Survey on Multimodal Large Language Models'}, page_content='Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.'), Document(metadata={'title': 'MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models'}, page_content='Multimodal Large Language Model (MLLM) relies on the powerful LLM to perform multimodal tasks, showing amazing emergent abilities in recent studies, such as writing poems based on an image. However, it is difficult for these case studies to fully reflect the performance of MLLM, lacking a comprehensive evaluation. In this paper, we fill in this blank, presenting the first comprehensive MLLM Evaluation benchmark MME. It measures both perception and cognition abilities on a total of 14 subtasks. In order to avoid data leakage that may arise from direct use of public datasets for evaluation, the annotations of instruction-answer pairs are all manually designed. The concise instruction design allows us to fairly compare MLLMs, instead of struggling in prompt engineering. Besides, with such an instruction, we can also easily carry out quantitative statistics. A total of 30 advanced MLLMs are comprehensively evaluated on our MME, which not only suggests that existing MLLMs still have a large room for improvement, but also reveals the potential directions for the subsequent model optimization. The data application manner and online leaderboards are released at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation.'), Document(metadata={'title': 'Kosmos-2: Grounding Multimodal Large Language Models to the World'}, page_content=\"We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. Specifically, we represent refer expressions as links in Markdown, i.e., ``[text span](bounding boxes)'', where object descriptions are sequences of location tokens. Together with multimodal corpora, we construct large-scale data of grounded image-text pairs (called GrIT) to train the model. In addition to the existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), Kosmos-2 integrates the grounding capability into downstream applications. We evaluate Kosmos-2 on a wide range of tasks, including (i) multimodal grounding, such as referring expression comprehension, and phrase grounding, (ii) multimodal referring, such as referring expression generation, (iii) perception-language tasks, and (iv) language understanding and generation. This work lays out the foundation for the development of Embodiment AI and sheds light on the big convergence of language, multimodal perception, action, and world modeling, which is a key step toward artificial general intelligence. Code and pretrained models are available at https://aka.ms/kosmos-2.\"), Document(metadata={'title': \"Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic\"}, page_content=\"In human conversations, individuals can indicate relevant regions within a scene while addressing others. In turn, the other person can then respond by referring to specific regions if necessary. This natural referential ability in dialogue remains absent in current Multimodal Large Language Models (MLLMs). To fill this gap, this paper proposes an MLLM called Shikra, which can handle spatial coordinate inputs and outputs in natural language. Its architecture consists of a vision encoder, an alignment layer, and a LLM. It is designed to be straightforward and simple, without the need for extra vocabularies, position encoder, pre-/post-detection modules, or external plug-in models. All inputs and outputs are in natural language form. Referential dialogue is a superset of various vision-language (VL) tasks. Shikra can naturally handle location-related tasks like REC and PointQA, as well as conventional VL tasks such as Image Captioning and VQA. Experimental results showcase Shikra's promising performance. Furthermore, it enables numerous exciting applications, like providing mentioned objects' coordinates in chains of thoughts and comparing user-pointed regions similarities. Our code, model and dataset are accessed at https://github.com/shikras/shikra.\"), Document(metadata={'title': 'SVIT: Scaling up Visual Instruction Tuning'}, page_content=\"Thanks to the emerging of foundation models, the large language and vision models are integrated to acquire the multimodal ability of visual captioning, question answering, etc. Although existing multimodal models present impressive performance of visual understanding and reasoning, their limits are still largely under-explored due to the scarcity of high-quality instruction tuning data. To push the limits of multimodal capability, we Scale up Visual Instruction Tuning (SVIT) by constructing a dataset of 4.2 million visual instruction tuning data including 1.6M conversation question-answer (QA) pairs, 1.6M complex reasoning QA pairs, 1.0M referring QA pairs and 106K detailed image descriptions. Besides the volume, the proposed dataset is also featured by the high quality and rich diversity, which is generated by prompting GPT-4 with the abundant manual annotations of images. We also propose a new data recipe to select subset with better diversity and balance, which evokes model's superior capabilities. Extensive experiments verify that SVIT-v1.5, trained on the proposed dataset, outperforms state-of-the-art Multimodal Large Language Models on popular benchmarks. The data and code are publicly available at https://github.com/BAAI-DCAI/Visual-Instruction-Tuning.\"), Document(metadata={'title': 'ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning'}, page_content=\"Human-AI interactivity is a critical aspect that reflects the usability of multimodal large language models (MLLMs). However, existing end-to-end MLLMs only allow users to interact with them through language instructions, leading to the limitation of the interactive accuracy and efficiency. In this study, we present precise referring instructions that utilize diverse reference representations such as points and boxes as referring prompts to refer to the special region. This enables MLLMs to focus on the region of interest and achieve finer-grained interaction. Based on precise referring instruction, we propose ChatSpot, a unified end-to-end multimodal large language model that supports diverse forms of interactivity including mouse clicks, drag-and-drop, and drawing boxes, which provides a more flexible and seamless interactive experience. We also construct a multi-grained vision-language instruction-following dataset based on existing datasets and GPT-4 generating. Furthermore, we design a series of evaluation tasks to assess the effectiveness of region recognition and interaction. Experimental results showcase ChatSpot's promising performance.\"), Document(metadata={'title': 'SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension'}, page_content='Based on powerful Large Language Models (LLMs), recent generative Multimodal Large Language Models (MLLMs) have gained prominence as a pivotal research area, exhibiting remarkable capability for both comprehension and generation. In this work, we address the evaluation of generative comprehension in MLLMs as a preliminary step towards a comprehensive assessment of generative models, by introducing a benchmark named SEED-Bench. SEED-Bench consists of 19K multiple choice questions with accurate human annotations (x 6 larger than existing benchmarks), which spans 12 evaluation dimensions including the comprehension of both the image and video modality. We develop an advanced pipeline for generating multiple-choice questions that target specific evaluation dimensions, integrating both automatic filtering and manual verification processes. Multiple-choice questions with groundtruth options derived from human annotation enables an objective and efficient assessment of model performance, eliminating the need for human or GPT intervention during evaluation. We further evaluate the performance of 18 models across all 12 dimensions, covering both the spatial and temporal understanding. By revealing the limitations of existing MLLMs through evaluation results, we aim for SEED-Bench to provide insights for motivating future research. We will launch and consistently maintain a leaderboard to provide a platform for the community to assess and investigate model capability.'), Document(metadata={'title': 'LISA: Reasoning Segmentation via Large Language Model'}, page_content='Although perception systems have made remarkable advancements in recent years, they still rely on explicit human instruction or pre-defined categories to identify the target objects before executing visual recognition tasks. Such systems cannot actively reason and comprehend implicit user intention. In this work, we propose a new segmentation task -- reasoning segmentation. The task is designed to output a segmentation mask given a complex and implicit query text. Furthermore, we establish a benchmark comprising over one thousand image-instruction-mask data samples, incorporating intricate reasoning and world knowledge for evaluation purposes. Finally, we present LISA: large Language Instructed Segmentation Assistant, which inherits the language generation capabilities of multimodal Large Language Models (LLMs) while also possessing the ability to produce segmentation masks. We expand the original vocabulary with a <SEG> token and propose the embedding-as-mask paradigm to unlock the segmentation capability. Remarkably, LISA can handle cases involving complex reasoning and world knowledge. Also, it demonstrates robust zero-shot capability when trained exclusively on reasoning-free datasets. In addition, fine-tuning the model with merely 239 reasoning segmentation data samples results in further performance enhancement. Both quantitative and qualitative experiments show our method effectively unlocks new reasoning segmentation capabilities for multimodal LLMs. Code, models, and data are available at https://github.com/dvlab-research/LISA.'), Document(metadata={'title': 'Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions'}, page_content=\"Recent advancements in Multimodal Large Language Models (MLLMs) have been utilizing Visual Prompt Generators (VPGs) to convert visual features into tokens that LLMs can recognize. This is achieved by training the VPGs on millions of image-caption pairs, where the VPG-generated tokens of images are fed into a frozen LLM to generate the corresponding captions. However, this image-captioning based training objective inherently biases the VPG to concentrate solely on the primary visual contents sufficient for caption generation, often neglecting other visual details. This shortcoming results in MLLMs' underperformance in comprehending demonstrative instructions consisting of multiple, interleaved, and multimodal instructions that demonstrate the required context to complete a task. To address this issue, we introduce a generic and lightweight Visual Prompt Generator Complete module (VPG-C), which can infer and complete the missing details essential for comprehending demonstrative instructions. Further, we propose a synthetic discriminative training strategy to fine-tune VPG-C, eliminating the need for supervised demonstrative instructions. As for evaluation, we build DEMON, a comprehensive benchmark for demonstrative instruction understanding. Synthetically trained with the proposed strategy, VPG-C achieves significantly stronger zero-shot performance across all tasks of DEMON. Further evaluation on the MME and OwlEval benchmarks also demonstrate the superiority of VPG-C. Our benchmark, code, and pre-trained models are available at https://github.com/DCDmllm/Cheetah.\"), Document(metadata={'title': 'Link-Context Learning for Multimodal LLMs'}, page_content='The ability to learn from context with novel concepts, and deliver appropriate responses are essential in human conversations. Despite current Multimodal Large Language Models (MLLMs) and Large Language Models (LLMs) being trained on mega-scale datasets, recognizing unseen images or understanding novel concepts in a training-free manner remains a challenge. In-Context Learning (ICL) explores training-free few-shot learning, where models are encouraged to ``learn to learn\" from limited tasks and generalize to unseen tasks. In this work, we propose link-context learning (LCL), which emphasizes \"reasoning from cause and effect\" to augment the learning capabilities of MLLMs. LCL goes beyond traditional ICL by explicitly strengthening the causal relationship between the support set and the query set. By providing demonstrations with causal links, LCL guides the model to discern not only the analogy but also the underlying causal associations between data points, which empowers MLLMs to recognize unseen images and understand novel concepts more effectively. To facilitate the evaluation of this novel approach, we introduce the ISEKAI dataset, comprising exclusively of unseen generated image-label pairs designed for link-context learning. Extensive experiments show that our LCL-MLLM exhibits strong link-context learning capabilities to novel concepts over vanilla MLLMs. Code and data will be released at https://github.com/isekai-portal/Link-Context-Learning.'), Document(metadata={'title': 'FashionLOGO: Prompting Multimodal Large Language Models for Fashion Logo Embeddings'}, page_content=\"Logo embedding models convert the product logos in images into vectors, enabling their utilization for logo recognition and detection within e-commerce platforms. This facilitates the enforcement of intellectual property rights and enhances product search capabilities. However, current methods treat logo embedding as a purely visual problem. A noteworthy issue is that visual models capture features more than logos. Instead, we view this as a multimodal task, using text as auxiliary information to facilitate the visual model's understanding of the logo. The emerging Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in both visual and textual understanding. Inspired by this, we propose an approach, \\\\textbf{FashionLOGO}, to explore how to prompt MLLMs to generate appropriate text for product images, which can help visual models achieve better logo embeddings. We adopt a cross-attention transformer block that enables visual embedding to automatically learn supplementary knowledge from textual embedding. Our extensive experiments on real-world datasets prove that FashionLOGO is capable of generating generic and robust logo embeddings, achieving state-of-the-art performance in all benchmarks.\"), Document(metadata={'title': 'WanJuan: A Comprehensive Multimodal Dataset for Advancing English and Chinese Large Models'}, page_content='The rise in popularity of ChatGPT and GPT-4 has significantly accelerated the development of large models, leading to the creation of numerous impressive large language models(LLMs) and multimodal large language models (MLLMs). These cutting-edge models owe their remarkable performance to high-quality data. However, the details of the training data used in leading paradigms are often kept confidential. This lack of transparency, coupled with the scarcity of open-source data, impedes further developments within the community. As a response, this paper presents \"Wan Juan\", a large-scale multimodal dataset composed of both Chinese and English data, collected from a wide range of web sources. The dataset incorporates text, image-text, and video modalities, with a total volume exceeding 2TB. It was utilized in the training of InternLM, a model that demonstrated significant advantages in multi-dimensional evaluations when compared to models of a similar scale. All data can be accessed at https://opendatalab.org.cn/WanJuan1.0.'), Document(metadata={'title': 'StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data'}, page_content=\"The remarkable multimodal capabilities demonstrated by OpenAI's GPT-4 have sparked significant interest in the development of multimodal Large Language Models (LLMs). A primary research objective of such models is to align visual and textual modalities effectively while comprehending human instructions. Current methodologies often rely on annotations derived from benchmark datasets to construct image-dialogue datasets for training purposes, akin to instruction tuning in LLMs. However, these datasets often exhibit domain bias, potentially constraining the generative capabilities of the models. In an effort to mitigate these limitations, we propose a novel data collection methodology that synchronously synthesizes images and dialogues for visual instruction tuning. This approach harnesses the power of generative models, marrying the abilities of ChatGPT and text-to-image generative models to yield a diverse and controllable dataset with varied image content. Additionally, datasets can be arbitrarily scaled. This not only provides greater flexibility compared to existing methodologies but also significantly enhances several model capabilities. Our research includes comprehensive experiments conducted on various datasets. The results emphasize substantial enhancements in more than ten commonly assessed capabilities. Additionally, our model achieves state-of-the-art results across multiple widely recognized multimodal benchmarks.\"), Document(metadata={'title': 'InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4'}, page_content='Multimodal large language models are typically trained in two stages: first pre-training on image-text pairs, and then fine-tuning using supervised vision-language instruction data. Recent studies have shown that large language models can achieve satisfactory results even with a limited amount of high-quality instruction-following data. In this paper, we introduce InstructionGPT-4, which is fine-tuned on a small dataset comprising only 200 examples, amounting to approximately 6\\\\% of the instruction-following data used in the alignment dataset for MiniGPT-4. To achieve this, we first propose several metrics to access the quality of multimodal instruction data. Based on these metrics, we present an effective and trainable data selector to automatically identify and filter low-quality vision-language data. By employing this method, InstructionGPT-4 outperforms the original MiniGPT-4 on various evaluations. Overall, our findings demonstrate that less but high-quality instruction tuning data is efficient in enabling multimodal large language models to generate better output. Our code is available at https://github.com/waltonfuture/InstructionGPT-4.'), Document(metadata={'title': 'VIGC: Visual Instruction Generation and Correction'}, page_content=\"The integration of visual encoders and large language models (LLMs) has driven recent progress in multimodal large language models (MLLMs). However, the scarcity of high-quality instruction-tuning data for vision-language tasks remains a challenge. The current leading paradigm, such as LLaVA, relies on language-only GPT-4 to generate data, which requires pre-annotated image captions and detection bounding boxes, suffering from understanding image details. A practical solution to this problem would be to utilize the available multimodal large language models (MLLMs) to generate instruction data for vision-language tasks. However, it's worth noting that the currently accessible MLLMs are not as powerful as their LLM counterparts, as they tend to produce inadequate responses and generate false information. As a solution for addressing the current issue, this paper proposes the Visual Instruction Generation and Correction (VIGC) framework that enables multimodal large language models to generate instruction-tuning data and progressively enhance its quality on-the-fly. Specifically, Visual Instruction Generation (VIG) guides the vision-language model to generate diverse instruction-tuning data. To ensure generation quality, Visual Instruction Correction (VIC) adopts an iterative update mechanism to correct any inaccuracies in data produced by VIG, effectively reducing the risk of hallucination. Leveraging the diverse, high-quality data generated by VIGC, we finetune mainstream models and validate data quality based on various evaluations. Experimental results demonstrate that VIGC not only compensates for the shortcomings of language-only data generation methods, but also effectively enhances the benchmark performance. The models, datasets, and code are available at https://opendatalab.github.io/VIGC.\"), Document(metadata={'title': 'Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models'}, page_content='Recently, Multimodal Large Language Models (MLLMs) that enable Large Language Models (LLMs) to interpret images through visual instruction tuning have achieved significant success. However, existing visual instruction tuning methods only utilize image-language instruction data to align the language and image modalities, lacking a more fine-grained cross-modal alignment. In this paper, we propose Position-enhanced Visual Instruction Tuning (PVIT), which extends the functionality of MLLMs by integrating an additional region-level vision encoder. This integration promotes a more detailed comprehension of images for the MLLM. In addition, to efficiently achieve a fine-grained alignment between the vision modules and the LLM, we design multiple data generation strategies to construct an image-region-language instruction dataset. Finally, we present both quantitative experiments and qualitative analysis that demonstrate the superiority of the proposed model. Code and data will be released at https://github.com/PVIT-official/PVIT.'), Document(metadata={'title': 'MLLM-DataEngine: An Iterative Refinement Approach for MLLM'}, page_content='Despite the great advance of Multimodal Large Language Models (MLLMs) in both instruction dataset building and benchmarking, the independence of training and evaluation makes current MLLMs hard to further improve their capability under the guidance of evaluation results with a relatively low human cost. In this paper, we propose MLLM-DataEngine, a novel closed-loop system that bridges data generation, model training, and evaluation. Within each loop iteration, the MLLM-DataEngine first analyze the weakness of the model based on the evaluation results, then generate a proper incremental dataset for the next training iteration and enhance the model capability iteratively. Compared with previous data collection methods which are separate from the benchmarking, the data generated by MLLM-DataEngine shows better targeting, quality, and correctness. For targeting, we propose an Adaptive Bad-case Sampling module, which adjusts the ratio of different types of data within each incremental dataset based on the benchmarking results. For quality, we resort to GPT-4 to generate high-quality data with each given data type. For correctness, prompt design is critical for the data generation results. Rather than previous hand-crafted prompt, we propose an Interactive Prompt Optimization strategy, which optimizes the prompt with the multi-round interaction between human and GPT, and improve the correctness of generated data greatly. Through extensive experiments, we find our MLLM-DataEngine could boost the MLLM capability in a targeted and automatic manner, with only a few human participation. We hope it could be a general solution for the following MLLMs building. The MLLM-DataEngine has been open-sourced and is now available at https://github.com/opendatalab/MLLM-DataEngine.'), Document(metadata={'title': 'Socratis: Are large multimodal models emotionally aware?'}, page_content='Existing emotion prediction benchmarks contain coarse emotion labels which do not consider the diversity of emotions that an image and text can elicit in humans due to various reasons. Learning diverse reactions to multimodal content is important as intelligent machines take a central role in generating and delivering content to society. To address this gap, we propose Socratis, a societal reactions benchmark, where each image-caption (IC) pair is annotated with multiple emotions and the reasons for feeling them. Socratis contains 18K free-form reactions for 980 emotions on 2075 image-caption pairs from 5 widely-read news and image-caption (IC) datasets. We benchmark the capability of state-of-the-art multimodal large language models to generate the reasons for feeling an emotion given an IC pair. Based on a preliminary human study, we observe that humans prefer human-written reasons over 2 times more often than machine-generated ones. This shows our task is harder than standard generation tasks because it starkly contrasts recent findings where humans cannot tell apart machine vs human-written news articles, for instance. We further see that current captioning metrics based on large vision-language models also fail to correlate with human preferences. We hope that these findings and our benchmark will inspire further research on training emotionally aware models.'), Document(metadata={'title': 'MultiWay-Adapater: Adapting large-scale multi-modal models for scalable image-text retrieval'}, page_content=\"As Multimodal Large Language Models (MLLMs) grow in size, adapting them to specialized tasks becomes increasingly challenging due to high computational and memory demands. Indeed, traditional fine-tuning methods are costly, due to the need for extensive, task-specific training. While efficient adaptation methods exist that aim to reduce these costs, in practice they suffer from shallow inter-modal alignment, which severely hurts model effectiveness. To tackle these computational challenges and improve inter-modal alignment, we introduce the MultiWay-Adapter (MWA), a novel framework featuring an 'Alignment Enhancer'. This enhancer deepens inter-modal alignment, enabling high transferability with minimal tuning effort. Our experiments show that unlike prior efficient tuning approaches, MWA maintains model effectiveness, while reducing training time by up-to 57%. MWA is also lightweight, increasing model size by only 2-3% (in terms of parameters) for state-of-the-art foundation models like BEiT-3 Large. These results demonstrate that MWA provides an efficient and effective adaptation method for MLLMs, significantly broadening their applicability.\"), Document(metadata={'title': 'NExT-GPT: Any-to-Any Multimodal LLM'}, page_content='While recently Multimodal Large Language Models (MM-LLMs) have made exciting strides, they mostly fall prey to the limitation of only input-side multimodal understanding, without the ability to produce content in multiple modalities. As we humans always perceive the world and communicate with people through various modalities, developing any-to-any MM-LLMs capable of accepting and delivering content in any modality becomes essential to human-level AI. To fill the gap, we present an end-to-end general-purpose any-to-any MM-LLM system, NExT-GPT. We connect an LLM with multimodal adaptors and different diffusion decoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio. By leveraging the existing well-trained highly-performing encoders and decoders, NExT-GPT is tuned with only a small amount of parameter (1%) of certain projection layers, which not only benefits low-cost training and also facilitates convenient expansion to more potential modalities. Moreover, we introduce a modality-switching instruction tuning (MosIT) and manually curate a high-quality dataset for MosIT, based on which NExT-GPT is empowered with complex cross-modal semantic understanding and content generation. Overall, our research showcases the promising possibility of building an AI agent capable of modeling universal modalities, paving the way for more human-like AI research in the community. Project page: https://next-gpt.github.io/'), Document(metadata={'title': 'HiLM-D: Towards High-Resolution Understanding in Multimodal Large Language Models for Autonomous Driving'}, page_content=\"Autonomous driving systems generally employ separate models for different tasks resulting in intricate designs. For the first time, we leverage singular multimodal large language models (MLLMs) to consolidate multiple autonomous driving tasks from videos, i.e., the Risk Object Localization and Intention and Suggestion Prediction (ROLISP) task. ROLISP uses natural language to simultaneously identify and interpret risk objects, understand ego-vehicle intentions, and provide motion suggestions, eliminating the necessity for task-specific architectures. However, lacking high-resolution (HR) information, existing MLLMs often miss small objects (e.g., traffic cones) and overly focus on salient ones (e.g., large trucks) when applied to ROLISP. We propose HiLM-D (Towards High-Resolution Understanding in MLLMs for Autonomous Driving), an efficient method to incorporate HR information into MLLMs for the ROLISP task. Especially, HiLM-D integrates two branches: (i) the low-resolution reasoning branch, can be any MLLMs, processes low-resolution videos to caption risk objects and discern ego-vehicle intentions/suggestions; (ii) the high-resolution perception branch (HR-PB), prominent to HiLM-D,, ingests HR images to enhance detection by capturing vision-specific HR feature maps and prioritizing all potential risks over merely salient objects. Our HR-PB serves as a plug-and-play module, seamlessly fitting into current MLLMs. Experiments on the ROLISP benchmark reveal HiLM-D's notable advantage over leading MLLMs, with improvements of 4.8% in BLEU-4 for captioning and 17.2% in mIoU for detection.\"), Document(metadata={'title': 'Investigating the Catastrophic Forgetting in Multimodal Large Language Models'}, page_content='Following the success of GPT4, there has been a surge in interest in multimodal large language model (MLLM) research. This line of research focuses on developing general-purpose LLMs through fine-tuning pre-trained LLMs and vision models. However, catastrophic forgetting, a notorious phenomenon where the fine-tuned model fails to retain similar performance compared to the pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM). In this paper, we introduce EMT: Evaluating MulTimodality for evaluating the catastrophic forgetting in MLLMs, by treating each MLLM as an image classifier. We first apply EMT to evaluate several open-source fine-tuned MLLMs and we discover that almost all evaluated MLLMs fail to retain the same performance levels as their vision encoders on standard image classification tasks. Moreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess performance throughout the fine-tuning. Interestingly, our results suggest that early-stage fine-tuning on an image dataset improves performance across other image datasets, by enhancing the alignment of text and visual features. However, as fine-tuning proceeds, the MLLMs begin to hallucinate, resulting in a significant loss of generalizability, even when the image encoder remains frozen. Our results suggest that MLLMs have yet to demonstrate performance on par with their vision models on standard image classification tasks and the current MLLM fine-tuning procedure still has room for improvement.'), Document(metadata={'title': 'DreamLLM: Synergistic Multimodal Comprehension and Creation'}, page_content=\"This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy. Project page: https://dreamllm.github.io.\"), Document(metadata={'title': \"How Robust is Google's Bard to Adversarial Image Attacks?\"}, page_content=\"Multimodal Large Language Models (MLLMs) that integrate text and other modalities (especially vision) have achieved unprecedented performance in various multimodal tasks. However, due to the unsolved adversarial robustness problem of vision models, MLLMs can have more severe safety and security risks by introducing the vision inputs. In this work, we study the adversarial robustness of Google's Bard, a competitive chatbot to ChatGPT that released its multimodal capability recently, to better understand the vulnerabilities of commercial MLLMs. By attacking white-box surrogate vision encoders or MLLMs, the generated adversarial examples can mislead Bard to output wrong image descriptions with a 22% success rate based solely on the transferability. We show that the adversarial examples can also attack other MLLMs, e.g., a 26% attack success rate against Bing Chat and a 86% attack success rate against ERNIE bot. Moreover, we identify two defense mechanisms of Bard, including face detection and toxicity detection of images. We design corresponding attacks to evade these defenses, demonstrating that the current defenses of Bard are also vulnerable. We hope this work can deepen our understanding on the robustness of MLLMs and facilitate future research on defenses. Our code is available at https://github.com/thu-ml/Attack-Bard. Update: GPT-4V is available at October 2023. We further evaluate its robustness under the same set of adversarial examples, achieving a 45% attack success rate.\"), Document(metadata={'title': 'VDC: Versatile Data Cleanser based on Visual-Linguistic Inconsistency by Multimodal Large Language Models'}, page_content='The role of data in building AI systems has recently been emphasized by the emerging concept of data-centric AI. Unfortunately, in the real-world, datasets may contain dirty samples, such as poisoned samples from backdoor attack, noisy labels in crowdsourcing, and even hybrids of them. The presence of such dirty samples makes the DNNs vunerable and unreliable.Hence, it is critical to detect dirty samples to improve the quality and realiability of dataset. Existing detectors only focus on detecting poisoned samples or noisy labels, that are often prone to weak generalization when dealing with dirty samples from other domains.In this paper, we find a commonality of various dirty samples is visual-linguistic inconsistency between images and associated labels. To capture the semantic inconsistency between modalities, we propose versatile data cleanser (VDC) leveraging the surpassing capabilities of multimodal large language models (MLLM) in cross-modal alignment and reasoning.It consists of three consecutive modules: the visual question generation module to generate insightful questions about the image; the visual question answering module to acquire the semantics of the visual content by answering the questions with MLLM; followed by the visual answer evaluation module to evaluate the inconsistency.Extensive experiments demonstrate its superior performance and generalization to various categories and types of dirty samples. The code is available at \\\\url{https://github.com/zihao-ai/vdc}.'), Document(metadata={'title': 'Guiding Instruction-based Image Editing via Multimodal Large Language Models'}, page_content='Instruction-based image editing improves the controllability and flexibility of image manipulation via natural commands without elaborate descriptions or regional masks. However, human instructions are sometimes too brief for current methods to capture and follow. Multimodal large language models (MLLMs) show promising capabilities in cross-modal understanding and visual-aware response generation via LMs. We investigate how MLLMs facilitate edit instructions and present MLLM-Guided Image Editing (MGIE). MGIE learns to derive expressive instructions and provides explicit guidance. The editing model jointly captures this visual imagination and performs manipulation through end-to-end training. We evaluate various aspects of Photoshop-style modification, global photo optimization, and local editing. Extensive experimental results demonstrate that expressive instructions are crucial to instruction-based image editing, and our MGIE can lead to a notable improvement in automatic metrics and human evaluation while maintaining competitive inference efficiency.'), Document(metadata={'title': 'Reformulating Vision-Language Foundation Models and Datasets Towards Universal Multimodal Assistants'}, page_content=\"Recent Multimodal Large Language Models (MLLMs) exhibit impressive abilities to perceive images and follow open-ended instructions. The capabilities of MLLMs depend on two crucial factors: the model architecture to facilitate the feature alignment of visual modules and large language models; the multimodal instruction tuning datasets for human instruction following. (i) For the model architecture, most existing models introduce an external bridge module to connect vision encoders with language models, which needs an additional feature-alignment pre-training. In this work, we discover that compact pre-trained vision language models can inherently serve as ``out-of-the-box'' bridges between vision and language. Based on this, we propose Muffin framework, which directly employs pre-trained vision-language models to act as providers of visual signals. (ii) For the multimodal instruction tuning datasets, existing methods omit the complementary relationship between different datasets and simply mix datasets from different tasks. Instead, we propose UniMM-Chat dataset which explores the complementarities of datasets to generate 1.1M high-quality and diverse multimodal instructions. We merge information describing the same image from diverse datasets and transforms it into more knowledge-intensive conversation data. Experimental results demonstrate the effectiveness of the Muffin framework and UniMM-Chat dataset. Muffin achieves state-of-the-art performance on a wide range of vision-language tasks, significantly surpassing state-of-the-art models like LLaVA and InstructBLIP. Our model and dataset are all accessible at https://github.com/thunlp/muffin.\"), Document(metadata={'title': 'DriveGPT4: Interpretable End-to-end Autonomous Driving via Large Language Model'}, page_content='Multimodal large language models (MLLMs) have emerged as a prominent area of interest within the research community, given their proficiency in handling and reasoning with non-textual data, including images and videos. This study seeks to extend the application of MLLMs to the realm of autonomous driving by introducing DriveGPT4, a novel interpretable end-to-end autonomous driving system based on LLMs. Capable of processing multi-frame video inputs and textual queries, DriveGPT4 facilitates the interpretation of vehicle actions, offers pertinent reasoning, and effectively addresses a diverse range of questions posed by users. Furthermore, DriveGPT4 predicts low-level vehicle control signals in an end-to-end fashion.These advanced capabilities are achieved through the utilization of a bespoke visual instruction tuning dataset, specifically tailored for autonomous driving applications, in conjunction with a mix-finetuning training strategy. DriveGPT4 represents the pioneering effort to leverage LLMs for the development of an interpretable end-to-end autonomous driving solution. Evaluations conducted on the BDD-X dataset showcase the superior qualitative and quantitative performance of DriveGPT4. Additionally, the fine-tuning of domain-specific data enables DriveGPT4 to yield close or even improved results in terms of autonomous driving grounding when contrasted with GPT4-V.'), Document(metadata={'title': 'MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens'}, page_content='The effectiveness of Multimodal Large Language Models (MLLMs) demonstrates a profound capability in multimodal understanding. However, the simultaneous generation of images with coherent texts is still underdeveloped. Addressing this, we introduce a novel interleaved vision-and-language generation method, centered around the concept of ``generative vokens\". These vokens serve as pivotal elements contributing to coherent image-text outputs. Our method is marked by a unique two-stage training strategy for description-free multimodal generation, which does not necessitate extensive descriptions of images. We integrate classifier-free guidance to enhance the alignment of generated images and texts, ensuring more seamless and contextually relevant multimodal interactions. Our model, MiniGPT-5, exhibits substantial improvement over the baseline models on multimodal generation datasets, including MMDialog and VIST. The human evaluation shows MiniGPT-5 is better than the baseline model on more than 56\\\\% cases for multimodal generation, highlighting its efficacy across diverse benchmarks.'), Document(metadata={'title': 'Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond'}, page_content='In this study, we explore the potential of Multimodal Large Language Models (MLLMs) in improving embodied decision-making processes for agents. While Large Language Models (LLMs) have been widely used due to their advanced reasoning skills and vast world knowledge, MLLMs like GPT4-Vision offer enhanced visual understanding and reasoning capabilities. We investigate whether state-of-the-art MLLMs can handle embodied decision-making in an end-to-end manner and whether collaborations between LLMs and MLLMs can enhance decision-making. To address these questions, we introduce a new benchmark called PCA-EVAL, which evaluates embodied decision-making from the perspectives of Perception, Cognition, and Action. Additionally, we propose HOLMES, a multi-agent cooperation framework that allows LLMs to leverage MLLMs and APIs to gather multimodal information for informed decision-making. We compare end-to-end embodied decision-making and HOLMES on our benchmark and find that the GPT4-Vision model demonstrates strong end-to-end embodied decision-making abilities, outperforming GPT4-HOLMES in terms of average decision accuracy (+3%). However, this performance is exclusive to the latest GPT4-Vision model, surpassing the open-source state-of-the-art MLLM by 26%. Our results indicate that powerful MLLMs like GPT4-Vision hold promise for decision-making in embodied agents, offering new avenues for MLLM research. Code and data are open at https://github.com/pkunlp-icler/PCA-EVAL/.'), Document(metadata={'title': 'Kosmos-G: Generating Images in Context with Multimodal Large Language Models'}, page_content='Recent advancements in subject-driven image generation have made significant strides. However, current methods still fall short in diverse application scenarios, as they require test-time tuning and cannot accept interleaved multi-image and text input. These limitations keep them far from the ultimate goal of \"image as a foreign language in image generation.\" This paper presents Kosmos-G, a model that leverages the advanced multimodal perception capabilities of Multimodal Large Language Models (MLLMs) to tackle the aforementioned challenge. Our approach aligns the output space of MLLM with CLIP using the textual modality as an anchor and performs compositional instruction tuning on curated data. Kosmos-G demonstrates an impressive capability of zero-shot subject-driven generation with interleaved multi-image and text input. Notably, the score distillation instruction tuning requires no modifications to the image decoder. This allows for a seamless substitution of CLIP and effortless integration with a myriad of U-Net techniques ranging from fine-grained controls to personalized image decoder variants. We posit Kosmos-G as an initial attempt towards the goal of \"image as a foreign language in image generation.\" The code can be found at https://aka.ms/Kosmos-G'), Document(metadata={'title': 'Fine-grained Audio-Visual Joint Representations for Multimodal Large Language Models'}, page_content='Audio-visual large language models (LLM) have drawn significant attention, yet the fine-grained combination of both input streams is rather under-explored, which is challenging but necessary for LLMs to understand general video inputs. To this end, a fine-grained audio-visual joint representation (FAVOR) learning framework for multimodal LLMs is proposed in this paper, which extends a text-based LLM to simultaneously perceive speech and audio events in the audio input stream and images or videos in the visual input stream, at the frame level. To fuse the audio and visual feature streams into joint representations and to align the joint space with the LLM input embedding space, we propose a causal Q-Former structure with a causal attention module to enhance the capture of causal relations of the audio-visual frames across time. An audio-visual evaluation benchmark (AVEB) is also proposed which comprises six representative single-modal tasks with five cross-modal tasks reflecting audio-visual co-reasoning abilities. While achieving competitive single-modal performance on audio, speech and image tasks in AVEB, FAVOR achieved over 20% accuracy improvements on the video question-answering task when fine-grained information or temporal causal reasoning is required. FAVOR, in addition, demonstrated remarkable video comprehension and reasoning abilities on tasks that are unprecedented by other multimodal LLMs. An interactive demo of FAVOR is available at https://github.com/BriansIDP/AudioVisualLLM.git, and the training code and model checkpoints will be released soon.'), Document(metadata={'title': 'Can We Edit Multimodal Large Language Models?'}, page_content='In this paper, we focus on editing Multimodal Large Language Models (MLLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights. Code and dataset are available in https://github.com/zjunlp/EasyEdit.'), Document(metadata={'title': 'AutoRepo: A general framework for multi-modal LLM-based automated construction reporting'}, page_content='Ensuring the safety, quality, and timely completion of construction projects is paramount, with construction inspections serving as a vital instrument towards these goals. Nevertheless, the predominantly manual approach of present-day inspections frequently results in inefficiencies and inadequate information management. Such methods often fall short of providing holistic, exhaustive assessments, consequently engendering regulatory oversights and potential safety hazards. To address this issue, this paper presents a novel framework named AutoRepo for automated generation of construction inspection reports. The unmanned vehicles efficiently perform construction inspections and collect scene information, while the multimodal large language models (LLMs) are leveraged to automatically generate the inspection reports. The framework was applied and tested on a real-world construction site, demonstrating its potential to expedite the inspection process, significantly reduce resource allocation, and produce high-quality, regulatory standard-compliant inspection reports. This research thus underscores the immense potential of multimodal large language models in revolutionizing construction inspection practices, signaling a significant leap forward towards a more efficient and safer construction management paradigm.'), Document(metadata={'title': 'MM-BigBench: Evaluating Multimodal Models on Multimodal Content Comprehension Tasks'}, page_content=\"The popularity of multimodal large language models (MLLMs) has triggered a recent surge in research efforts dedicated to evaluating these models. Nevertheless, existing evaluation studies of MLLMs primarily focus on the comprehension and reasoning of unimodal (vision) content, neglecting performance evaluations in the domain of multimodal (vision-language) content understanding. Beyond multimodal reasoning, tasks related to multimodal content comprehension necessitate a profound understanding of multimodal contexts, achieved through the multimodal interaction to obtain a final answer. In this paper, we introduce a comprehensive assessment framework called MM-BigBench, which incorporates a diverse range of metrics to offer an extensive evaluation of the performance of various models and instructions across a wide spectrum of diverse multimodal content comprehension tasks. Consequently, our work complements research on the performance of MLLMs in multimodal comprehension tasks, achieving a more comprehensive and holistic evaluation of MLLMs. To begin, we employ the Best Performance metric to ascertain each model's performance upper bound on different datasets. Subsequently, the Mean Relative Gain metric offers an assessment of the overall performance of various models and instructions, while the Stability metric measures their sensitivity. Furthermore, previous research centers on evaluating models independently or solely assessing instructions, neglecting the adaptability between models and instructions. We propose the Adaptability metric to quantify the adaptability between models and instructions. Our paper evaluates a total of 20 language models (14 MLLMs) on 14 multimodal datasets spanning 6 tasks, with 10 instructions for each task, and derives novel insights. Our code will be released at https://github.com/declare-lab/MM-BigBench.\"), Document(metadata={'title': 'RoboLLM: Robotic Vision Tasks Grounded on Multimodal Large Language Models'}, page_content='Robotic vision applications often necessitate a wide range of visual perception tasks, such as object detection, segmentation, and identification. While there have been substantial advances in these individual tasks, integrating specialized models into a unified vision pipeline presents significant engineering challenges and costs. Recently, Multimodal Large Language Models (MLLMs) have emerged as novel backbones for various downstream tasks. We argue that leveraging the pre-training capabilities of MLLMs enables the creation of a simplified framework, thus mitigating the need for task-specific encoders. Specifically, the large-scale pretrained knowledge in MLLMs allows for easier fine-tuning to downstream robotic vision tasks and yields superior performance. We introduce the RoboLLM framework, equipped with a BEiT-3 backbone, to address all visual perception tasks in the ARMBench challenge-a large-scale robotic manipulation dataset about real-world warehouse scenarios. RoboLLM not only outperforms existing baselines but also substantially reduces the engineering burden associated with model selection and tuning. The source code is publicly available at https://github.com/longkukuhi/armbench.'), Document(metadata={'title': 'Benchmarking Sequential Visual Input Reasoning and Prediction in Multimodal Large Language Models'}, page_content=\"Multimodal large language models (MLLMs) have shown great potential in perception and interpretation tasks, but their capabilities in predictive reasoning remain under-explored. To address this gap, we introduce a novel benchmark that assesses the predictive reasoning capabilities of MLLMs across diverse scenarios. Our benchmark targets three important domains: abstract pattern reasoning, human activity prediction, and physical interaction prediction. We further develop three evaluation methods powered by large language model to robustly quantify a model's performance in predicting and reasoning the future based on multi-visual context. Empirical experiments confirm the soundness of the proposed benchmark and evaluation methods via rigorous testing and reveal pros and cons of current popular MLLMs in the task of predictive reasoning. Lastly, our proposed benchmark provides a standardized evaluation framework for MLLMs and can facilitate the development of more advanced models that can reason and predict over complex long sequence of multimodal input.\"), Document(metadata={'title': 'Woodpecker: Hallucination Correction for Multimodal Large Language Models'}, page_content='Hallucination is a big shadow hanging over the rapidly evolving Multimodal Large Language Models (MLLMs), referring to the phenomenon that the generated text is inconsistent with the image content. In order to mitigate hallucinations, existing studies mainly resort to an instruction-tuning manner that requires retraining the models with specific data. In this paper, we pave a different way, introducing a training-free method named Woodpecker. Like a woodpecker heals trees, it picks out and corrects hallucinations from the generated text. Concretely, Woodpecker consists of five stages: key concept extraction, question formulation, visual knowledge validation, visual claim generation, and hallucination correction. Implemented in a post-remedy manner, Woodpecker can easily serve different MLLMs, while being interpretable by accessing intermediate outputs of the five stages. We evaluate Woodpecker both quantitatively and qualitatively and show the huge potential of this new paradigm. On the POPE benchmark, our method obtains a 30.66%/24.33% improvement in accuracy over the baseline MiniGPT-4/mPLUG-Owl. The source code is released at https://github.com/BradyFU/Woodpecker.'), Document(metadata={'title': 'Towards Perceiving Small Visual Details in Zero-shot Visual Question Answering with Multimodal LLMs'}, page_content=\"Multimodal Large Language Models (MLLMs) have recently achieved promising zero-shot accuracy on visual question answering (VQA) -- a fundamental task affecting various downstream applications and domains. Given the great potential for the broad use of these models, it is important to investigate their limitations in dealing with different image and question properties. In this work, we investigate whether MLLMs can perceive small details as well as large details in images. In particular, we show that their zero-shot accuracy in answering visual questions is very sensitive to the size of the visual subject of the question, declining up to 46% with size. Furthermore, we show that this effect is causal by observing that human visual cropping can significantly mitigate their sensitivity to size. Inspired by the usefulness of human cropping, we then propose five automatic visual cropping methods -- leveraging either external localization models or the decision process of the given MLLM itself -- as inference time mechanisms to improve the zero-shot performance of MLLMs. We study their effectiveness on four popular VQA datasets, and a subset of the VQAv2 dataset tailored towards fine visual details. Our findings suggest that MLLMs should be used with caution in detail-sensitive VQA applications, and that visual cropping is a promising direction to improve their zero-shot performance. To facilitate further investigation of MLLMs' behaviors, our code and data are publicly released.\"), Document(metadata={'title': 'ChEF: A Comprehensive Evaluation Framework for Standardized Assessment of Multimodal Large Language Models'}, page_content=\"Multimodal Large Language Models (MLLMs) have shown impressive abilities in interacting with visual content with myriad potential downstream tasks. However, even though a list of benchmarks has been proposed, the capabilities and limitations of MLLMs are still not comprehensively understood, due to a lack of a standardized and holistic evaluation framework. To this end, we present the first Comprehensive Evaluation Framework (ChEF) that can holistically profile each MLLM and fairly compare different MLLMs. First, we structure ChEF as four modular components, i.e., Scenario as scalable multimodal datasets, Instruction as flexible instruction retrieving formulae, Inferencer as reliable question answering strategies, and Metric as indicative task-specific score functions. Based on them, ChEF facilitates versatile evaluations in a standardized framework, and new evaluations can be built by designing new Recipes (systematic selection of these four components). Notably, current MLLM benchmarks can be readily summarized as recipes of ChEF. Second, we introduce 6 new recipes to quantify competent MLLMs' desired capabilities (or called desiderata, i.e., calibration, in-context learning, instruction following, language performance, hallucination, and robustness) as reliable agents that can perform real-world multimodal interactions. Third, we conduct a large-scale evaluation of 9 prominent MLLMs on 9 scenarios and 6 desiderata. Our evaluation summarized over 20 valuable observations concerning the generalizability of MLLMs across various scenarios and the composite capability of MLLMs required for multimodal interactions. We will publicly release all the detailed implementations for further analysis, as well as an easy-to-use modular toolkit for the integration of new recipes and models, so that ChEF can be a growing evaluation framework for the MLLM community.\"), Document(metadata={'title': 'Octavius: Mitigating Task Interference in MLLMs via LoRA-MoE'}, page_content='Recent studies have demonstrated Large Language Models (LLMs) can extend their zero-shot generalization capabilities to multimodal learning through instruction tuning. As more modalities and downstream tasks are introduced, negative conflicts and interference may have a worse impact on performance. While this phenomenon has been overlooked in previous work, we propose a novel and extensible framework, called Octavius, for comprehensive studies and experimentation on multimodal learning with Multimodal Large Language Models (MLLMs). Specifically, we combine the well-known Mixture-of-Experts (MoE) and one of the representative PEFT techniques, i.e., LoRA, designing a novel LLM-based decoder, called LoRA-MoE, for multimodal learning. To the best of our knowledge, we are one of the pioneering efforts to introduce MoE into MLLMs to address this problem. The experimental results (about 20% improvement) have shown the effectiveness and versatility of our design in various 2D and 3D downstream tasks. Code and datasets are available at https://openlamm.github.io/tutorial/.'), Document(metadata={'title': 'What Large Language Models Bring to Text-rich VQA?'}, page_content=\"Text-rich VQA, namely Visual Question Answering based on text recognition in the images, is a cross-modal task that requires both image comprehension and text recognition. In this work, we focus on investigating the advantages and bottlenecks of LLM-based approaches in addressing this problem. To address the above concern, we separate the vision and language modules, where we leverage external OCR models to recognize texts in the image and Large Language Models (LLMs) to answer the question given texts. The whole framework is training-free benefiting from the in-context ability of LLMs. This pipeline achieved superior performance compared to the majority of existing Multimodal Large Language Models (MLLM) on four text-rich VQA datasets. Besides, based on the ablation study, we find that LLM brings stronger comprehension ability and may introduce helpful knowledge for the VQA problem. The bottleneck for LLM to address text-rich VQA problems may primarily lie in visual part. We also combine the OCR module with MLLMs and pleasantly find that the combination of OCR module with MLLM also works. It's worth noting that not all MLLMs can comprehend the OCR information, which provides insights into how to train an MLLM that preserves the abilities of LLM.\")], 'results': [Document(id='9e9ac81d-692f-441b-8926-4b6ba73ad784', metadata={'title': 'A Survey on Multimodal Large Language Models'}, page_content='Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.'), Document(id='128c9e1b-adb3-49e0-a873-881354331b12', metadata={'title': 'A Survey on Multimodal Large Language Models'}, page_content='Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.'), Document(id='01a7f21a-97a8-45a2-a73c-74526485470f', metadata={'title': 'A Survey on Multimodal Large Language Models'}, page_content='Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.')], 'context': '[1] A Survey on Multimodal Large Language Models\\nRecently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.\\n\\n[2] A Survey on Multimodal Large Language Models\\nRecently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.\\n\\n[3] A Survey on Multimodal Large Language Models\\nRecently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.', 'final_answer': \"Multimodal Large Language Models (MLLMs) are a growing area of research, exemplified by models like GPT-4V that leverage LLMs for multimodal tasks [1, 2, 3]. These models demonstrate new capabilities such as generating stories from images and performing math reasoning without OCR, which are not typical of traditional multimodal approaches, hinting at a potential path toward artificial general intelligence [1, 2, 3]. Current research is heavily focused on creating MLLMs that can rival or surpass GPT-4V [1, 2, 3]. Key areas of investigation include MLLM architecture, training methodologies, data utilization, and evaluation techniques [1, 2, 3]. Furthermore, research is exploring how to broaden MLLMs' capabilities to encompass finer granularity, a wider range of modalities and languages, and diverse application scenarios. This includes tackling challenges like multimodal hallucination and incorporating advanced techniques like Multimodal In-Context Learning (M-ICL), Multimodal Chain-of-Thought (M-CoT), and LLM-Aided Visual Reasoning (LAVR) [1, 2, 3].\", 'expert_opinion': 'Multimodal Large Language Models (MLLMs) are a rising research area, with models like GPT-4V using LLMs to perform multimodal tasks [1, 2, 3]. These models exhibit emergent capabilities like image-based story writing and OCR-free math reasoning, which are uncommon in traditional multimodal methods, suggesting a possible route to artificial general intelligence [1, 2, 3]. Research is focused on developing MLLMs that can compete with or outperform GPT-4V [1, 2, 3]. The key aspects of MLLMs under investigation include architecture, training strategies, data, and evaluation methods [1, 2, 3]. Research also explores expanding MLLMs to support more granularity, modalities, languages, and scenarios, as well as addressing multimodal hallucination and incorporating techniques like Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR) [1, 2, 3].', 'reviewer_opinion': \"The expert's response accurately summarizes the information provided in the context documents. All claims are supported by citations to the provided documents. The response directly addresses the user's query about multimodal LLMs, avoiding the information the user specified they were not interested in.\", 'messages': [AIMessage(content='Retrieving expert insights based on the papers and cite the papers...', additional_kwargs={}, response_metadata={}, id='22fe505c-90a0-4118-95eb-6045f2c066c7'), HumanMessage(content=\"Context: [1] A Survey on Multimodal Large Language Models\\nRecently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.\\n\\n[2] A Survey on Multimodal Large Language Models\\nRecently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.\\n\\n[3] A Survey on Multimodal Large Language Models\\nRecently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.\\n\\nQuery: latest advances in multimodal learning Consider User's response for the search query: I dont want to know about latest advancements, I want to know about multimodal llms\", additional_kwargs={}, response_metadata={}, id='c9f3c950-8322-4da3-a95b-1115429cd167'), AIMessage(content='Multimodal Large Language Models (MLLMs) are a rising research area, with models like GPT-4V using LLMs to perform multimodal tasks [1, 2, 3]. These models exhibit emergent capabilities like image-based story writing and OCR-free math reasoning, which are uncommon in traditional multimodal methods, suggesting a possible route to artificial general intelligence [1, 2, 3]. Research is focused on developing MLLMs that can compete with or outperform GPT-4V [1, 2, 3]. The key aspects of MLLMs under investigation include architecture, training strategies, data, and evaluation methods [1, 2, 3]. Research also explores expanding MLLMs to support more granularity, modalities, languages, and scenarios, as well as addressing multimodal hallucination and incorporating techniques like Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR) [1, 2, 3].', additional_kwargs={}, response_metadata={}, id='5d83e61e-b24a-4136-81d1-2c670290ed0f'), AIMessage(content=\"🧪 Reviewing the expert's findings for validity...\", additional_kwargs={}, response_metadata={}, id='53d69017-f1f4-47e6-bb32-dd68bf9bc464'), HumanMessage(content='Expert said:\\nMultimodal Large Language Models (MLLMs) are a rising research area, with models like GPT-4V using LLMs to perform multimodal tasks [1, 2, 3]. These models exhibit emergent capabilities like image-based story writing and OCR-free math reasoning, which are uncommon in traditional multimodal methods, suggesting a possible route to artificial general intelligence [1, 2, 3]. Research is focused on developing MLLMs that can compete with or outperform GPT-4V [1, 2, 3]. The key aspects of MLLMs under investigation include architecture, training strategies, data, and evaluation methods [1, 2, 3]. Research also explores expanding MLLMs to support more granularity, modalities, languages, and scenarios, as well as addressing multimodal hallucination and incorporating techniques like Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR) [1, 2, 3].', additional_kwargs={}, response_metadata={}, id='f5d7f20d-0d50-4ae5-98b5-9d750c4eaa60'), AIMessage(content=\"The expert's response accurately summarizes the information provided in the context documents. All claims are supported by citations to the provided documents. The response directly addresses the user's query about multimodal LLMs, avoiding the information the user specified they were not interested in.\", additional_kwargs={}, response_metadata={}, id='da43df89-5565-45f7-a7ca-2cadfe5fe841'), AIMessage(content='🧠 Synthesizing expert and reviewer input...', additional_kwargs={}, response_metadata={}, id='a8c88e72-36ea-42e5-b04d-989449074e06'), HumanMessage(content=\"Expert Opinion:\\nMultimodal Large Language Models (MLLMs) are a rising research area, with models like GPT-4V using LLMs to perform multimodal tasks [1, 2, 3]. These models exhibit emergent capabilities like image-based story writing and OCR-free math reasoning, which are uncommon in traditional multimodal methods, suggesting a possible route to artificial general intelligence [1, 2, 3]. Research is focused on developing MLLMs that can compete with or outperform GPT-4V [1, 2, 3]. The key aspects of MLLMs under investigation include architecture, training strategies, data, and evaluation methods [1, 2, 3]. Research also explores expanding MLLMs to support more granularity, modalities, languages, and scenarios, as well as addressing multimodal hallucination and incorporating techniques like Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR) [1, 2, 3].\\n\\nReviewer Feedback:\\nThe expert's response accurately summarizes the information provided in the context documents. All claims are supported by citations to the provided documents. The response directly addresses the user's query about multimodal LLMs, avoiding the information the user specified they were not interested in.\\n\\nFinal Question: latest advances in multimodal learning Consider User's response for the search query: I dont want to know about latest advancements, I want to know about multimodal llms\", additional_kwargs={}, response_metadata={}, id='8839537b-c869-4fa8-89ed-7e6e2e068c3f'), AIMessage(content=\"Multimodal Large Language Models (MLLMs) are a growing area of research, exemplified by models like GPT-4V that leverage LLMs for multimodal tasks [1, 2, 3]. These models demonstrate new capabilities such as generating stories from images and performing math reasoning without OCR, which are not typical of traditional multimodal approaches, hinting at a potential path toward artificial general intelligence [1, 2, 3]. Current research is heavily focused on creating MLLMs that can rival or surpass GPT-4V [1, 2, 3]. Key areas of investigation include MLLM architecture, training methodologies, data utilization, and evaluation techniques [1, 2, 3]. Furthermore, research is exploring how to broaden MLLMs' capabilities to encompass finer granularity, a wider range of modalities and languages, and diverse application scenarios. This includes tackling challenges like multimodal hallucination and incorporating advanced techniques like Multimodal In-Context Learning (M-ICL), Multimodal Chain-of-Thought (M-CoT), and LLM-Aided Visual Reasoning (LAVR) [1, 2, 3].\", additional_kwargs={}, response_metadata={}, id='3d559587-e7b5-43df-b4a6-0eb472a47d39'), HumanMessage(content='Thank you!', additional_kwargs={}, response_metadata={}, id='02db0fa4-84e0-48e6-8c63-23f46fc0329c')], 'retry_count': 2}\n","output_type":"stream"}],"execution_count":215},{"cell_type":"code","source":"state = runnable.get_state()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:55:36.170033Z","iopub.execute_input":"2025-04-22T05:55:36.170342Z","iopub.status.idle":"2025-04-22T05:55:36.183316Z","shell.execute_reply.started":"2025-04-22T05:55:36.170322Z","shell.execute_reply":"2025-04-22T05:55:36.182015Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/4283719797.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunnable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: Pregel.get_state() missing 1 required positional argument: 'config'"],"ename":"TypeError","evalue":"Pregel.get_state() missing 1 required positional argument: 'config'","output_type":"error"}],"execution_count":169}]}